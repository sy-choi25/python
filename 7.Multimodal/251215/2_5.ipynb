{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a59eade8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199df4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:14<00:00, 12.0MB/s] \n"
     ]
    }
   ],
   "source": [
    "# 상위 디렉토리의 config 모듈 import\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
    "from config import get_device, set_seed, get_optimal_batch_size\n",
    "\n",
    "\n",
    "def print_section(title):\n",
    "    \"\"\"섹션 제목 출력\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\" {title}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1. 환경 설정\n",
    "# ============================================================\n",
    "def setup_environment(force_cpu=False):\n",
    "    \"\"\"환경 설정\"\"\"\n",
    "    print_section(\"1. 환경 설정\")\n",
    "    \n",
    "    # 디바이스 설정 (config 모듈 사용)\n",
    "    device = get_device(force_cpu=force_cpu)\n",
    "    \n",
    "    # 재현성을 위한 시드 설정\n",
    "    set_seed(42)\n",
    "    \n",
    "    print(f\"\\n[시드 설정]\")\n",
    "    print(f\"  랜덤 시드: 42\")\n",
    "    \n",
    "    return device\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. 데이터셋 준비 (CIFAR-10)\n",
    "# ============================================================\n",
    "def prepare_cifar10_dataset():\n",
    "    \"\"\"CIFAR-10 데이터셋 준비\"\"\"\n",
    "    print_section(\"2. CIFAR-10 데이터셋 준비\")\n",
    "    \n",
    "    # CIFAR-10 클래스\n",
    "    classes = ('airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "    \n",
    "    print(f\"\\n[CIFAR-10 정보]\")\n",
    "    print(f\"  클래스 수: 10\")\n",
    "    print(f\"  클래스: {classes}\")\n",
    "    print(f\"  이미지 크기: 32 x 32\")\n",
    "    print(f\"  학습 데이터: 50,000장\")\n",
    "    print(f\"  테스트 데이터: 10,000장\")\n",
    "    \n",
    "    # ImageNet 통계 (ViT 사전학습에 사용됨)\n",
    "    MEAN = [0.485, 0.456, 0.406]\n",
    "    STD = [0.229, 0.224, 0.225]\n",
    "    \n",
    "    # ViT는 224x224 입력이 필요하므로 리사이즈\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=MEAN, std=STD),\n",
    "    ])\n",
    "    \n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=MEAN, std=STD),\n",
    "    ])\n",
    "    \n",
    "    print(f\"\\n[데이터 증강]\")\n",
    "    print(f\"  학습용: Resize, RandomHorizontalFlip, RandomRotation, ColorJitter\")\n",
    "    print(f\"  검증용: Resize만 적용\")\n",
    "    \n",
    "    # 데이터셋 로드\n",
    "    print(f\"\\n[데이터셋 다운로드/로드 중...]\")\n",
    "    data_dir = './data'\n",
    "    \n",
    "    train_dataset = datasets.CIFAR10(\n",
    "        root=data_dir, train=True, download=True, transform=train_transform\n",
    "    )\n",
    "    test_dataset = datasets.CIFAR10(\n",
    "        root=data_dir, train=False, download=True, transform=val_transform\n",
    "    )\n",
    "    \n",
    "    # 학습/검증 분리 (학습의 10%를 검증으로)\n",
    "    train_size = int(0.9 * len(train_dataset))\n",
    "    val_size = len(train_dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        train_dataset, [train_size, val_size]\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n[데이터 분할]\")\n",
    "    print(f\"  학습 데이터: {len(train_dataset)}\")\n",
    "    print(f\"  검증 데이터: {len(val_dataset)}\")\n",
    "    print(f\"  테스트 데이터: {len(test_dataset)}\")\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset, classes\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. 데이터 로더 생성\n",
    "# ============================================================\n",
    "def create_dataloaders(train_dataset, val_dataset, test_dataset, batch_size=32):\n",
    "    \"\"\"데이터 로더 생성\"\"\"\n",
    "    print_section(\"3. 데이터 로더 생성\")\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, \n",
    "        num_workers=0, pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=0, pin_memory=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=0, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n[데이터 로더 설정]\")\n",
    "    print(f\"  배치 크기: {batch_size}\")\n",
    "    print(f\"  학습 배치 수: {len(train_loader)}\")\n",
    "    print(f\"  검증 배치 수: {len(val_loader)}\")\n",
    "    print(f\"  테스트 배치 수: {len(test_loader)}\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. ViT 모델 준비\n",
    "# ============================================================\n",
    "def prepare_vit_model(num_classes, device, strategy='full'):\n",
    "    \"\"\"ViT 모델 준비\"\"\"\n",
    "    print_section(\"4. ViT 모델 준비\")\n",
    "    \n",
    "    try:\n",
    "        from transformers import ViTForImageClassification, ViTConfig\n",
    "        \n",
    "        # 사전학습 모델 로드\n",
    "        model_name = \"google/vit-base-patch16-224\"\n",
    "        print(f\"\\n[모델 로드]\")\n",
    "        print(f\"  사전학습 모델: {model_name}\")\n",
    "        \n",
    "        model = ViTForImageClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_classes,\n",
    "            ignore_mismatched_sizes=True  # classifier 크기 불일치 무시\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n[원본 모델 정보]\")\n",
    "        print(f\"  원본 클래스 수: 1000 (ImageNet)\")\n",
    "        print(f\"  새 클래스 수: {num_classes}\")\n",
    "        \n",
    "        # 파인튜닝 전략 적용\n",
    "        if strategy == 'linear_probe':\n",
    "            # Linear Probing: Encoder 동결\n",
    "            print(f\"\\n[파인튜닝 전략: Linear Probing]\")\n",
    "            for param in model.vit.parameters():\n",
    "                param.requires_grad = False\n",
    "            print(f\"  ViT Encoder: 동결됨 (학습 안 함)\")\n",
    "            print(f\"  Classifier: 학습됨\")\n",
    "            \n",
    "        elif strategy == 'partial':\n",
    "            # 일부 레이어만 학습 (마지막 2개 블록)\n",
    "            print(f\"\\n[파인튜닝 전략: Partial Fine-tuning]\")\n",
    "            for param in model.vit.parameters():\n",
    "                param.requires_grad = False\n",
    "            # 마지막 2개 encoder 블록 학습\n",
    "            for param in model.vit.encoder.layer[-2:].parameters():\n",
    "                param.requires_grad = True\n",
    "            print(f\"  ViT Encoder (Layer 0-9): 동결됨\")\n",
    "            print(f\"  ViT Encoder (Layer 10-11): 학습됨\")\n",
    "            print(f\"  Classifier: 학습됨\")\n",
    "            \n",
    "        else:  # full\n",
    "            print(f\"\\n[파인튜닝 전략: Full Fine-tuning]\")\n",
    "            print(f\"  전체 모델 학습\")\n",
    "        \n",
    "        # 학습 가능한 파라미터 수 계산\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        \n",
    "        print(f\"\\n[파라미터 정보]\")\n",
    "        print(f\"  전체 파라미터: {total_params:,}\")\n",
    "        print(f\"  학습 파라미터: {trainable_params:,}\")\n",
    "        print(f\"  학습 비율: {trainable_params/total_params*100:.2f}%\")\n",
    "        \n",
    "        model = model.to(device)\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"  transformers 라이브러리가 필요합니다.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5. 학습 함수\n",
    "# ============================================================\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, device, epoch):\n",
    "    \"\"\"한 에포크 학습\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch}')\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(pbar):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 통계\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.logits.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # 진행 상황 업데이트\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{running_loss/(batch_idx+1):.4f}',\n",
    "            'acc': f'{100.*correct/total:.2f}%'\n",
    "        })\n",
    "    \n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def evaluate(model, data_loader, criterion, device):\n",
    "    \"\"\"모델 평가\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.logits.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    avg_loss = running_loss / len(data_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6. 전체 학습 루프\n",
    "# ============================================================\n",
    "def train_model(model, train_loader, val_loader, device, num_epochs=5, lr=1e-4):\n",
    "    \"\"\"전체 학습 루프\"\"\"\n",
    "    print_section(\"6. 모델 학습\")\n",
    "    \n",
    "    if model is None:\n",
    "        print(\"  모델이 준비되지 않았습니다.\")\n",
    "        return None\n",
    "    \n",
    "    # 손실 함수 및 옵티마이저\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=lr,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    \n",
    "    # Learning rate 스케줄러\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=num_epochs, eta_min=1e-6\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n[학습 설정]\")\n",
    "    print(f\"  손실 함수: CrossEntropyLoss\")\n",
    "    print(f\"  옵티마이저: AdamW\")\n",
    "    print(f\"  초기 Learning Rate: {lr}\")\n",
    "    print(f\"  Weight Decay: 0.01\")\n",
    "    print(f\"  스케줄러: CosineAnnealingLR\")\n",
    "    print(f\"  에포크 수: {num_epochs}\")\n",
    "    \n",
    "    # 학습 기록\n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [],\n",
    "        'val_loss': [], 'val_acc': []\n",
    "    }\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    print(f\"\\n[학습 시작]\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # 학습\n",
    "        train_loss, train_acc = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, device, epoch\n",
    "        )\n",
    "        \n",
    "        # 검증\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "        \n",
    "        # 스케줄러 업데이트\n",
    "        scheduler.step()\n",
    "        \n",
    "        # 기록 저장\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # 출력\n",
    "        print(f\"\\n  Epoch {epoch}/{num_epochs}\")\n",
    "        print(f\"    Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"    Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "        print(f\"    LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "        \n",
    "        # 최고 성능 모델 저장\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f\"    >> Best model saved! (Val Acc: {best_val_acc:.2f}%)\")\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"\\n[학습 완료]\")\n",
    "    print(f\"  총 학습 시간: {elapsed_time/60:.2f}분\")\n",
    "    print(f\"  최고 검증 정확도: {best_val_acc:.2f}%\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7. 학습 결과 시각화\n",
    "# ============================================================\n",
    "def plot_training_history(history):\n",
    "    \"\"\"학습 기록 시각화\"\"\"\n",
    "    print_section(\"7. 학습 결과 시각화\")\n",
    "    \n",
    "    if history is None:\n",
    "        print(\"  학습 기록이 없습니다.\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Loss 그래프\n",
    "    axes[0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "    axes[0].plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Training and Validation Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    # Accuracy 그래프\n",
    "    axes[1].plot(history['train_acc'], label='Train Acc', marker='o')\n",
    "    axes[1].plot(history['val_acc'], label='Val Acc', marker='s')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy (%)')\n",
    "    axes[1].set_title('Training and Validation Accuracy')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    save_path = 'training_history.png'\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"\\n  학습 기록 저장: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 8. 테스트 및 혼동 행렬\n",
    "# ============================================================\n",
    "def test_model(model, test_loader, device, classes):\n",
    "    \"\"\"테스트 세트에서 모델 평가\"\"\"\n",
    "    print_section(\"8. 테스트 평가\")\n",
    "    \n",
    "    if model is None:\n",
    "        print(\"  모델이 준비되지 않았습니다.\")\n",
    "        return\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc='Testing'):\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.logits.max(1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "    \n",
    "    # 전체 정확도\n",
    "    correct = sum(p == l for p, l in zip(all_preds, all_labels))\n",
    "    accuracy = 100. * correct / len(all_labels)\n",
    "    \n",
    "    print(f\"\\n[테스트 결과]\")\n",
    "    print(f\"  테스트 정확도: {accuracy:.2f}%\")\n",
    "    \n",
    "    # 클래스별 정확도\n",
    "    print(f\"\\n[클래스별 정확도]\")\n",
    "    for i, cls in enumerate(classes):\n",
    "        cls_indices = [j for j, l in enumerate(all_labels) if l == i]\n",
    "        cls_correct = sum(all_preds[j] == i for j in cls_indices)\n",
    "        cls_acc = 100. * cls_correct / len(cls_indices) if cls_indices else 0\n",
    "        print(f\"  {cls:<12}: {cls_acc:.2f}%\")\n",
    "    \n",
    "    # 혼동 행렬 시각화\n",
    "    try:\n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        import seaborn as sns\n",
    "        \n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=classes, yticklabels=classes)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title('Confusion Matrix')\n",
    "        \n",
    "        save_path = 'confusion_matrix.png'\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"\\n  혼동 행렬 저장: {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"\\n  sklearn, seaborn 라이브러리가 필요합니다.\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 9. 빠른 테스트 (데모용)\n",
    "# ============================================================\n",
    "def quick_demo(device):\n",
    "    \"\"\"빠른 데모 (적은 데이터로 테스트)\"\"\"\n",
    "    print_section(\"빠른 데모 모드\")\n",
    "    \n",
    "    print(\"\\n[데모 모드로 실행]\")\n",
    "    print(\"  - 데이터 1000장만 사용\")\n",
    "    print(\"  - 1 에포크만 학습\")\n",
    "    print(\"  - Linear Probing 전략\")\n",
    "    \n",
    "    # 데이터셋 준비\n",
    "    MEAN = [0.485, 0.456, 0.406]\n",
    "    STD = [0.229, 0.224, 0.225]\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=MEAN, std=STD),\n",
    "    ])\n",
    "    \n",
    "    # CIFAR-10 로드\n",
    "    train_dataset = datasets.CIFAR10(\n",
    "        root='./data', train=True, download=True, transform=transform\n",
    "    )\n",
    "    \n",
    "    # 1000장만 사용\n",
    "    indices = list(range(1000))\n",
    "    train_subset = torch.utils.data.Subset(train_dataset, indices[:800])\n",
    "    val_subset = torch.utils.data.Subset(train_dataset, indices[800:])\n",
    "    \n",
    "    train_loader = DataLoader(train_subset, batch_size=32, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_subset, batch_size=32, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # 모델 준비 (Linear Probing)\n",
    "    model = prepare_vit_model(num_classes=10, device=device, strategy='linear_probe')\n",
    "    \n",
    "    if model is None:\n",
    "        return\n",
    "    \n",
    "    # 1 에포크 학습\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=1e-3\n",
    "    )\n",
    "    \n",
    "    print(\"\\n[학습 시작 (1 에포크)]\")\n",
    "    train_loss, train_acc = train_one_epoch(\n",
    "        model, train_loader, criterion, optimizer, device, 1\n",
    "    )\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "    \n",
    "    print(f\"\\n[결과]\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 메인 실행\n",
    "# ============================================================\n",
    "def main(force_cpu=False):\n",
    "    \"\"\"메인 실행 함수\"\"\"\n",
    "    print(\"\\n\" + \"#\" * 60)\n",
    "    print(\"# ViT 파인튜닝 기초 실습\")\n",
    "    print(\"#\" * 60)\n",
    "    \n",
    "    # 환경 설정\n",
    "    device = setup_environment(force_cpu=force_cpu)\n",
    "    \n",
    "    # 사용자 선택\n",
    "    print(\"\\n[실행 모드 선택]\")\n",
    "    print(\"  1. 빠른 데모 (1000장, 1 에포크)\")\n",
    "    print(\"  2. 전체 학습 (50000장, 5 에포크)\")\n",
    "    \n",
    "    # 데모 모드로 실행 (실제 사용 시 input()으로 선택)\n",
    "    mode = \"1\"  # 데모 모드\n",
    "    \n",
    "    if mode == \"1\":\n",
    "        quick_demo(device)\n",
    "    else:\n",
    "        # 데이터셋 준비\n",
    "        train_dataset, val_dataset, test_dataset, classes = prepare_cifar10_dataset()\n",
    "        \n",
    "        # 데이터 로더 생성\n",
    "        batch_size = get_optimal_batch_size('base', device)\n",
    "        train_loader, val_loader, test_loader = create_dataloaders(\n",
    "            train_dataset, val_dataset, test_dataset, batch_size=batch_size\n",
    "        )\n",
    "        \n",
    "        # 모델 준비\n",
    "        model = prepare_vit_model(num_classes=10, device=device, strategy='linear_probe')\n",
    "        \n",
    "        # 학습\n",
    "        history = train_model(model, train_loader, val_loader, device, \n",
    "                             num_epochs=5, lr=1e-3)\n",
    "        \n",
    "        # 결과 시각화\n",
    "        plot_training_history(history)\n",
    "        \n",
    "        # 테스트\n",
    "        test_model(model, test_loader, device, classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
