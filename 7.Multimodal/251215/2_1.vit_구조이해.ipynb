{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5adbf9ea",
   "metadata": {},
   "source": [
    "### <span style=\"color: Gold\">ViT의 구조와 Attention 메커니즘 이해</span>\n",
    "\n",
    "<span style=\"font-size:12px;\">\n",
    "\n",
    "- ViT가 이미지를 문장처럼 다루는 방식\"을 이해하는 단계\n",
    "- 이미지는 → Patch → Token → Transformer 입력\n",
    "- Q·K·V는 방향 + 중요도 판단용 벡터\n",
    "- Attention은 \"어디를 볼지\"를 계산하는 연산\n",
    "\n",
    "- 핵심!!\n",
    "     1. 입력 & 전처리: \n",
    "        - 이미지를 224×224로 resize 픽셀 값 정규화 → 모델이 기대하는 고정 입력 형태로 맞춤\n",
    "     2. 패치 임베딩 (Image → Token): \n",
    "        - 이미지를 16×16 패치로 분할-> 총 196개 패치  \n",
    "        - 각 패치를 768차원 벡터로 변환(Conv2d: kernel=16, stride=16)\n",
    "        - [CLS] 토큰을 맨 앞에 추가 \n",
    "        - 학습 가능한 위치 임베딩을 더함\n",
    "     3. Transformer Encoder\n",
    "        - 여러 개의 멀티헤드 셀프 어텐션 + FFN 블록 통과\n",
    "        - 모든 패치가 서로를 직접 참조\n",
    "        - CNN처럼 국소적이 아니라 처음부터 전역 관계 학습\n",
    "        - [CLS] 토큰이 중요한 패치 정보들을 흡수\n",
    "     4. 분류 헤드 (MLP)\n",
    "        - 최종 [CLS] 토큰만 사용\n",
    "        - MLP(Linear layers)에 통과\n",
    "        - 1000차원 logits 출력 (ImageNet 기준)\n",
    "        - 가장 큰 값 → 예측 클래스\n",
    "\n",
    "<br>\n",
    "\n",
    "<span style=\"color: Gold\">\n",
    "\n",
    "**=> ViT는 이미지를 “패치 시퀀스”로 바꿔 Transformer로 전역 관계를 학습하고, [CLS] 토큰 하나로 전체 이미지를 분류**  \n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "2d8a09e0",
   "metadata": {},
   "source": [
    "<span style=\"color: lightblue;\">\n",
    "<span style=\"font-size:12px;\">\n",
    "\n",
    "CNN은 “위치는 중요하고, 관계는 천천히 배운다”    \n",
    "ViT는 “관계가 중요하고, 위치는 배워서 안다”  \n",
    "\n",
    "<br>\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e521161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c88b903",
   "metadata": {},
   "source": [
    "<span style=\"color: Gold\"> 1. 패치분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb4f9d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 이미지 크기 : 224 x 224\n",
      " 패치 크기 : 16 x 16\n",
      " 채널 수 : 3\n",
      " 패치 수 : 14 x14\n",
      " 더미 이미지 생성\n",
      " 입력 이미지 shape : torch.Size([1, 3, 224, 224])\n",
      " \n",
      " 패치 임베딩 후\n",
      " conv2d 출력 shape : torch.Size([1, 768, 14, 14])\n",
      " Flatten 후 shape : torch.Size([1, 196, 768])\n",
      " \n",
      " 패치 수  : 196\n",
      " 각 패치의 임베딩 차원 수  : 768\n"
     ]
    }
   ],
   "source": [
    "def patch_embedding():\n",
    "    '''이미지를 패치로 분할하는 과정(patch embedding)'''\n",
    "    # 설정\n",
    "    image_size = 224\n",
    "    patch_size = 16\n",
    "    channels = 3\n",
    "    embedding_dim = 768\n",
    "\n",
    "    # 패치수 계산\n",
    "    num_patchs = (image_size // patch_size) **2\n",
    "    print(f' 이미지 크기 : {image_size} x {image_size }')\n",
    "    print(f' 패치 크기 : {patch_size} x {patch_size }')\n",
    "    print(f' 채널 수 : {channels}')\n",
    "    print(f' 패치 수 : {image_size // patch_size} x{image_size // patch_size}')\n",
    "\n",
    "    # 더미 이미지 생성\n",
    "    dummy_image = torch.randn(1,channels, image_size, image_size)\n",
    "    print(f' 더미 이미지 생성')\n",
    "    print(f' 입력 이미지 shape : {dummy_image.shape}') # [1,3,224,224]\n",
    "\n",
    "    # 패치분할(conv2 사용)\n",
    "    # conv2 stride = patch_size 겹치지 않는 패치 추출 / # stride = 커널이 한 번에 몇 칸씩 이동하느냐\n",
    "    patch_embed = nn.Conv2d(in_channels=channels, out_channels=embedding_dim, kernel_size=patch_size, stride=patch_size) # 입력하는 채널의 수, 출력하는 채널의 수, kernel_size -> 몇개씩 도장을 찍느냐, stride -> 이동 간격\n",
    "\n",
    "    # 패치 임베딩 적용\n",
    "    patches = patch_embed(dummy_image)\n",
    "    print(f' \\n 패치 임베딩 후')\n",
    "    print(f' conv2d 출력 shape : {patches.shape}') # [1, 768, 14, 14]\n",
    "\n",
    "    # Flatten : (배치사이즈(B), 임베딩 차이(D), 이미지 높이(H), 이미지 넓이(W)) -> (B, 196(HxW), D) -> (1, 196, 768)\n",
    "    patches_flat = patches.flatten(2).transpose(1,2)\n",
    "    print(f' Flatten 후 shape : {patches_flat.shape}') # [1, 196, 768]\n",
    "\n",
    "    # 각 패치는 768차원 벡터\n",
    "    print(f' \\n 패치 수  : {patches_flat.shape[1]}')\n",
    "    print(f' 각 패치의 임베딩 차원 수  : {patches_flat.shape[2]}')\n",
    "    return patches_flat\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    patch_embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5098b21",
   "metadata": {},
   "source": [
    "<span style=\"color: Gold\"> 2. 위치 임베딩의 역할\n",
    "- 트랜스포머는 전체를 보며, 인접 유사성은 학습 효율을 위한 정렬!  \n",
    " 거리를 기반으로 한 학습이 아님"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d943f340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 위치 임베딩 shape : torch.Size([1, 197, 768])\n",
      " 총 위치 수 : 197 (패치 196 + cls 토큰 1개)\n"
     ]
    }
   ],
   "source": [
    "def positional_embedding():\n",
    "    '''위치 임베딩'''\n",
    "    num_patches = 196\n",
    "    embedding_dim = 768\n",
    "\n",
    "    # 위치 임베딩 생성  \n",
    "    # 이 텐서는 학습대상 Optimizer 에 의해 업데이트\n",
    "    position_embedding = nn.Parameter(torch.randn(1, num_patches+1, embedding_dim)) # num_patches+1 -> +1을 하는 이유는 cls 토큰을 추가하기 위해서\n",
    "    print(f' 위치 임베딩 shape : {position_embedding.shape}')\n",
    "    print(f' 총 위치 수 : {num_patches+1} (패치 196 + cls 토큰 1개)')\n",
    "\n",
    "    # 배치 차원 제거 -> 각 위치를 하나의 벡터로 다루기 위해 배치 크기가 1인 형태는 분석 시 불필요\n",
    "    pos_emb = position_embedding.squeeze(0) #squeeze(0)-> 첫번째(배치크기)를 숨긴다\n",
    "\n",
    "    # 코사인 유사도 계산을 위해서 정규화는 필수 -> 코사인 유사도를 하는 이유는 의미의 방향만 비교하려고 사용\n",
    "    # 위치 정보\n",
    "    # → positional embedding으로 해결\n",
    "    # 의미 관계\n",
    "    # → attention + 코사인 유사도(정규화 내적)로 해결\n",
    "    pos_emb_norm = pos_emb/pos_emb.norm(dim=1,keepdim=True) # 단위벡터로 나눔\n",
    "    # 코사인 유사도 행렬\n",
    "    similarity = torch.mm(pos_emb_norm,pos_emb_norm.t())    # [197,197]\n",
    "    # 지금은 학습전이라서 랜덤이지만 학습 후에는 인접 패치까지 유사해짐\n",
    "    return position_embedding\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    positional_embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c40582",
   "metadata": {},
   "source": [
    "<span style=\"font-size:13px;\"> \n",
    "<span style=\"color: lightblue\"> Self-Attention 핵심 정리 </span>\n",
    "\n",
    "\n",
    "**1. 내적(Dot Product)이란?**\n",
    "- 두 벡터가 얼마나 같은 방향을 보는지를 하나의 숫자로 나타낸 것\n",
    "- 의미\n",
    "    - 방향이 비슷할수록 값이 큼\n",
    "    - 직각이면 0\n",
    "    - 반대 방향이면 음수\n",
    "    - 벡터의 크기도 함께 반영됨\n",
    "\n",
    "=> **내적 = 방향 유사도 + 크기 영향**  \n",
    "\n",
    "<br>\n",
    "\n",
    "**2. Attention의 전체 흐름**\n",
    "\n",
    "- 내적(Dot Product) → 관련성\n",
    "- 스케일링(√d)      → 안정성\n",
    "- Softmax          → 선택\n",
    "- 가중합(Σ w·V)    → 정보 조합  \n",
    "\n",
    "<br>\n",
    "\n",
    "**3. 왜 Q · K 가 “관련성”이 되는가**  \n",
    "\n",
    "Attention이 묻는 핵심 질문:\n",
    "> “이 토큰은 다른 어떤 토큰을 참고해야 하나?”\n",
    "\n",
    "이를 수식으로 표현하면:\n",
    "```text\n",
    "Query · Key\n",
    "```\n",
    "\n",
    "* **Query(Q)** : 내가 찾고 있는 방향\n",
    "* **Key(K)**   : 내가 가지고 있는 방향\n",
    "\n",
    "```text\n",
    "방향이 맞음 → 내적 큼 → 주목\n",
    "방향 다름 → 내적 작음 → 무시\n",
    "```\n",
    "\n",
    "=> 그래서 **Q · K = 관련성 점수**  \n",
    "\n",
    "<br>\n",
    "\n",
    "**4. Query / Key / Value의 역할**\n",
    "- Query (Q)\n",
    "> \"나는 무엇을 찾고 있나?\"\n",
    "\n",
    "- Key (K)\n",
    "> \"나는 어떤 특징을 갖고 있나?\"\n",
    "\n",
    "- Value (V)\n",
    "> \"내가 실제로 전달할 정보는 무엇인가?\"\n",
    "\n",
    "=> **Q, K, V는 모두 같은 토큰 임베딩에서 나온 서로 다른 표현**  \n",
    "\n",
    "<br>\n",
    "\n",
    "**5. 벡터 크기(scale)의 의미**\n",
    "\n",
    "* 중요한 특징 → 벡터 크기 큼\n",
    "* 약한 특징 → 벡터 크기 작음\n",
    "\n",
    "하지만 차원(d)이 커질수록:\n",
    "\n",
    "```text\n",
    "Q · K 값이 과도하게 커짐\n",
    "→ softmax가 한쪽으로 쏠림\n",
    "→ gradient 소실 / 폭주\n",
    "```\n",
    "\n",
    "그래서 사용:\n",
    "\n",
    "```text\n",
    "(Q · K) / √d\n",
    "```\n",
    "\n",
    "=> **스케일링 = 학습 안정성 확보**  \n",
    "\n",
    "<br>\n",
    "\n",
    "**6. Softmax의 역할**\n",
    "\n",
    "> **관련성 점수를 확률 분포로 변환하는 단계**\n",
    "\n",
    "* 모든 값의 합 = 1\n",
    "* 상대적 중요도만 남김\n",
    "\n",
    "=> 여러 후보 중 **무엇을 얼마나 참고할지 선택**  \n",
    "\n",
    "<br>\n",
    "\n",
    "**7. 최종 Attention 계산 요약**\n",
    "\n",
    "```text\n",
    "Q · K          → 누구를 볼지 결정\n",
    "softmax       → 얼마나 볼지 결정\n",
    "Σ(weight × V) → 실제 정보 조합  \n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "**8. 핵심 구조 요약 (반드시 기억)**\n",
    "\n",
    "```text\n",
    "Query · Key  → 관계 판단용\n",
    "Value        → 정보 전달용\n",
    "```\n",
    "\n",
    "=> **판단과 내용은 분리돼야 한다**  \n",
    "  \n",
    "<br>\n",
    "\n",
    "**9. 한 문장 요약**\n",
    "\n",
    "> **Self-Attention은\n",
    "> “누가 중요한지를 판단(Q·K)하고,\n",
    "> 그 정보(Value)를 비율대로 섞는 메커니즘이다.”**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ea1a00",
   "metadata": {},
   "source": [
    "<span style=\"color: Gold\"> 3. CLS 토큰\n",
    "- 토큰이 여러개가 아닌 하나의 CLS를 expend(view)로 공유한다 \n",
    "-> 하나의 파라미터를 모든 배치가 공유하며 gradient는 전부 합쳐서 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fa6632a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CLS 토큰 shape : torch.Size([1, 1, 768])\n",
      " 배치 확장 shape : torch.Size([2, 1, 768])\n",
      " 패치 임베딩 shape : torch.Size([2, 196, 768])\n",
      "결합후shape : torch.Size([2, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "def cls_token():\n",
    "    embedding_dim = 768\n",
    "    num_paches = 196\n",
    "    batch_size = 2\n",
    "    \n",
    "    # cls 토큰 생성\n",
    "    cls_token = nn.Parameter(torch.rand(1,1,embedding_dim)) # 학습을 통해 이미지를 요약하는 벡터로 진화\n",
    "    print(f' CLS 토큰 shape : {cls_token.shape}')   # [1,1,768]\n",
    "    # 배치 크기에 맞게 CLS 토큰 확장\n",
    "    cls_tokens = cls_token.expand(batch_size,-1,-1) # 배치마다 동일한 cls 토큰이 필요, expand는 view만 확장\n",
    "    print(f' 배치 확장 shape : {cls_tokens.shape}')   # [2,1,768]\n",
    "    # 패치 임베딩 생성\n",
    "    patch_embeddings = torch.randn(batch_size, num_paches, embedding_dim)\n",
    "    print(f' 패치 임베딩 shape : {patch_embeddings.shape}')\n",
    "    # CLS 토큰을 시퀀스 맨 앞에 추가\n",
    "    embeddings = torch.cat([cls_tokens,patch_embeddings], dim=1)    # dim=1 -> 토큰 차원 / [cls, patch1 ,... patch 196]\n",
    "    print(f'결합후shape : {embeddings.shape}') # [2,197,768]\n",
    "    return embeddings\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    cls_token()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ec36b0",
   "metadata": {},
   "source": [
    "<span style=\"color: Gold\"> 4. self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a5a202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " QKV shape : torch.Size([1, 197, 2304])\n",
      " Q shape : torch.Size([1, 12, 197, 64])\n",
      " K shape : torch.Size([1, 12, 197, 64])\n",
      " V shape : torch.Size([1, 12, 197, 64])\n",
      "\n",
      "[Attention Score]\n",
      "  Attention shape: torch.Size([1, 12, 197, 197])\n",
      "  Softmax 후 합계 (각 행): 1.0000\n",
      "\n",
      "[Attention 적용]\n",
      "  출력 shape: torch.Size([1, 12, 197, 64])\n",
      "  헤드 결합 후: torch.Size([1, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "def self_attention():\n",
    "    '''self_aattention 메커니즘\n",
    "    Mult-Head self-attention의 구성요소\n",
    "    Q(query)\n",
    "    K(key)\n",
    "    V(value)\n",
    "    '''\n",
    "    embedding_dim = 768\n",
    "    num_heads = 12\n",
    "    head_dim = embedding_dim//num_heads # 각 헤드가 담당하는차원\n",
    "    selq_len = 197 # 196패치 +1 cls\n",
    "    batch_size = 1  # 샘플1개\n",
    "    # 더미데이터 생성\n",
    "    x = torch.randn(batch_size,selq_len,embedding_dim) # [1,197,768]\n",
    "    # QKV 선형 레이어\n",
    "    qkv_proj = nn.Linear(embedding_dim,embedding_dim*3) # 하나의 Linear 연산으로 QKV 동시에 생성\n",
    "    # QKV 계산\n",
    "    qkv = qkv_proj(x)\n",
    "    print(f' QKV shape : {qkv.shape}')  # [1, 197, 2304]\n",
    "    # Q K V \n",
    "    qkv = qkv.reshape(batch_size, selq_len,3, num_heads, head_dim) # [B, N, 3, heads, head_dim] / B: 배치크기, N: 토큰수(CLS+패치)\n",
    "    qkv = qkv.permute(2,0,3,1,4) # [3,B,heads,N,head_dim]    # 계산하기 쉽게 q,k,v만 앞으로 꺼내자 / permute-> 축을 바꾸는 연산\n",
    "    q, k , v = qkv[0], qkv[1], qkv[2]\n",
    "    print(f' Q shape : {q.shape}')\n",
    "    print(f' K shape : {k.shape}')\n",
    "    print(f' V shape : {v.shape}')\n",
    "\n",
    "\n",
    "# Attention Score 계산\n",
    "    scale = head_dim ** -0.5\n",
    "    attn = (q @ k.transpose(-2, -1)) * scale\n",
    "    # Q : [B, h, N, d]\n",
    "    # K : [B, h, N, d] -> Kᵀ : [B, h, d, N]\n",
    "\n",
    "    # Q  : (..., N, d)\n",
    "    # Kᵀ : (..., d, N)\n",
    "    # ------------------\n",
    "    # 결과: (..., N, N)\n",
    "    # -> d(head_dim)가 사라지는 이유는 d는 비교를 위한 공간이기 때문에 비교가 끝나면 점수만 남는다 => Q @ Kᵀ의 결과는 토큰×토큰 = (N, N)\n",
    "    print(f\"\\n[Attention Score]\")\n",
    "    print(f\"  Attention shape: {attn.shape}\")  # [1, 12, 197, 197]\n",
    "    \n",
    "    # Softmax\n",
    "    attn = attn.softmax(dim=-1)\n",
    "    print(f\"  Softmax 후 합계 (각 행): {attn[0, 0, 0].sum().item():.4f}\")  # 1.0\n",
    "    \n",
    "    # Value와 곱하기\n",
    "    out = attn @ v\n",
    "    print(f\"\\n[Attention 적용]\")\n",
    "    print(f\"  출력 shape: {out.shape}\")  # [1, 12, 197, 64]\n",
    "    \n",
    "    # 헤드 결합 \n",
    "    # 원래 형태 =>  out.shape == [B=1, heads=12, N=197, head_dim=64]\n",
    "    # 최종 원하는 형태 =>  [B, N, embedding_dim]  # embedding_dim = heads * head_dim = 768\n",
    "    out = out.transpose(1, 2).reshape(batch_size, selq_len, embedding_dim)\n",
    "    print(f\"  헤드 결합 후: {out.shape}\")  # [1, 197, 768]\n",
    "    \n",
    "    return attn\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    self_attention()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e02bc21",
   "metadata": {},
   "source": [
    "<span style=\"font-size:13px;\"> \n",
    "<span style=\"color: lightblue\"> Multi-Head Self-Attention 요약 </span>  \n",
    "\n",
    "<br>\n",
    "\n",
    "1. 핵심 문장\n",
    "**Multi-Head Self-Attention은 하나의 시선으로는 놓치는 관계를 여러 관점에서 동시에 보게 만드는 장치다.**  \n",
    "<br>\n",
    "\n",
    "2. Single-Head Attention이 부족한 이유\n",
    "\n",
    "- Single-Head는:\n",
    "    - 하나의 관계 기준, 하나의 유사도 공간, 하나의 시선\n",
    "    - 이 모든 정보를 **하나의 벡터 공간**에서 처리되어, 관계가 섞이고, 특정 패턴이 묻힌다  \n",
    "\n",
    "    - 이미지는 동시에 여러 기준이 중요하다:\n",
    "        - 색깔 (Color)\n",
    "        - 형태 (Shape)\n",
    "        - 위치 (Position)\n",
    "        - 질감 (Texture)\n",
    "\n",
    "<br>\n",
    "\n",
    "3. Multi-Head의 핵심 아이디어\n",
    "\n",
    "> **같은 입력을  \n",
    "> “차원을 나눠서” 여러 시선으로 본다**  \n",
    "\n",
    "<br>\n",
    "\n",
    "4.  차원을 나눈다는 의미\n",
    "\n",
    "예시:\n",
    "- 전체 임베딩 차원: 768\n",
    "- Head 수: 12\n",
    "\n",
    "768 = 12 heads × 64 dim\n",
    "\n",
    "yaml\n",
    "코드 복사\n",
    "\n",
    "각 head는:\n",
    "- 서로 다른 Q, K, V 투영\n",
    "- 서로 다른 관계 패턴에 집중\n",
    "\n",
    "👉 결과적으로:\n",
    "- 어떤 head는 형태\n",
    "- 어떤 head는 색 대비\n",
    "- 어떤 head는 위치 관계\n",
    "- 어떤 head는 질감\n",
    "\n",
    "을 본다.  \n",
    "\n",
    "<br>\n",
    "\n",
    "5. 왜 768 차원을 하나로 쓰면 안 되나?\n",
    "\n",
    "[768 차원 단일 공간]\n",
    "\n",
    "yaml\n",
    "코드 복사\n",
    "\n",
    "이 경우:\n",
    "- 모든 관계가 섞임\n",
    "- 평균화 발생\n",
    "- 미묘한 패턴이 사라짐\n",
    "\n",
    "👉 **표현력 감소**  \n",
    "\n",
    "<br>\n",
    "\n",
    "6. 왜 다시 합치는가? (Concat + Linear)\n",
    "\n",
    "각 head는:\n",
    "- 부분적인 관점\n",
    "- 불완전한 정보\n",
    "\n",
    "그래서 마지막에:\n",
    "\n",
    "[Head1 | Head2 | ... | HeadN]\n",
    "↓\n",
    "Linear\n",
    "↓\n",
    "하나의 의미 공간\n",
    "\n",
    "yaml\n",
    "코드 복사\n",
    "\n",
    "👉 여러 관점을 **통합된 표현**으로 재조합  \n",
    "\n",
    "<br>\n",
    "\n",
    "7. 구조 한 줄 요약\n",
    "\n",
    "관계 분해 (Multi-Head)\n",
    "→ 관점별 이해\n",
    "→ 의미 통합 (Linear)\n",
    "\n",
    "yaml\n",
    "코드 복사  \n",
    "\n",
    "<br>\n",
    "\n",
    "8. 최종 요약 문장\n",
    "\n",
    "> **Multi-Head Self-Attention은  \n",
    "> 관계를 쪼개서 보고,  \n",
    "> 다시 하나의 의미로 합치는 구조다.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3209ece8",
   "metadata": {},
   "source": [
    "<span style=\"color: Gold\"> 5. Multi-Layer Perceptron </span>   \n",
    "- Attention이 모은 정보를 다시 가공하는 단계\n",
    "- Attention은 “어디를 볼지” 결정하고, MLP는 “본 걸 어떻게 해석할지” 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "793e0bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp():\n",
    "    '''mlp 블럭'''\n",
    "    embedding_dim = 768\n",
    "    mlp_dim = embedding_dim*4 # 일반적으로 4배 확장\n",
    "    print(f' 입력/ 출력 차원 : {embedding_dim}')\n",
    "    print(f'히든차원 : {mlp_dim}')\n",
    "    # MLP 블럭 정의\n",
    "    mlp = nn.Sequential(\n",
    "        nn.Linear(embedding_dim,mlp_dim),\n",
    "        nn.GELU(), # 단순선형을 비선형으로 변형\n",
    "        nn.Linear(mlp_dim,embedding_dim)\n",
    "    )\n",
    "    \n",
    "     # 파라미터 수 계산\n",
    "    total_params = sum(p.numel() for p in mlp.parameters())\n",
    "    print(f\"  MLP 파라미터 수: {total_params:,}\")\n",
    "    \n",
    "    # 더미 입력\n",
    "    x = torch.randn(1, 197, embedding_dim)\n",
    "    print(f\"\\n[입력/출력]\")\n",
    "    print(f\"  입력 shape: {x.shape}\")\n",
    "    \n",
    "    # MLP 적용\n",
    "    out = mlp(x)\n",
    "    print(f\"  출력 shape: {out.shape}\")\n",
    "    \n",
    "    print(f\"\\n[GELU 활성화 함수]\")\n",
    "    print(f\"  GELU(x) = x * Phi(x)\")\n",
    "    print(f\"  ReLU보다 부드럽고, 음수 입력에도 작은 값 출력\")\n",
    "    \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d02d79",
   "metadata": {},
   "source": [
    "<span style=\"color: Gold\"> 6. transformer encoder block 구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cc12bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_block():\n",
    "    '''Transformer Encoder 블럭'''\n",
    "    class TransformerBlock(nn.Module):\n",
    "        \"\"\"간단한 Transformer Block 구현\"\"\"\n",
    "        def __init__(self, dim=768, num_heads=12, mlp_ratio=4.0):\n",
    "            super().__init__()\n",
    "            self.norm1 = nn.LayerNorm(dim)\n",
    "            self.attn = nn.MultiheadAttention(dim, num_heads, batch_first=True)\n",
    "            self.norm2 = nn.LayerNorm(dim)\n",
    "            self.mlp = nn.Sequential(\n",
    "                nn.Linear(dim, int(dim * mlp_ratio)),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(int(dim * mlp_ratio), dim),\n",
    "            )\n",
    "        \n",
    "        def forward(self, x):\n",
    "            # Pre-norm 구조\n",
    "            # Attention with residual\n",
    "            x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n",
    "            # MLP with residual\n",
    "            x = x + self.mlp(self.norm2(x))\n",
    "            return x\n",
    "    \n",
    "    # 블록 생성\n",
    "    block = TransformerBlock()\n",
    "    \n",
    "    # 파라미터 수\n",
    "    total_params = sum(p.numel() for p in block.parameters())\n",
    "    print(f\"\\n[Transformer Block 구성]\")\n",
    "    print(f\"  1. Layer Normalization\")\n",
    "    print(f\"  2. Multi-Head Self-Attention\")\n",
    "    print(f\"  3. Residual Connection\")\n",
    "    print(f\"  4. Layer Normalization\")\n",
    "    print(f\"  5. MLP (Feed-Forward)\")\n",
    "    print(f\"  6. Residual Connection\")\n",
    "    print(f\"\\n  블록당 파라미터 수: {total_params:,}\")\n",
    "    \n",
    "    # 더미 입력\n",
    "    x = torch.randn(1, 197, 768)\n",
    "    print(f\"\\n[입력/출력]\")\n",
    "    print(f\"  입력 shape: {x.shape}\")\n",
    "    \n",
    "    out = block(x)\n",
    "    print(f\"  출력 shape: {out.shape}\")\n",
    "    \n",
    "    # ViT-Base는 12개 블록\n",
    "    print(f\"\\n[ViT-Base 전체 파라미터]\")\n",
    "    print(f\"  12개 블록 파라미터: {total_params * 12:,}\")\n",
    "    \n",
    "    return block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7e615b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 이미지 크기 : 224 x 224\n",
      " 패치 크기 : 16 x 16\n",
      " 채널 수 : 3\n",
      " 패치 수 : 14 x14\n",
      " 더미 이미지 생성\n",
      " 입력 이미지 shape : torch.Size([1, 3, 224, 224])\n",
      " \n",
      " 패치 임베딩 후\n",
      " conv2d 출력 shape : torch.Size([1, 768, 14, 14])\n",
      " Flatten 후 shape : torch.Size([1, 196, 768])\n",
      " \n",
      " 패치 수  : 196\n",
      " 각 패치의 임베딩 차원 수  : 768\n",
      " 위치 임베딩 shape : torch.Size([1, 197, 768])\n",
      " 총 위치 수 : 197 (패치 196 + cls 토큰 1개)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CLS 토큰 shape : torch.Size([1, 1, 768])\n",
      " 배치 확장 shape : torch.Size([2, 1, 768])\n",
      " 패치 임베딩 shape : torch.Size([2, 196, 768])\n",
      "결합후shape : torch.Size([2, 197, 768])\n",
      " QKV shape : torch.Size([1, 197, 2304])\n",
      " Q shape : torch.Size([1, 12, 197, 64])\n",
      " K shape : torch.Size([1, 12, 197, 64])\n",
      " V shape : torch.Size([1, 12, 197, 64])\n",
      "\n",
      "[Attention Score]\n",
      "  Attention shape: torch.Size([1, 12, 197, 197])\n",
      "  Softmax 후 합계 (각 행): 1.0000\n",
      "\n",
      "[Attention 적용]\n",
      "  출력 shape: torch.Size([1, 12, 197, 64])\n",
      "  헤드 결합 후: torch.Size([1, 197, 768])\n",
      " 입력/ 출력 차원 : 768\n",
      "히든차원 : 3072\n",
      "  MLP 파라미터 수: 4,722,432\n",
      "\n",
      "[입력/출력]\n",
      "  입력 shape: torch.Size([1, 197, 768])\n",
      "  출력 shape: torch.Size([1, 197, 768])\n",
      "\n",
      "[GELU 활성화 함수]\n",
      "  GELU(x) = x * Phi(x)\n",
      "  ReLU보다 부드럽고, 음수 입력에도 작은 값 출력\n",
      "\n",
      "[Transformer Block 구성]\n",
      "  1. Layer Normalization\n",
      "  2. Multi-Head Self-Attention\n",
      "  3. Residual Connection\n",
      "  4. Layer Normalization\n",
      "  5. MLP (Feed-Forward)\n",
      "  6. Residual Connection\n",
      "\n",
      "  블록당 파라미터 수: 7,087,872\n",
      "\n",
      "[입력/출력]\n",
      "  입력 shape: torch.Size([1, 197, 768])\n",
      "  출력 shape: torch.Size([1, 197, 768])\n",
      "\n",
      "[ViT-Base 전체 파라미터]\n",
      "  12개 블록 파라미터: 85,054,464\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # 1. 패치임베딩\n",
    "    patcheds = patch_embedding()\n",
    "    # 2. 위치 임베딩\n",
    "    pos_embed = positional_embedding()\n",
    "    # 3. CLS 토큰\n",
    "    embeddings = cls_token()\n",
    "    # 4. self_attention\n",
    "    attention = self_attention()\n",
    "    # 5. mlp\n",
    "    mlp_output = mlp()\n",
    "    # 6. transformer block\n",
    "    block = transformer_block()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7068d9",
   "metadata": {},
   "source": [
    "시각화(더미데이터)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc323f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " QKV shape : torch.Size([1, 197, 2304])\n",
      " Q shape : torch.Size([1, 12, 197, 64])\n",
      " K shape : torch.Size([1, 12, 197, 64])\n",
      " V shape : torch.Size([1, 12, 197, 64])\n",
      "\n",
      "[Attention Score]\n",
      "  Attention shape: torch.Size([1, 12, 197, 197])\n",
      "  Softmax 후 합계 (각 행): 1.0000\n",
      "\n",
      "[Attention 적용]\n",
      "  출력 shape: torch.Size([1, 12, 197, 64])\n",
      "  헤드 결합 후: torch.Size([1, 197, 768])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def _visualization_cls_attention(atten, image_size=224, patch_size=16):\n",
    "    '''cls 토큰이 각 패치를 얼마나 주목하는지 시각화\n",
    "    attention : [1, heads, 197, 197]\n",
    "    '''\n",
    "    # cls - > patch attention만 추출\n",
    "    cls_atten = atten[0, :, 0, 1:]  # [heads, 196]\n",
    "    # head 평균\n",
    "    cls_atten_mean = cls_atten.mean(dim = 0)  # [196]\n",
    "    # 14 x 14 reshape\n",
    "    grid_size = image_size // patch_size\n",
    "    atten_map = cls_atten_mean.reshape(grid_size, grid_size)\n",
    "    atten_map = atten_map/ atten_map.max()\n",
    "    return atten_map.detach().cpu().numpy()\n",
    "\n",
    "# 더미이미지 + attention 시각화\n",
    "def demo_cls_attention_visualization():\n",
    "    # 1. 더미 이미지\n",
    "    image = np.random.rand(224,224,3)\n",
    "    # self_attention 계산\n",
    "    atten = self_attention()\n",
    "    # 3. attention map 생성\n",
    "    atten_map = _visualization_cls_attention(atten)\n",
    "    # 시각화\n",
    "    plt.imshow(image)\n",
    "    plt.imshow(atten_map, cmap = 'jet', alpha=0.5)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "    demo_cls_attention_visualization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0ef89b",
   "metadata": {},
   "source": [
    "시각화(실제이미지)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a695ffc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " QKV shape : torch.Size([1, 197, 2304])\n",
      " Q shape : torch.Size([1, 12, 197, 64])\n",
      " K shape : torch.Size([1, 12, 197, 64])\n",
      " V shape : torch.Size([1, 12, 197, 64])\n",
      "\n",
      "[Attention Score]\n",
      "  Attention shape: torch.Size([1, 12, 197, 197])\n",
      "  Softmax 후 합계 (각 행): 1.0000\n",
      "\n",
      "[Attention 적용]\n",
      "  출력 shape: torch.Size([1, 12, 197, 64])\n",
      "  헤드 결합 후: torch.Size([1, 197, 768])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def _visualization_cls_attention(atten, image_size=224, patch_size=16):\n",
    "    '''cls 토큰이 각 패치를 얼마나 주목하는지 시각화\n",
    "    attention : [1, heads, 197, 197]\n",
    "    '''\n",
    "    # cls - > patch attention만 추출\n",
    "    cls_atten = atten[0, :, 0, 1:]  # [heads, 196]\n",
    "    # head 평균\n",
    "    cls_atten_mean = cls_atten.mean(dim = 0)  # [196]\n",
    "    # 14 x 14 reshape\n",
    "    grid_size = image_size // patch_size\n",
    "    atten_map = cls_atten_mean.reshape(grid_size, grid_size)\n",
    "    atten_map = atten_map/ atten_map.max()\n",
    "    return atten_map.detach().cpu().numpy()\n",
    "\n",
    "# 더미이미지 + attention 시각화\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "def demo_cls_attention_visualization(is_dummy:bool = True, url:str = None,image_size:str = 224):\n",
    "    # 1. 더미 이미지\n",
    "    if is_dummy :\n",
    "        image = np.random.rand(224,224,3)\n",
    "    elif url.startswith('http'):\n",
    "        response = requests.get(url)\n",
    "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "        image = image.resize((image_size, image_size))\n",
    "        image = np.array(image) / 255.0  # 정규화\n",
    "    else:  # 로컬 이미지 로드\n",
    "        image = Image.open(url).convert('RGB')\n",
    "        image = image.resize((image_size, image_size))\n",
    "        image = np.array(image) / 255.0  # 정규화\n",
    "\n",
    "    # self_attention 계산\n",
    "    atten = self_attention()\n",
    "    # 3. attention map 생성\n",
    "    atten_map = _visualization_cls_attention(atten)\n",
    "    # 시각화\n",
    "    plt.imshow(image)\n",
    "    plt.imshow(atten_map, cmap = 'jet', alpha=0.5)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "    url = 'C:/python_src/7.Multimodal/Cat03.jpg'\n",
    "    demo_cls_attention_visualization(False, url=url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
