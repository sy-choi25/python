{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b48a2f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7d48457dfcc48f2a28d2bba6a643c04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "\n",
    "# 모델 이미지 프로세스 로드\n",
    "model_name = 'google/vit-base-patch16-224'\n",
    "image_processor = AutoImageProcessor.from_pretrained(\n",
    "    model_name,\n",
    "    use_fast = True\n",
    ")\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    model_name,\n",
    "    device_map = 'auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8cdb62ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미지 크기: (960, 686)\n",
      "전처리 후 텐서의 크기 : torch.Size([1, 3, 224, 224])\n",
      "예측결과: ....\n",
      "클래스 ID: 287\n",
      "클래스 라벨: lynx, catamount\n",
      "확률(신뢰도): 0.44071435928344727\n",
      "    1. lynx, catamount : 0.44\n",
      "    2. cougar, puma, catamount, mountain lion, painter, panther, Felis concolor : 0.03\n",
      "    3. snow leopard, ounce, Panthera uncia : 0.03\n",
      "    4. Egyptian cat : 0.02\n",
      "    5. tiger cat : 0.02\n",
      "이미지 크기: (640, 480)\n",
      "전처리 후 텐서의 크기 : torch.Size([1, 3, 224, 224])\n",
      "예측결과: ....\n",
      "클래스 ID: 285\n",
      "클래스 라벨: Egyptian cat\n",
      "확률(신뢰도): 0.9374890327453613\n",
      "    1. Egyptian cat : 0.94\n",
      "    2. tabby, tabby cat : 0.04\n",
      "    3. tiger cat : 0.01\n",
      "    4. lynx, catamount : 0.00\n",
      "    5. Siamese cat, Siamese : 0.00\n"
     ]
    }
   ],
   "source": [
    "# 이미지 로드\n",
    "\n",
    "image_urls = [\n",
    "        \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\",\n",
    "        \"http://images.cocodataset.org/val2017/000000039769.jpg\",\n",
    "    ]\n",
    "\n",
    "for idx, url in enumerate(image_urls,1):\n",
    "    try:\n",
    "        # 이미지다운로드\n",
    "        image = Image.open(requests.get(url, stream = True).raw)    # PIL.Image.open(...) → 이미지를 메모리 내 열기/ requests.get(...).raw → 인터넷에서 이미지 파일 스트림을 가져옴\n",
    "        print(f'이미지 크기: {image.size}')\n",
    "        # 이미지 전처리 \n",
    "        inputs = image_processor(image, return_tensors = 'pt').to(model.device) # 이미지의 전처리가 필요한 이유 - 모델에 입력되는 건 이미지가 아닌 텐서\n",
    "        # 크기 통일 (예: 224×224)\n",
    "        # 정규화 (0~1)\n",
    "        # 채널 배치 (C×H×W)\n",
    "        # 배치 차원 추가 (1×3×224×224)\n",
    "        print(f\"전처리 후 텐서의 크기 : {inputs['pixel_values'].shape}\")\n",
    "        # 추론\n",
    "        with torch.no_grad():   # 추론(inference)일 때 gradient 계산 비활성화 (속도↑ 메모리↓)/ gradient(기울기)를 계산하지 않는 모드로 바꾼다/ 학습이 아니라 결과만 낼 때는 역전파가 필요없으니 메모리와 속도를 아끼는 것. \" 모델 읽기 전용 모드\"라고 생각\n",
    "            outputs = model(**inputs)   # 이 과정에서 이루어 지는 것 -> 시각적 특징을 파악하고 어느 클래스일 가능성이 높은지 계산하는 단계\n",
    "                                            # 이미지 패치 쪼개기\n",
    "                                            # 패치를 linear projection\n",
    "                                            # 여러 층의 Transformer 인코더 통해 feature 추출\n",
    "                                            # 마지막에 classifier head 적용\n",
    "                                            # class별 점수(logit) 생성\n",
    "            logits = outputs.logits # 모델이 각 클래스 별로 준 원시 점수. 여기서 argmax하면 가장 높은 점수를 가진 클래스가 예측 결과가 된다\n",
    "        # 결과 해석\n",
    "        predicted_class_id = logits.argmax(dim=-1).item()\n",
    "        predicted_class_label = model.config.id2label[predicted_class_id]\n",
    "        confidence = torch.softmax(logits,dim = 1)[0][predicted_class_id].item()\n",
    "        print('예측결과: ....')\n",
    "        print(f'클래스 ID: {predicted_class_id}')\n",
    "        print(f'클래스 라벨: {predicted_class_label}')\n",
    "        print(f'확률(신뢰도): {confidence}')\n",
    "        # top5 예측 결과\n",
    "        probs = torch.softmax(logits,dim = -1)[0]\n",
    "        top5_probs, top5_indices = torch.topk(probs, 5)\n",
    "        for i, (prob, idx) in enumerate(zip(top5_probs,top5_indices),1):\n",
    "            label = model.config.id2label[idx.item()]\n",
    "            print(f'    {i}. {label} : {prob.item():.2f}')\n",
    "    except Exception as e:\n",
    "        print(f'오류발생 : {e}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
