{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93e3b4f3-71e2-466c-9843-24594e0ded4f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: xformers==0.0.27.post2 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (0.0.27.post2)\n",
      "Requirement already satisfied: numpy in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from xformers==0.0.27.post2) (2.2.5)\n",
      "Requirement already satisfied: torch==2.4.0 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from xformers==0.0.27.post2) (2.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from torch==2.4.0->xformers==0.0.27.post2) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from torch==2.4.0->xformers==0.0.27.post2) (4.15.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from torch==2.4.0->xformers==0.0.27.post2) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from torch==2.4.0->xformers==0.0.27.post2) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from torch==2.4.0->xformers==0.0.27.post2) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from torch==2.4.0->xformers==0.0.27.post2) (2025.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from jinja2->torch==2.4.0->xformers==0.0.27.post2) (3.0.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from sympy->torch==2.4.0->xformers==0.0.27.post2) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 필수 라이브러리 설치\n",
    "%pip install -q diffusers transformers accelerate peft bitsandbytes\n",
    "%pip install xformers==0.0.27.post2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c24f1f6-94a8-4af7-b26c-91f396d77d08",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/usr/local/lib/python3.11/dist-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n",
      "Keyword arguments {'dtype': torch.float16} are not expected by StableDiffusionXLPipeline and will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e388611b2dc4f13a60973e0aef402c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a88336639fa6419ab7a6e2a7a1cb20ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 모델로드 float32   # 1\n",
    "# VAE dtype  float32    # 이미지를 인코딩 디코딩 하는 역활  # 2\n",
    "# Mixed Precision 비활성화  AMP\n",
    "# Learning_rate 1e-5\n",
    "# BatchSize 1\n",
    "# LoRa dropout 0.0\n",
    "# Loss계산 float32\n",
    "#  Gradient Clipping 추가\n",
    "#  nan 체크 추가\n",
    "\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from diffusers import StableDiffusionXLPipeline, DDPMScheduler\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# ================== 설정 ==================\n",
    "DATASET_NAME = \"lambdalabs/naruto-blip-captions\"\n",
    "MODEL_NAME = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "OUTPUT_DIR = \"./sdxl-lora-output\"\n",
    "RESOLUTION = 512\n",
    "BATCH_SIZE = 1          # 수정: 1로 시작\n",
    "LEARNING_RATE = 1e-5    # 수정: 1e-8은 너무 낮음\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "EPOCHS = 3\n",
    "MAX_GRAD_NORM = 1.0\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ================== 모델 로드 ==================\n",
    "print(\"모델 로딩 중...\")\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch. float32,  # 수정: float32로 로드  # 1\n",
    "    use_safetensors=True,\n",
    ")\n",
    "\n",
    "# ================== LoRA 설정 및 적용 ==================\n",
    "print(\"LoRA 설정 적용 중...\")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"to_q\", \"to_k\", \"to_v\", \"to_out. 0\",\n",
    "    ],\n",
    "    lora_dropout=0.0,  # 수정: dropout 제거 (안정성)\n",
    ")\n",
    "\n",
    "unet = pipe.unet\n",
    "unet = get_peft_model(unet, lora_config)\n",
    "unet.print_trainable_parameters()\n",
    "\n",
    "# ================== 메모리 최적화 ==================\n",
    "print(\"메모리 최적화 설정 중...\")\n",
    "\n",
    "try:\n",
    "    pipe.enable_xformers_memory_efficient_attention()\n",
    "    print(\"xformers 활성화 완료\")\n",
    "except Exception as e: \n",
    "    print(f\"xformers 사용 불가:  {e}\")\n",
    "    pipe.enable_attention_slicing()\n",
    "\n",
    "pipe.enable_vae_slicing()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# ================== 모델 배치 ==================\n",
    "print(\"모델 설정 중...\")\n",
    "\n",
    "# VAE는 float32 유지 (안정성)\n",
    "pipe.vae.to(device, dtype=torch.float32)  # 2\n",
    "pipe.text_encoder.to(device)\n",
    "pipe.text_encoder_2.to(device)\n",
    "unet.to(device)\n",
    "\n",
    "# 학습하지 않는 모듈 freeze\n",
    "pipe.vae.requires_grad_(False)\n",
    "pipe.text_encoder.requires_grad_(False)\n",
    "pipe.text_encoder_2.requires_grad_(False)\n",
    "\n",
    "# VAE를 eval 모드로\n",
    "pipe.vae.eval()\n",
    "\n",
    "# ================== SDXL용 헬퍼 함수 ==================\n",
    "def compute_time_ids(original_size, crops_coords_top_left, target_size, device, dtype):\n",
    "    add_time_ids = list(original_size + crops_coords_top_left + target_size)\n",
    "    add_time_ids = torch.tensor([add_time_ids], dtype=dtype, device=device)\n",
    "    return add_time_ids\n",
    "\n",
    "def encode_prompt_sdxl(pipe, prompt, device):\n",
    "    # Text Encoder 1\n",
    "    text_inputs = pipe.tokenizer(\n",
    "        prompt,\n",
    "        padding=\"max_length\",\n",
    "        max_length=pipe.tokenizer.model_max_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    text_input_ids = text_inputs.input_ids. to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        prompt_embeds = pipe. text_encoder(\n",
    "            text_input_ids,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        pooled_prompt_embeds = prompt_embeds[0]\n",
    "        prompt_embeds = prompt_embeds.hidden_states[-2]\n",
    "    \n",
    "    # Text Encoder 2\n",
    "    text_inputs_2 = pipe. tokenizer_2(\n",
    "        prompt,\n",
    "        padding=\"max_length\",\n",
    "        max_length=pipe.tokenizer_2.model_max_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    text_input_ids_2 = text_inputs_2.input_ids.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        prompt_embeds_2 = pipe.text_encoder_2(\n",
    "            text_input_ids_2,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        pooled_prompt_embeds_2 = prompt_embeds_2[0]\n",
    "        prompt_embeds_2 = prompt_embeds_2.hidden_states[-2]\n",
    "    \n",
    "    prompt_embeds = torch.concat([prompt_embeds, prompt_embeds_2], dim=-1)\n",
    "    \n",
    "    return prompt_embeds, pooled_prompt_embeds_2\n",
    "\n",
    "# ================== 데이터셋 로드 ==================\n",
    "print(\"데이터셋 로딩 중...\")\n",
    "dataset = load_dataset(DATASET_NAME, split='train[:100]')\n",
    "print(f'original dataset:  {dataset.column_names}')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((RESOLUTION, RESOLUTION)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "def preprocess(example):\n",
    "    image = example['image'].convert('RGB')\n",
    "    return {\n",
    "        'pixel_values': transform(image),\n",
    "        'caption': example['text']\n",
    "    }\n",
    "\n",
    "dataset = dataset. map(preprocess, remove_columns=dataset.column_names)\n",
    "dataset.set_format(type='torch', columns=['pixel_values', 'caption'])\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# ================== 학습 설정 ==================\n",
    "optimizer = torch.optim. AdamW(\n",
    "    filter(lambda p: p.requires_grad, unet.parameters()),\n",
    "    lr=LEARNING_RATE,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=1e-2,\n",
    "    eps=1e-8\n",
    ")\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(MODEL_NAME, subfolder='scheduler')\n",
    "\n",
    "# Mixed Precision 비활성화 (안정성 우선)\n",
    "use_amp = False\n",
    "\n",
    "# ================== 학습 루프 ==================\n",
    "print(\"LoRA 학습 시작!\")\n",
    "unet.train()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    total_loss = 0\n",
    "    valid_steps = 0\n",
    "\n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        \n",
    "        pixel_values = batch[\"pixel_values\"].to(device, dtype=torch.float32)\n",
    "\n",
    "        # Latent 인코딩 (float32)\n",
    "        with torch.no_grad():\n",
    "            latent_dist = pipe.vae.encode(pixel_values).latent_dist\n",
    "            latents = latent_dist.sample()\n",
    "            latents = latents * pipe.vae. config.scaling_factor\n",
    "            latents = latents.to(dtype=unet.dtype)\n",
    "\n",
    "        # 노이즈 추가\n",
    "        noise = torch.randn_like(latents)\n",
    "        timesteps = torch.randint(\n",
    "            0, noise_scheduler.config.num_train_timesteps,\n",
    "            (latents.shape[0],), device=device\n",
    "        ).long()\n",
    "        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "        # 캡션 처리\n",
    "        captions = batch[\"caption\"]\n",
    "        if isinstance(captions, torch.Tensor):\n",
    "            captions = [str(c) for c in captions]\n",
    "        elif isinstance(captions, str):\n",
    "            captions = [captions]\n",
    "        else: \n",
    "            captions = list(captions)\n",
    "\n",
    "        # SDXL 텍스트 임베딩\n",
    "        prompt_embeds, pooled_prompt_embeds = encode_prompt_sdxl(pipe, captions, device)\n",
    "        prompt_embeds = prompt_embeds.to(dtype=unet.dtype)\n",
    "        pooled_prompt_embeds = pooled_prompt_embeds.to(dtype=unet.dtype)\n",
    "\n",
    "        # SDXL time_ids 생성\n",
    "        add_time_ids = compute_time_ids(\n",
    "            (RESOLUTION, RESOLUTION),\n",
    "            (0, 0),\n",
    "            (RESOLUTION, RESOLUTION),\n",
    "            device,\n",
    "            unet. dtype\n",
    "        )\n",
    "        add_time_ids = add_time_ids.repeat(latents.shape[0], 1)\n",
    "\n",
    "        # UNet 예측\n",
    "        noise_pred = unet(\n",
    "            noisy_latents,\n",
    "            timesteps,\n",
    "            encoder_hidden_states=prompt_embeds,\n",
    "            added_cond_kwargs={\n",
    "                \"text_embeds\":  pooled_prompt_embeds,\n",
    "                \"time_ids\": add_time_ids\n",
    "            },\n",
    "            return_dict=False\n",
    "        )[0]\n",
    "\n",
    "        # Loss 계산 (float32로 계산)\n",
    "        loss = torch. nn.functional.mse_loss(noise_pred. float(), noise.float())\n",
    "        \n",
    "        # nan 체크\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            print(f\"Step {step}: nan/inf detected, skipping...\")\n",
    "            optimizer.zero_grad()\n",
    "            continue\n",
    "        \n",
    "        loss = loss / GRADIENT_ACCUMULATION_STEPS\n",
    "        loss.backward()\n",
    "\n",
    "        if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "            # Gradient Clipping\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                filter(lambda p: p.requires_grad, unet.parameters()),\n",
    "                MAX_GRAD_NORM\n",
    "            )\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        valid_steps += 1\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    if valid_steps > 0:\n",
    "        avg_loss = total_loss / valid_steps\n",
    "        print(f\"Epoch {epoch+1} 완료 - 평균 Loss:  {avg_loss:.4f}\")\n",
    "    else:\n",
    "        print(f\"Epoch {epoch+1} 완료 - 유효한 step 없음\")\n",
    "\n",
    "# ================== LoRA 가중치 저장 ==================\n",
    "print(\"LoRA 가중치 저장 중...\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "unet.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"저장 완료: {OUTPUT_DIR}\")\n",
    "\n",
    "print(\"LoRA 학습 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a3cdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추론...\n",
    "import torch\n",
    "from diffusers import StableDiffusionXLPipeline\n",
    "from peft import PeftModel\n",
    "import os\n",
    "MODEL_NAME = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "LORA_PATH = \"./sdxl-lora-output\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 모델 로드\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch. float32,  # 수정: float32로 로드  # 1\n",
    "    use_safetensors=True,\n",
    ")\n",
    "pipe.to(device)\n",
    "# lora 가중치 로드\n",
    "pipe.unet = PeftModel.from_pretrained(pipe.unet,LORA_PATH)\n",
    "# 메모리 최적화\n",
    "try:\n",
    "    pipe.enable_xformers_memory_efficient_attention()\n",
    "    print(\"xformers 활성화 완료\")\n",
    "except Exception as e: \n",
    "    print(f\"xformers 사용 불가:  {e}\")\n",
    "    pipe.enable_attention_slicing()\n",
    "\n",
    "pipe.enable_vae_slicing()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 이미지 생성\n",
    "prompt = \"a ninja cat in naruto anime style, highly detailed\"\n",
    "negative_prompt = \"blurry, low quality, distorted, ugly\"\n",
    "\n",
    "image = pipe(\n",
    "    prompt=prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    num_inference_steps=30,\n",
    "    guidance_scale=7.5,\n",
    "    width=512,\n",
    "    height=512,\n",
    "    generator=torch.Generator(device).manual_seed(42)\n",
    ").images[0]\n",
    "\n",
    "# 이미지 저장\n",
    "output_path = \"generated_image.png\"\n",
    "image.save(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88042b0",
   "metadata": {},
   "source": [
    "모델전체 저장(예비용으로 LoRA백업)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97382cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델로드 float32   # 1\n",
    "# VAE dtype  float32    # 이미지를 인코딩 디코딩 하는 역활  # 2\n",
    "# Mixed Precision 비활성화  AMP\n",
    "# Learning_rate 1e-5\n",
    "# BatchSize 1\n",
    "# LoRa dropout 0.0\n",
    "# Loss계산 float32\n",
    "#  Gradient Clipping 추가\n",
    "#  nan 체크 추가\n",
    "\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from diffusers import StableDiffusionXLPipeline, DDPMScheduler\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# ================== 설정 ==================\n",
    "DATASET_NAME = \"lambdalabs/naruto-blip-captions\"\n",
    "MODEL_NAME = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "LORA_OUTPUT_DIR = \"./sdxl-lora-output\"    # LoRA only (backup)\n",
    "MERGED_OUTPUT_DIR = \"./sdxl-merged-output\" # hole model save\n",
    "RESOLUTION = 512\n",
    "BATCH_SIZE = 1          # 수정: 1로 시작\n",
    "LEARNING_RATE = 1e-5    # 수정: 1e-8은 너무 낮음\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "EPOCHS = 3\n",
    "MAX_GRAD_NORM = 1.0\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ================== 모델 로드 ==================\n",
    "print(\"모델 로딩 중...\")\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch. float32,  # 수정: float32로 로드  # 1\n",
    "    use_safetensors=True,\n",
    ")\n",
    "\n",
    "# ================== LoRA 설정 및 적용 ==================\n",
    "print(\"LoRA 설정 적용 중...\")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"to_q\", \"to_k\", \"to_v\", \"to_out. 0\",\n",
    "    ],\n",
    "    lora_dropout=0.0,  # 수정: dropout 제거 (안정성)\n",
    ")\n",
    "\n",
    "unet = pipe.unet\n",
    "unet = get_peft_model(unet, lora_config)\n",
    "unet.print_trainable_parameters()\n",
    "\n",
    "# ================== 메모리 최적화 ==================\n",
    "print(\"메모리 최적화 설정 중...\")\n",
    "\n",
    "try:\n",
    "    pipe.enable_xformers_memory_efficient_attention()\n",
    "    print(\"xformers 활성화 완료\")\n",
    "except Exception as e: \n",
    "    print(f\"xformers 사용 불가:  {e}\")\n",
    "    pipe.enable_attention_slicing()\n",
    "\n",
    "pipe.enable_vae_slicing()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# ================== 모델 배치 ==================\n",
    "print(\"모델 설정 중...\")\n",
    "\n",
    "# VAE는 float32 유지 (안정성)\n",
    "pipe.vae.to(device, dtype=torch.float32)  # 2\n",
    "pipe.text_encoder.to(device)\n",
    "pipe.text_encoder_2.to(device)\n",
    "unet.to(device)\n",
    "\n",
    "# 학습하지 않는 모듈 freeze\n",
    "pipe.vae.requires_grad_(False)\n",
    "pipe.text_encoder.requires_grad_(False)\n",
    "pipe.text_encoder_2.requires_grad_(False)\n",
    "\n",
    "# VAE를 eval 모드로\n",
    "pipe.vae.eval()\n",
    "\n",
    "# ================== SDXL용 헬퍼 함수 ==================\n",
    "def compute_time_ids(original_size, crops_coords_top_left, target_size, device, dtype):\n",
    "    add_time_ids = list(original_size + crops_coords_top_left + target_size)\n",
    "    add_time_ids = torch.tensor([add_time_ids], dtype=dtype, device=device)\n",
    "    return add_time_ids\n",
    "\n",
    "def encode_prompt_sdxl(pipe, prompt, device):\n",
    "    # Text Encoder 1\n",
    "    text_inputs = pipe.tokenizer(\n",
    "        prompt,\n",
    "        padding=\"max_length\",\n",
    "        max_length=pipe.tokenizer.model_max_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    text_input_ids = text_inputs.input_ids. to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        prompt_embeds = pipe. text_encoder(\n",
    "            text_input_ids,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        pooled_prompt_embeds = prompt_embeds[0]\n",
    "        prompt_embeds = prompt_embeds.hidden_states[-2]\n",
    "    \n",
    "    # Text Encoder 2\n",
    "    text_inputs_2 = pipe. tokenizer_2(\n",
    "        prompt,\n",
    "        padding=\"max_length\",\n",
    "        max_length=pipe.tokenizer_2.model_max_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    text_input_ids_2 = text_inputs_2.input_ids.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        prompt_embeds_2 = pipe.text_encoder_2(\n",
    "            text_input_ids_2,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        pooled_prompt_embeds_2 = prompt_embeds_2[0]\n",
    "        prompt_embeds_2 = prompt_embeds_2.hidden_states[-2]\n",
    "    \n",
    "    prompt_embeds = torch.concat([prompt_embeds, prompt_embeds_2], dim=-1)\n",
    "    \n",
    "    return prompt_embeds, pooled_prompt_embeds_2\n",
    "\n",
    "# ================== 데이터셋 로드 ==================\n",
    "print(\"데이터셋 로딩 중...\")\n",
    "dataset = load_dataset(DATASET_NAME, split='train[:100]')\n",
    "print(f'original dataset:  {dataset.column_names}')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((RESOLUTION, RESOLUTION)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "def preprocess(example):\n",
    "    image = example['image'].convert('RGB')\n",
    "    return {\n",
    "        'pixel_values': transform(image),\n",
    "        'caption': example['text']\n",
    "    }\n",
    "\n",
    "dataset = dataset. map(preprocess, remove_columns=dataset.column_names)\n",
    "dataset.set_format(type='torch', columns=['pixel_values', 'caption'])\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# ================== 학습 설정 ==================\n",
    "optimizer = torch.optim. AdamW(\n",
    "    filter(lambda p: p.requires_grad, unet.parameters()),\n",
    "    lr=LEARNING_RATE,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=1e-2,\n",
    "    eps=1e-8\n",
    ")\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(MODEL_NAME, subfolder='scheduler')\n",
    "\n",
    "# Mixed Precision 비활성화 (안정성 우선)\n",
    "use_amp = False\n",
    "\n",
    "# ================== 학습 루프 ==================\n",
    "print(\"LoRA 학습 시작!\")\n",
    "unet.train()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    total_loss = 0\n",
    "    valid_steps = 0\n",
    "\n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        \n",
    "        pixel_values = batch[\"pixel_values\"].to(device, dtype=torch.float32)\n",
    "\n",
    "        # Latent 인코딩 (float32)\n",
    "        with torch.no_grad():\n",
    "            latent_dist = pipe.vae.encode(pixel_values).latent_dist\n",
    "            latents = latent_dist.sample()\n",
    "            latents = latents * pipe.vae. config.scaling_factor\n",
    "            latents = latents.to(dtype=unet.dtype)\n",
    "\n",
    "        # 노이즈 추가\n",
    "        noise = torch.randn_like(latents)\n",
    "        timesteps = torch.randint(\n",
    "            0, noise_scheduler.config.num_train_timesteps,\n",
    "            (latents.shape[0],), device=device\n",
    "        ).long()\n",
    "        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "        # 캡션 처리\n",
    "        captions = batch[\"caption\"]\n",
    "        if isinstance(captions, torch.Tensor):\n",
    "            captions = [str(c) for c in captions]\n",
    "        elif isinstance(captions, str):\n",
    "            captions = [captions]\n",
    "        else: \n",
    "            captions = list(captions)\n",
    "\n",
    "        # SDXL 텍스트 임베딩\n",
    "        prompt_embeds, pooled_prompt_embeds = encode_prompt_sdxl(pipe, captions, device)\n",
    "        prompt_embeds = prompt_embeds.to(dtype=unet.dtype)\n",
    "        pooled_prompt_embeds = pooled_prompt_embeds.to(dtype=unet.dtype)\n",
    "\n",
    "        # SDXL time_ids 생성\n",
    "        add_time_ids = compute_time_ids(\n",
    "            (RESOLUTION, RESOLUTION),\n",
    "            (0, 0),\n",
    "            (RESOLUTION, RESOLUTION),\n",
    "            device,\n",
    "            unet. dtype\n",
    "        )\n",
    "        add_time_ids = add_time_ids.repeat(latents.shape[0], 1)\n",
    "\n",
    "        # UNet 예측\n",
    "        noise_pred = unet(\n",
    "            noisy_latents,\n",
    "            timesteps,\n",
    "            encoder_hidden_states=prompt_embeds,\n",
    "            added_cond_kwargs={\n",
    "                \"text_embeds\":  pooled_prompt_embeds,\n",
    "                \"time_ids\": add_time_ids\n",
    "            },\n",
    "            return_dict=False\n",
    "        )[0]\n",
    "\n",
    "        # Loss 계산 (float32로 계산)\n",
    "        loss = torch. nn.functional.mse_loss(noise_pred. float(), noise.float())\n",
    "        \n",
    "        # nan 체크\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            print(f\"Step {step}: nan/inf detected, skipping...\")\n",
    "            optimizer.zero_grad()\n",
    "            continue\n",
    "        \n",
    "        loss = loss / GRADIENT_ACCUMULATION_STEPS\n",
    "        loss.backward()\n",
    "\n",
    "        if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "            # Gradient Clipping\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                filter(lambda p: p.requires_grad, unet.parameters()),\n",
    "                MAX_GRAD_NORM\n",
    "            )\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        valid_steps += 1\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    if valid_steps > 0:\n",
    "        avg_loss = total_loss / valid_steps\n",
    "        print(f\"Epoch {epoch+1} 완료 - 평균 Loss:  {avg_loss:.4f}\")\n",
    "    else:\n",
    "        print(f\"Epoch {epoch+1} 완료 - 유효한 step 없음\")\n",
    "\n",
    "# ================== LoRA 가중치 저장 ==================\n",
    "print(\"LoRA 가중치 저장 중...\")\n",
    "os.makedirs(LORA_OUTPUT_DIR, exist_ok=True)\n",
    "unet.save_pretrained(LORA_OUTPUT_DIR)\n",
    "print(f\"저장 완료: {LORA_OUTPUT_DIR}\")\n",
    "# ================== LoRA 병합 및 전체모델 저장 ==================\n",
    "# LoRA를 기본 모델에 병합\n",
    "unet = unet.merge_and_unload()\n",
    "# 병합된 unet을 파이프라인에 다시 할당\n",
    "pipe.unet = unet\n",
    "# 전체모델 저장\n",
    "os.makedirs(MERGED_OUTPUT_DIR, exist_ok=True)\n",
    "pipe.save_pretrained(MERGED_OUTPUT_DIR,safe_serialization=True)\n",
    "print(f\"저장 완료: {MERGED_OUTPUT_DIR}\")\n",
    "\n",
    "print(\"LoRA 학습 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f27525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추론 - 모델 전체\n",
    "\n",
    "\n",
    "import torch\n",
    "from diffusers import StableDiffusionXLPipeline\n",
    "import zipfile\n",
    "\n",
    "# 압축 해제\n",
    "print(\"압축 해제 중...\")\n",
    "with zipfile.ZipFile(\"sdxl-merged-model.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"./\")\n",
    "print(\"압축 해제 완료\")\n",
    "\n",
    "# 모델 로드 (다운로드 없음!)\n",
    "print(\"모델 로딩 중...\")\n",
    "device = \"cuda\"\n",
    "\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"./sdxl-merged-model\",\n",
    "    torch_dtype=torch. float16,\n",
    "    local_files_only=True  # 로컬에서만 로드\n",
    ")\n",
    "pipe.to(device)\n",
    "pipe.enable_vae_slicing()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 이미지 생성\n",
    "print(\"이미지 생성 중...\")\n",
    "image = pipe(\n",
    "    prompt=\"a ninja cat in naruto anime style, highly detailed\",\n",
    "    negative_prompt=\"blurry, low quality, distorted, ugly\",\n",
    "    num_inference_steps=30,\n",
    "    guidance_scale=7.5,\n",
    "    width=512,\n",
    "    height=512,\n",
    "    generator=torch.Generator(device=device).manual_seed(42)\n",
    ").images[0]\n",
    "\n",
    "image.save(\"output.png\")\n",
    "print(\"저장 완료:  output.png\")\n",
    "image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
