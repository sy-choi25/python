{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "939d899a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (4.4.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: filelock in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from datasets) (2.2.5)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from anyio->httpx<1.0.0->datasets) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cdd0ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\SAMSUNG\\miniconda3\\envs\\llm_env\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 필수 라이브러리 임포트\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Transformers 라이브러리\n",
    "from transformers import (\n",
    "    pipeline,                              # 고수준 API - 가장 쉬운 방법\n",
    "    AutoTokenizer,                         # 자동 토크나이저\n",
    "    AutoModelForQuestionAnswering,         # QA 모델 자동 로더\n",
    "    DistilBertTokenizerFast,              # DistilBERT 고속 토크나이저\n",
    "    DistilBertForQuestionAnswering,        # DistilBERT QA 모델\n",
    "    ElectraTokenizer,                      # ELECTRA 토크나이저 (한글)\n",
    "    ElectraForQuestionAnswering,           # ELECTRA QA 모델 (한글)\n",
    "    DefaultDataCollator,                   # 기본 데이터 콜레이터\n",
    "    TrainingArguments,                     # 학습 하이퍼파라미터\n",
    "    Trainer,                               # 범용 트레이너\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14e9e1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07545f66",
   "metadata": {},
   "source": [
    "[1] 모델을 불러서 사용하기 (Inference)\n",
    "- pipeline\n",
    "- AutoModel / AutoTokenizer\n",
    "\n",
    "[2] 학습 데이터 (SQuAD)\n",
    "- Fine-tuning용 정답 데이터\n",
    "\n",
    "[3] 모델 학습시키기 (Fine-tuning)\n",
    "- 전처리\n",
    "- 학습\n",
    "- 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adec6bc",
   "metadata": {},
   "source": [
    "<span style=\"color: Gold\"> pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee3bf08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer1 : the process of deriving high-quality information from text\n",
      "answer2 : 답변 없음\n",
      " answer1 = {'score': 0.42419031262397766, 'start': 95, 'end': 153, 'answer': 'the process of deriving high-quality information from text'}\n",
      " answer2 = {'score': 0.04562292993068695, 'start': 624, 'end': 670, 'answer': 'information extraction, data mining, and a KDD'}\n"
     ]
    }
   ],
   "source": [
    "question_answer = pipeline (\"question-answering\",model = 'distilbert-base-cased-distilled-squad')\n",
    "context = \"\"\"Text mining, also referred to as text data mining (abbr.: TDM), similar to text analytics, \n",
    "is the process of deriving high-quality information from text. It involves \n",
    "\"the discovery by computer of new, previously unknown information, \n",
    "by automatically extracting information from different written resources.\" \n",
    "Written resources may include websites, books, emails, reviews, and articles. \n",
    "High-quality information is typically obtained by devising patterns and trends \n",
    "by means such as statistical pattern learning. According to Hotho et al. (2005)\n",
    "we can distinguish between three different perspectives of text mining: \n",
    "information extraction, data mining, and a KDD (Knowledge Discovery in Databases) process.\"\"\"\n",
    "\n",
    "question1 = \"What is text mining?\"\n",
    "question2 = \"What are the perspectives of text mining?\"\n",
    "\n",
    "# 질의 응답 수행\n",
    "answer1 = question_answer(context = context, question=question1)\n",
    "answer2 = question_answer(context = context, question=question2)\n",
    "if answer1['score'] < 0.1 :\n",
    "  print(f'answer1 : 답변 없음')\n",
    "else:\n",
    "  print(f\"answer1 : {answer1['answer']}\")\n",
    "if answer2['score'] < 0.1 :\n",
    "  print(f'answer2 : 답변 없음')\n",
    "else:\n",
    "  print(f\"answer2 : {answer2['answer']}\")\n",
    "\n",
    "print(f' answer1 = {answer1 }')\n",
    "print(f' answer2 = {answer2 }')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a320f77",
   "metadata": {},
   "source": [
    "<span style=\"color: Gold\"> AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7eb128b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer1 : the process of deriving high - quality information from text\n"
     ]
    }
   ],
   "source": [
    "#AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-cased-distilled-squad')\n",
    "model = AutoModelForQuestionAnswering.from_pretrained('distilbert-base-cased-distilled-squad')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "inputs = tokenizer(question1, context, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "  outputs = model(**inputs)\n",
    "start_score = outputs.start_logits\n",
    "end_score = outputs.end_logits\n",
    "answer_start = torch.argmax(start_score)\n",
    "answer_end = torch.argmax(end_score)\n",
    "answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][answer_start:answer_end+1]))\n",
    "print(f'answer1 : {answer}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2eb7d1",
   "metadata": {},
   "source": [
    "<span style=\"color: Gold\"> SQuAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a685644",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1e9ccb4",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Only a single TORCH_LIBRARY can be used to register the namespace prims; please put all of your definitions in a single TORCH_LIBRARY block.  If you were trying to specify implementations, consider using TORCH_LIBRARY_IMPL (which can be duplicated).  If you really intended to define operators for a single namespace in a distributed way, you can use TORCH_LIBRARY_FRAGMENT to explicitly indicate this.  Previous registration of TORCH_LIBRARY was registered at c:\\Users\\SAMSUNG\\miniconda3\\envs\\llm_env\\lib\\site-packages\\torch\\_prims\\__init__.py:37; latest registration was registered at c:\\Users\\SAMSUNG\\miniconda3\\envs\\llm_env\\lib\\site-packages\\torch\\_prims\\__init__.py:37",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n",
      "File \u001b[1;32mc:\\Users\\SAMSUNG\\miniconda3\\envs\\llm_env\\lib\\site-packages\\datasets\\__init__.py:17\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Datasets Authors and the TensorFlow Datasets Authors.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4.4.1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_dataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Column, Dataset\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ReadInstruction\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ArrowBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\n",
      "File \u001b[1;32mc:\\Users\\SAMSUNG\\miniconda3\\envs\\llm_env\\lib\\site-packages\\datasets\\arrow_dataset.py:104\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfilesystems\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_remote_filesystem\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfingerprint\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     93\u001b[0m     fingerprint_transform,\n\u001b[0;32m     94\u001b[0m     format_kwargs_for_fingerprint,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    102\u001b[0m     validate_fingerprint,\n\u001b[0;32m    103\u001b[0m )\n\u001b[1;32m--> 104\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformatting\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m format_table, get_format_type_from_alias, get_formatter, query_table\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformatting\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformatting\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LazyDict, _is_range_contiguous\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minfo\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DatasetInfo, DatasetInfosDict\n",
      "File \u001b[1;32mc:\\Users\\SAMSUNG\\miniconda3\\envs\\llm_env\\lib\\site-packages\\datasets\\formatting\\__init__.py:91\u001b[0m\n\u001b[0;32m     88\u001b[0m     _register_unavailable_formatter(_polars_error, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolars\u001b[39m\u001b[38;5;124m\"\u001b[39m, aliases\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mTORCH_AVAILABLE:\n\u001b[1;32m---> 91\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch_formatter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TorchFormatter\n\u001b[0;32m     93\u001b[0m     _register_formatter(TorchFormatter, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m, aliases\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytorch\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\SAMSUNG\\miniconda3\\envs\\llm_env\\lib\\site-packages\\datasets\\formatting\\torch_formatter.py:32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Import torch once at module level once\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     _torch_available \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\SAMSUNG\\miniconda3\\envs\\llm_env\\lib\\site-packages\\torch\\__init__.py:2680\u001b[0m\n\u001b[0;32m   2676\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m vmap \u001b[38;5;28;01mas\u001b[39;00m vmap\n\u001b[0;32m   2679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m TYPE_CHECKING:\n\u001b[1;32m-> 2680\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations\n\u001b[0;32m   2682\u001b[0m \u001b[38;5;66;03m# Enable CUDA Sanitizer\u001b[39;00m\n\u001b[0;32m   2683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTORCH_CUDA_SANITIZER\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n",
      "File \u001b[1;32mc:\\Users\\SAMSUNG\\miniconda3\\envs\\llm_env\\lib\\site-packages\\torch\\_meta_registrations.py:12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_prims_common\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutils\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SymBool, SymFloat, Tensor\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decomp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     13\u001b[0m     _add_op_to_registry,\n\u001b[0;32m     14\u001b[0m     _convert_out_params,\n\u001b[0;32m     15\u001b[0m     global_decomposition_table,\n\u001b[0;32m     16\u001b[0m     meta_table,\n\u001b[0;32m     17\u001b[0m )\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpOverload\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_prims\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _prim_elementwise_meta, ELEMENTWISE_PRIM_TYPE_PROMOTION_KIND\n",
      "File \u001b[1;32mc:\\Users\\SAMSUNG\\miniconda3\\envs\\llm_env\\lib\\site-packages\\torch\\_decomp\\__init__.py:276\u001b[0m\n\u001b[0;32m    272\u001b[0m             decompositions\u001b[38;5;241m.\u001b[39mpop(op, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    275\u001b[0m \u001b[38;5;66;03m# populate the table\u001b[39;00m\n\u001b[1;32m--> 276\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decomp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecompositions\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_refs\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcore_aten_decompositions\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCustomDecompTable\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\SAMSUNG\\miniconda3\\envs\\llm_env\\lib\\site-packages\\torch\\_decomp\\decompositions.py:16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_meta_registrations\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_prims\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mprims\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_prims_common\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutils\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mF\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\SAMSUNG\\miniconda3\\envs\\llm_env\\lib\\site-packages\\torch\\_prims\\__init__.py:37\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moverrides\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m handle_torch_function, has_torch_function\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pytree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tree_flatten, tree_map, tree_unflatten\n\u001b[1;32m---> 37\u001b[0m prim \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlibrary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLibrary\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprims\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDEF\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m prim_impl \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlibrary\u001b[38;5;241m.\u001b[39mLibrary(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprims\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIMPL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompositeExplicitAutograd\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m prim_backend_select_impl \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlibrary\u001b[38;5;241m.\u001b[39mLibrary(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprims\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIMPL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBackendSelect\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\SAMSUNG\\miniconda3\\envs\\llm_env\\lib\\site-packages\\torch\\library.py:109\u001b[0m, in \u001b[0;36mLibrary.__init__\u001b[1;34m(self, ns, kind, dispatch_key)\u001b[0m\n\u001b[0;32m    107\u001b[0m frame \u001b[38;5;241m=\u001b[39m traceback\u001b[38;5;241m.\u001b[39mextract_stack(limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    108\u001b[0m filename, lineno \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39mfilename, frame\u001b[38;5;241m.\u001b[39mlineno\n\u001b[1;32m--> 109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm: Optional[Any] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch_library\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdispatch_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlineno\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mns \u001b[38;5;241m=\u001b[39m ns\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op_defs: \u001b[38;5;28mset\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Only a single TORCH_LIBRARY can be used to register the namespace prims; please put all of your definitions in a single TORCH_LIBRARY block.  If you were trying to specify implementations, consider using TORCH_LIBRARY_IMPL (which can be duplicated).  If you really intended to define operators for a single namespace in a distributed way, you can use TORCH_LIBRARY_FRAGMENT to explicitly indicate this.  Previous registration of TORCH_LIBRARY was registered at c:\\Users\\SAMSUNG\\miniconda3\\envs\\llm_env\\lib\\site-packages\\torch\\_prims\\__init__.py:37; latest registration was registered at c:\\Users\\SAMSUNG\\miniconda3\\envs\\llm_env\\lib\\site-packages\\torch\\_prims\\__init__.py:37"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0726d849",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#SQuAD 데이터셋 로드 분석\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# 스탠포드 대학에서 공개한 질의응답 벤치마크 - Extractive QA 표준\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m squad \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msquad\u001b[39m\u001b[38;5;124m'\u001b[39m, split \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain[:5000]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m squad \u001b[38;5;241m=\u001b[39m squad\u001b[38;5;241m.\u001b[39mtrain_test_split(test_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(squad[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m][:\u001b[38;5;241m10\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "#SQuAD 데이터셋 로드 분석\n",
    "# 스탠포드 대학에서 공개한 질의응답 벤치마크 - Extractive QA 표준\n",
    "\n",
    "squad = load_dataset('squad', split = 'train[:5000]')\n",
    "squad = squad.train_test_split(test_size = 0.2, seed=42)\n",
    "\n",
    "print(squad['train'][0]['context'][:10])\n",
    "print(squad['train'][0]['question'][:10])\n",
    "print(squad['train'][0]['answers'])\n",
    "print(squad['train'][0]['answers']['answer_start'])\n",
    "print(squad['train'][0]['context'][98:98+len('Neo-Confucian establishment')])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e28c74",
   "metadata": {},
   "source": [
    "<span style=\"color: Gold\"> fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27d67b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전학습만 된 모델(QA헤드는 초기화) distilbert-base-uncased\n",
    "# 한국어 학습이 가능하지만 성능 보장 못하고 비효율적\n",
    "# distilbert-base-uncased 영어전용 모델. 한국어를 전처리할때 어간 및 품사 등이 달라서 심하게 왜곡될 수 있음\\\n",
    "# 한국어면 한국어 전용 베이스모델에 파인튜닝을 해야 함 또는 다국어 모델에 적용\n",
    "# mBERT BERT-base-mutilingual-cased\n",
    "# klue/bert-base 등등\n",
    "\n",
    "tokenzier = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased')\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f9ce5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA헤드는 아직 학습되지 않음(랜덤 가중치가 적용)\n",
    "test_context = \"\"\"The city is the birthplace of many cultural movements, including the Harlem \n",
    "Renaissance in literature and visual art; abstract expressionism \n",
    "(also known as the New York School) in painting; and hip hop, punk, salsa, disco, \n",
    "freestyle, Tin Pan Alley, and Jazz in music. New York City has been considered \n",
    "the dance capital of the world. The city is also widely celebrated in popular lore, \n",
    "frequently the setting for books, movies, and television programs.\"\"\"\n",
    "    \n",
    "test_question = \"The dance capital of the world is what city in the US?\"\n",
    "# fine-tuning이 되지 않은 모델\n",
    "inputs = tokenizer(test_question, test_context,return_tensors = 'pt').to(device)\n",
    "with torch.no_grad():\n",
    "  outputs = model(**inputs)\n",
    "start = torch.argmax(outputs.start_logits)\n",
    "end = torch.argmax(outputs.end_logits)\n",
    "start, end\n",
    "inputs_ids = inputs['input_ids'].tolist()[0]\n",
    "answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs_ids[start:end+1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff5e64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 필수 라이브러리 임포트\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Transformers 라이브러리\n",
    "from transformers import (\n",
    "    pipeline,                              # 고수준 API - 가장 쉬운 방법\n",
    "    AutoTokenizer,                         # 자동 토크나이저\n",
    "    AutoModelForQuestionAnswering,         # QA 모델 자동 로더\n",
    "    DistilBertTokenizerFast,              # DistilBERT 고속 토크나이저\n",
    "    DistilBertForQuestionAnswering,        # DistilBERT QA 모델\n",
    "    ElectraTokenizer,                      # ELECTRA 토크나이저 (한글)\n",
    "    ElectraForQuestionAnswering,           # ELECTRA QA 모델 (한글)\n",
    "    DefaultDataCollator,                   # 기본 데이터 콜레이터\n",
    "    TrainingArguments,                     # 학습 하이퍼파라미터\n",
    "    Trainer,                               # 범용 트레이너\n",
    ")\n",
    "from datasets import load_dataset\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43da65bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer = pipeline(\"question-answering\",model = 'distilbert-base-cased-distilled-squad')\n",
    "\n",
    "context = \"\"\"Text mining, also referred to as text data mining (abbr.: TDM), similar to text analytics,\n",
    "is the process of deriving high-quality information from text. It involves\n",
    "\"the discovery by computer of new, previously unknown information,\n",
    "by automatically extracting information from different written resources.\"\n",
    "Written resources may include websites, books, emails, reviews, and articles.\n",
    "High-quality information is typically obtained by devising patterns and trends\n",
    "by means such as statistical pattern learning. According to Hotho et al. (2005)\n",
    "we can distinguish between three different perspectives of text mining:\n",
    "information extraction, data mining, and a KDD (Knowledge Discovery in Databases) process.\"\"\"\n",
    "\n",
    "question1 = \"What is text mining?\"\n",
    "question2 = \"What are the perspectives of text mining?\"\n",
    "\n",
    "# 질의 응답 수행\n",
    "answer1 = question_answer(context=context, question=question1)\n",
    "answer2 = question_answer(context=context, question=question2)\n",
    "if answer1['score'] < 0.1:\n",
    "  print(f'answer1 : 답변 없음')\n",
    "else:\n",
    "  print(f\"answer1 : {answer1['answer']}\")\n",
    "if answer2['score'] < 0.1:\n",
    "  print(f'answer2 : 답변 없음')\n",
    "else:\n",
    "  print(f\"answer2 : {answer2['answer']}\")\n",
    "\n",
    "# AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased-distilled-squad\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-cased-distilled-squad\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "inputs = tokenizer(question1, context, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "start_score = outputs.start_logits\n",
    "end_score  = outputs.end_logits\n",
    "answer_start = torch.argmax(start_score)\n",
    "answer_end = torch.argmax(end_score)\n",
    "answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][answer_start:answer_end+1]))\n",
    "print(f\"answer1 : {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af805df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQuAD 데이터셋 로드 분석\n",
    "# 스탠포드 대학에서 공개한 질의응답 벤치마크 - Extractive QA 표준\n",
    "squad = load_dataset('squad', split='train[:5000]')\n",
    "valid = load_dataset('squad', split='validation[:1000]')\n",
    "\n",
    "model_name = \"distilbert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "max_length = 384\n",
    "stride = 128\n",
    "\n",
    "def preprocess(example):\n",
    "    questions = [q.strip() for q in example[\"question\"]]\n",
    "    contexts = example[\"context\"]\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        questions,\n",
    "        contexts,\n",
    "        truncation=\"only_second\",\n",
    "        max_length=max_length,\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = tokenized.pop(\"offset_mapping\")\n",
    "    sample_map = tokenized.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = example[\"answers\"][sample_idx]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = start_char + len(answer[\"text\"][0])\n",
    "\n",
    "        sequence_ids = tokenized.sequence_ids(i)\n",
    "\n",
    "        # context token 영역 찾기\n",
    "        ctx_start = sequence_ids.index(1)\n",
    "        ctx_end = len(sequence_ids) - sequence_ids[::-1].index(1) - 1\n",
    "\n",
    "        # 정답이 context에 없는 경우\n",
    "        if not (offsets[ctx_start][0] <= start_char <= offsets[ctx_end][1]):\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # 토큰 단위 start\n",
    "            start_token = ctx_start\n",
    "            while start_token <= ctx_end and offsets[start_token][0] <= start_char:\n",
    "                start_token += 1\n",
    "            start_positions.append(start_token - 1)\n",
    "\n",
    "            # 토큰 단위 end\n",
    "            end_token = ctx_end\n",
    "            while end_token >= ctx_start and offsets[end_token][1] >= end_char:\n",
    "                end_token -= 1\n",
    "            end_positions.append(end_token + 1)\n",
    "\n",
    "    tokenized[\"start_positions\"] = start_positions\n",
    "    tokenized[\"end_positions\"] = end_positions\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "train_dataset = squad.map(preprocess, batched=True, remove_columns=squad.column_names)\n",
    "valid_dataset = valid.map(preprocess, batched=True, remove_columns=valid.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25467fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(train_dataset)\n",
    "print(squad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f234a525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# 3. 모델 로드\n",
    "# --------------------------------------------------------------------------------\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 4. 학습 설정\n",
    "# --------------------------------------------------------------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./distilbert_squad\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 5. Trainer 구성\n",
    "# --------------------------------------------------------------------------------\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DefaultDataCollator(),\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
