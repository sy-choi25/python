{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7aa0c64",
   "metadata": {},
   "source": [
    "<span style=\"color: Gold\"> 개체명 인식: NER\n",
    "- 텍스트에서 특정 의미를 가진 단어나 구절을 찾아내고 분류하는 작업\n",
    "- “여기서 사람 이름은 뭐야?”, “여기서 장소는 어디야?”, “조직 이름이 있니?”, “시간/날짜/수량 같은 정보는?” 이런 내용을 자동으로 찾아주는 NLP 기술\n",
    "- 자연어처리의 기초적인 테스크"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5b47e2",
   "metadata": {},
   "source": [
    "<span style=\"font-size:12px;\">텍스트</span>\n",
    "\n",
    "\n",
    "- <예시문> 홍길동은 2025년 11월 19월 서울시청에서 삼성전자 직원을 만났다\n",
    "  - 홍길동 - [인명]\n",
    "  - 2024년 1월 19일 - [날짜]\n",
    "  - 서울시청 - [지명]\n",
    "  - 삼성전자 - [기관명]\n",
    "\n",
    "- 활용분야\n",
    "  - 뉴스기사 : 기사에서 인물, 장소, 기관 자동추출\n",
    "  - 의료문서 : 병명, 약물명, 증상\n",
    "  - 계약서 : 회사명, 날짜, 금액\n",
    "  - 쳇봇 : 사용자 질문에 핵심정보 파악\n",
    "\n",
    "- BIO 태깅\n",
    "  - B(Being) 개체 시작\n",
    "  - I(inside) 개체 내부\n",
    "  -  O (Outside) 개체가 아님"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fb09c8",
   "metadata": {},
   "source": [
    "<span style=\"font-size:12px;\">\n",
    "\n",
    "### <span style=\"color: lightblue;\"> BERT 기반 NER 모델 개발 과정 요약\n",
    "\n",
    "| 단계 (Stage) | 주요 목표 및 역할 | 핵심 코드/개념 | 문맥적 의미 (비유) |\n",
    "| :---: | :--- | :--- | :--- |\n",
    "| **1. 데이터 준비** | 훈련할 **'교과서'**를 만들고 컴퓨터 언어로 번역합니다. | `train_labels`, `tokenizer(...)`, `is_split_into_words=True` | 원본 텍스트를 **숫자(텐서)**로 변환하고 정답 라벨(`PER`, `LOC` 등)을 준비합니다. |\n",
    "| **2. 모델 정의 및 테스트** | **AI 뇌(설계도)**를 만들고 데이터 흐름을 1차 점검합니다. | `SimpleNERModel`, `torch.argmax(dim=-1)`, `id2label` | **BERT**를 기반으로 분류층을 연결한 후, 예측값(`logits`)을 뽑아내는 과정이 잘 작동하는지 확인합니다. |\n",
    "| **3. 모델 학습 (Training)** | 모델에게 반복적으로 문제를 풀게 하여 **지식(가중치)**을 쌓게 합니다. | `loss.backward()`, `optimizer.step()`, `zero_grad()` | **Loss(오차)**를 채점하고, **역전파**를 통해 오차를 수정하여 모델이 점차 똑똑해지게 만듭니다. |\n",
    "| **4. 모델 평가 및 활용** | 학습된 모델의 **실력을 측정**하고 실전에 투입합니다. | Accuracy, F1-Score | 학습하지"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cd89a0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2308a0",
   "metadata": {},
   "source": [
    "<span style=\"color: Gold\"> 1. 데이터 준비\n",
    "- 1. 라벨(Tag) 정의 및 인코딩\n",
    "    - 사용할 모든 개체명 태그(PER, LOC, ORG, O 등)를 정의\n",
    "    - 각 태그를 고유한 숫자 ID로 매핑하는 딕셔너리(label2id, id2label)를 생성\n",
    "- 2. 토크나이징\n",
    "    - tokenizer를 사용하여 문장을 토큰 ID로 변환\n",
    "    - BERT 특수 토큰([CLS], [SEP])을 추가하고, 최대 길이에 맞춰 자르기(Truncation) 및 패딩(Padding) 처리를 수행\n",
    "- 3. 라벨 정렬\n",
    "    - 토크나이징으로 인해 쪼개진 단어(Subword)에 맞춰 정답 라벨도 복사하거나 마스킹(Masking)하여 길이를 맞춘다\n",
    "    - 패딩된 토큰의 라벨은 손실 계산 시 무시되도록 -100 등의 값으로 설정\n",
    "- 4. 데이터 로드 생성\n",
    "    - 가공된 데이터를 담는 Dataset 객체를 만든다\n",
    "    - 데이터를 배치(Batch) 단위로 묶어 모델에게 공급할 DataLoader를 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5196404c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0497ddc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 김철수는         | B-PER    | 'PER' 개체의 시작\n",
      " 2024년        | B-DAT    | 'DAT' 개체의 시작\n",
      " 1월           | I-DAT    | 'DAT' 개체의 내부\n",
      " 15일          | I-DAT    | 'DAT' 개체의 내부\n",
      " 서울시청에서       | B-LOC    | 'LOC' 개체의 시작\n",
      " 삼성전자         | B-ORG    | 'ORG' 개체의 시작\n",
      " 직원을          | O        | 개체가 아님\n",
      " 만났다          | O        | 개체가 아님\n"
     ]
    }
   ],
   "source": [
    "# BIO 태깅\n",
    "tokens = [\"김철수는\", \"2024년\", \"1월\", \"15일\", \"서울시청에서\", \"삼성전자\", \"직원을\", \"만났다\"]\n",
    "bio_tags = [\"B-PER\", \"B-DAT\", \"I-DAT\", \"I-DAT\", \"B-LOC\", \"B-ORG\", \"O\", \"O\"] # B-DAT 처음 나온 날짜의 정보, I-DAT 앞에 날짜의 정보가 있으면 b가 아닌 i로 \n",
    "for token, tag in zip(tokens, bio_tags):\n",
    "    if tag.startswith('B-'):\n",
    "        desc = f\"'{tag[2:]}' 개체의 시작\"\n",
    "    elif tag.startswith('I-'):\n",
    "        desc = f\"'{tag[2:]}' 개체의 내부\"\n",
    "    else :\n",
    "        desc = \"개체가 아님\"\n",
    "    print(f\" {token:12} | {tag:8} | {desc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b0c086b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습데이터\n",
    "\n",
    "train_sentences = [\n",
    "    [\"김철수는\", \"서울에\", \"산다\"],\n",
    "    [\"이영희는\", \"2024년에\", \"부산으로\", \"이사했다\"],\n",
    "    [\"삼성전자는\", \"대한민국의\", \"대기업이다\"],\n",
    "    [\"박지성은\", \"축구선수다\"],\n",
    "    [\"2025년\", \"1월\", \"1일은\", \"새해다\"],\n",
    "]\n",
    "\n",
    "train_labels = [\n",
    "    [\"B-PER\", \"B-LOC\", \"O\"],\n",
    "    [\"B-PER\", \"B-DAT\", \"B-LOC\", \"O\"],\n",
    "    [\"B-ORG\", \"B-LOC\", \"O\"],\n",
    "    [\"B-PER\", \"O\"],\n",
    "    [\"B-DAT\", \"I-DAT\", \"I-DAT\", \"O\"],\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "611a9efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement sentencepiec (from versions: none)\n",
      "ERROR: No matching distribution found for sentencepiec\n"
     ]
    }
   ],
   "source": [
    "%pip install sentencepiec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b0b1b4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[517, 490, 494,   0, 517,   0, 491,   0, 491,   0, 517,   0,   0,   0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ast import mod\n",
    "# 토크나이져\n",
    "MODEL_NAME = 'skt/kobert-base-v1'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "text = '김철수는 서울에 산다'\n",
    "#토크나이져\n",
    "tokens = tokenizer.tokenize(text)\n",
    "# 인코딩\n",
    "encoded = tokenizer(text,return_tensors='pt')\n",
    "encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f3e66f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e160b79",
   "metadata": {},
   "source": [
    "<span style=\"color: Gold\"> 2. 모델 정의 및 순전파 테스트\n",
    "\n",
    "- 1. BERT 모델 로드\n",
    "    - 사전 학습된 **BERT 모델(AutoModel)**을 로드\n",
    "- 2. koBERT 인코더: 문장의 의미를 이해\n",
    "- 3. 분류기(Liner): 예측\n",
    "- 4. 출력 라벨:  B-PER 0 B-LOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3e4c6a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트문장 김철수는서울에산다\n",
      "정답라벨 B-PERB-LOCO\n",
      "김철수는       -> O        정답 : B-PER\n",
      "김철수는       -> O        정답 : B-PER\n",
      "김철수는       -> O        정답 : B-PER\n",
      "김철수는       -> O        정답 : B-PER\n",
      "서울에        -> O        정답 : B-LOC\n",
      "서울에        -> O        정답 : B-LOC\n",
      "서울에        -> O        정답 : B-LOC\n",
      "서울에        -> B-ORG    정답 : B-LOC\n",
      "서울에        -> O        정답 : B-LOC\n",
      "서울에        -> O        정답 : B-LOC\n",
      "산다         -> O        정답 : O\n",
      "산다         -> O        정답 : O\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "class SimpleNERModel(nn.Module):\n",
    "  def __init__(self, num_labels) -> None:\n",
    "    super(SimpleNERModel, self).__init__()\n",
    "    self.num_labels = num_labels\n",
    "    self.bert = AutoModel.from_pretrained(MODEL_NAME)\n",
    "    self.dropout = nn.Dropout(0.1)\n",
    "    self.clf = nn.Linear(self.bert.config.hidden_size,  self.num_labels)\n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    # kobert로 문자 인코딩\n",
    "    outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "    # 마지막 은닉상태 추출\n",
    "    sequence_output =  outputs.last_hidden_state\n",
    "    # Dropout 적용\n",
    "    sequence_output = self.dropout(sequence_output)\n",
    "    # 분류기\n",
    "    logits = self.clf(sequence_output)\n",
    "    return logits\n",
    "# 라벨의 개수\n",
    "label_list = sorted(list(set([data for i in train_labels for data in i])))\n",
    "label2id = { label:i for i, label in enumerate(label_list)}\n",
    "id2label = { i:label for i, label in enumerate(label_list)}\n",
    "\n",
    "model = SimpleNERModel(num_labels=len(label_list))\n",
    "\n",
    "# 모델 학습\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "# 순전파 테스트\n",
    "# 1. 문장과 정답 꺼내기\n",
    "sample_sentence = train_sentences[0]\n",
    "sample_label = train_labels[0]\n",
    "print(f\"테스트문장 {''.join(sample_sentence)}\")\n",
    "print(f\"정답라벨 {''.join(sample_label)}\")\n",
    "# 2. 토크나이저로 숫자 변환\n",
    "encoding = tokenizer(sample_sentence,\n",
    "                     return_tensors='pt',   \n",
    "                     truncation = True,     \n",
    "                     padding = True,        \n",
    "                     max_length = 32, \n",
    "                     is_split_into_words = True)  # 이미 띄어쓰기 된 리스트야\n",
    "input_ids = encoding['input_ids'].to(device)\n",
    "attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "with torch.no_grad(): # 지금은 학습 아니니까 기록하지마\n",
    "  model.eval()        # 평가모드이기 떄문에 dropout 끄기\n",
    "  logits = model(input_ids, attention_mask)   # 모델 통과 (결과:logits)\n",
    "  predictions = torch.argmax(logits, dim=-1)  # 가장 높은 점수 찍기(argmax), 각 단어마다 7개 라벨 후보 중 1등을 뽑아라\n",
    "                                              # 7개인걸 어떻게 알 수 있느냐?->label_list = set([data for i in train_labels for data in i])# 예: label_list = {'PER', 'LOC', 'ORG', 'DATE', 'TIME', 'ETC', 'O'} -> 총 7개!\n",
    "# 숫자를 다시 글자로\n",
    "word_ids =  encoding.word_ids(batch_index=0)\n",
    "pred_labels = []\n",
    "\n",
    "for i, word_idx in enumerate(word_ids):\n",
    "  if word_idx is not None and i < len(predictions[0]):  # 특수 토큰(CLS, SEP)이나 패딩은 건너뛰어줘\n",
    "    pred_label = id2label[ predictions[0][i].item() ]   # 모델이 예측한 번호를 실제 라벨 이름으로 바꾸기 (예: 1 -> 'PER')\n",
    "    if word_idx < len(sample_sentence):\n",
    "      print(f'{sample_sentence[word_idx]:10} -> {pred_label:8} 정답 : {sample_label[word_idx]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "461eeb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 DataSet\n",
    "from torch.utils.data import Dataset\n",
    "class NerDataSet(Dataset):\n",
    "  def __init__(self,sentences,labels, tokenizer,max_len=64) -> None:\n",
    "    self.sentences = sentences\n",
    "    self.labels = labels\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "  def __len__(self):\n",
    "    return len(self.sentences)\n",
    "  def __getitem__(self,idx):\n",
    "    words = self.sentences[idx]\n",
    "    lbls = self.labels[idx]\n",
    "    encoding = self.tokenizer(\n",
    "        words,\n",
    "        return_tensors='pt',\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=self.max_len,\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "    word_ids = encoding.word_ids(batch_index = 0)\n",
    "    label_ids = []\n",
    "    for w in word_ids:\n",
    "      if w is None:\n",
    "        label_ids.append(-100)\n",
    "      else:\n",
    "        label_ids.append(label2id[lbls[w]])\n",
    "    return {\n",
    "        'input_ids' : encoding['input_ids'].squeeze(),\n",
    "        'attention_mask' : encoding['attention_mask'].squeeze(),\n",
    "        'labels' : torch.tensor(label_ids)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "25a77981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataset = NerDataSet(train_sentences,train_labels,tokenizer,max_len=10)\n",
    "train_loader = DataLoader(train_dataset,batch_size=2,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eeb336e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c0957a",
   "metadata": {},
   "source": [
    "<span style=\"color: Gold\"> 3. 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b92ca865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 선언 및 학습\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "model = SimpleNERModel(num_labels=len(label_list))\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1ddca8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 loss : 1.8437\n",
      "epoch 2 loss : 1.5022\n",
      "epoch 3 loss : 1.3574\n",
      "epoch 4 loss : 1.3915\n",
      "epoch 5 loss : 1.4190\n",
      "epoch 6 loss : 1.5183\n",
      "epoch 7 loss : 1.2911\n",
      "epoch 8 loss : 1.3610\n",
      "epoch 9 loss : 1.2155\n",
      "epoch 10 loss : 1.1997\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "  model.train()\n",
    "  total_loss = 0\n",
    "  for batch in train_loader:\n",
    "    optimizer.zero_grad()\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    logits = model(input_ids, attention_mask)\n",
    "    loss = criterion(logits.view(-1, model.num_labels), labels.view(-1))\n",
    "    total_loss += loss.item()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "  print(f'epoch {epoch+1} loss : {total_loss/len(train_loader):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1f98fcd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['김철수는', '서울에', '산다'] ['B-PER', 'B-LOC', 'O']\n",
      "{'input_ids': tensor([[517, 490, 494,   0, 517,   0, 491,   0, 491,   0, 517,   0,   0,   0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "결과\n",
      "김철수는 -> B-PER 정답 : B-PER\n",
      "김철수는 -> B-PER 정답 : B-PER\n",
      "김철수는 -> B-PER 정답 : B-PER\n",
      "김철수는 -> B-PER 정답 : B-PER\n",
      "서울에 -> B-PER 정답 : B-LOC\n",
      "서울에 -> B-PER 정답 : B-LOC\n",
      "서울에 -> B-PER 정답 : B-LOC\n",
      "서울에 -> B-PER 정답 : B-LOC\n",
      "서울에 -> B-PER 정답 : B-LOC\n",
      "서울에 -> B-PER 정답 : B-LOC\n",
      "산다 -> B-PER 정답 : O\n",
      "산다 -> B-PER 정답 : O\n"
     ]
    }
   ],
   "source": [
    "sample_sentence, sample_label =  train_sentences[0], train_labels[0]\n",
    "print(sample_sentence, sample_label)\n",
    "encoding = tokenizer(\n",
    "        sample_sentence,\n",
    "        return_tensors='pt',\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=20,\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "print(encoding)\n",
    "input_ids = encoding['input_ids'].to(device)\n",
    "attention_mask = encoding['attention_mask'].to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  logits = model(input_ids, attention_mask)\n",
    "  predictions = torch.argmax(logits, dim=-1)[0]\n",
    "print('결과')\n",
    "word_ids = encoding.word_ids(batch_index=0)\n",
    "for i ,word_idx in enumerate(word_ids):\n",
    "  if word_idx is not None:\n",
    "    pred_label = id2label[predictions[i].item()]\n",
    "    print(f'{sample_sentence[word_idx]} -> {pred_label} 정답 : {sample_label[word_idx]}')\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b48b02d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cd2f8a",
   "metadata": {},
   "source": [
    "전체과정 흐름\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
