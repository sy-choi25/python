{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8ffa722",
   "metadata": {},
   "source": [
    "1. 데이터 전처리 단계 : 텍스트를 모델이 이해할 수 있는 숫자로 변환\n",
    "    - `토큰화 & 정수 인코딩` : 문장을 단어와 같은 작은 단위(토큰)로 나누고, 각 단어에 고유한 숫자(정수)를 부여하는 과정\n",
    "    - `워드 임베딩` : 정수 인코딩된 단어들을 의미를 함축한 고차원의 실수 벡터로 변환하는 기술. 모델은 단어간의 의미적 유사성을 학습\n",
    "    - `시퀀스 패딩` : 딥러닝 모델에 입력하려면 모든 문장의 길이를 동일하게 맞춰야함. 문장 길이를 맞추기 위해 특정 숫자 (보통0)을 채워 넣어 문장의 길이를 통일\n",
    "\n",
    "2. 딥러닝 모델링 단계 : 텍스트의 특징을 학습하여 감성 분류\n",
    "    - `Baseline` : 일반 신경망 (Dense) : 워드 임베딩 벡터를 단순히 펼쳐서 입력으로 사용\n",
    "        - 장점: 구현이 간단하여 기준 성능(Baseline)을 확인하기 좋음\n",
    "        - 단점: 단어의 순서나 문맥을 고려하지 못해 성능에 한계\n",
    "    - `Simple RNN(Recurrent Neural Network)` :  순환 신경망으로, 단어의 순서를 고려하여 문맥을 파악할 수 있는 모델. 전 단어의 정보를 다음 단어를 처리할 때 함께 사용\n",
    "    - `LSTM (Long Short-Term Memory)` : Simple RNN의 기울기 소실 문제 해결. 장기적인 정보도 기억\n",
    "    - `Bidirectional LSTM (양방향 LSTM)` : LSTM을 개선. 문장을 앞에서 뒤로 그리고 뒤에서 앞으로 양방향으로 읽어 문맥을 더 정확하게 파악"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04c906c",
   "metadata": {},
   "source": [
    "* 토큰화 : 텍스트를 숫자로 변환하는 과정\n",
    "- 토큰화 3단계\n",
    "    - 1. fit_on_text(texts) -> 가장 빈도가 높은 단어의 인덱스를 구축해서 딕셔너리 생성\n",
    "    - 2. texts_to_sequence(texts) -> 각 문서를 정수 시퀀스 변환\n",
    "    - 3. pad_sequence() -> 길이 정규화 (같은 길이)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953ee355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 딥러닝에서 워드 임베딩 레이어 : 각 단어를 고정된 크기의 실수 벡터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4fa056c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_reviews = [\n",
    "    \"this movie is great and wonderful\",\n",
    "    \"bad movie with poor acting\",\n",
    "    \"great movie absolutely wonderful\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e9ebb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이토치 버전\n",
    "# 토큰화\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3e6f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 분할 및 빈도 계산\n",
    "all_words = []\n",
    "for i in [review.split() for review in sample_reviews]:\n",
    "    all_words.extend(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38afd6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('movie', 3), ('great', 2)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어빈도\n",
    "word_freq = Counter(all_words)\n",
    "word_freq.most_common(2)        # 상위 2개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da16cddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UNK': 1,\n",
       " 'movie': 2,\n",
       " 'great': 3,\n",
       " 'wonderful': 4,\n",
       " 'this': 5,\n",
       " 'is': 6,\n",
       " 'and': 7,\n",
       " 'bad': 8,\n",
       " 'with': 9,\n",
       " 'poor': 10}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. tokenizer 구현\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self,num_words = 10, oov_token = 'UNK'):       # 토크나이저 초기화/ num_words: 토크나이저가 단어장에 포함할 최대 단어 수/ oov_token('Out-Of-Vocabulary') 단어장에 없는 단어를 대체할 특별 토큰. (기본값: 'UNK')\n",
    "        self.num_words = num_words\n",
    "        self.oov_token = oov_token\n",
    "        self.word_index = {}\n",
    "        self.index_word = {}\n",
    "    def fit_on_texts(self,texts):       # 단어장 생성\n",
    "        all_words = []\n",
    "        for i in [review.split() for review in sample_reviews]:\n",
    "            all_words.extend(i)\n",
    "        word_freq = Counter(all_words)\n",
    "        # 빈도 높은 순서로 인덱스 부여\n",
    "        # oov 토큰을 1로 설정\n",
    "        self.word_index[self.oov_token] = 1\n",
    "        self.index_word[1] = self.oov_token\n",
    "        idx = 2      \n",
    "        for word, _ in word_freq.most_common(self.num_words -1):\n",
    "            self.word_index[word] = idx\n",
    "            self.index_word[idx] = word\n",
    "            idx += 1\n",
    "    def texts_to_sequences(self,texts):     # 텍스트를 정수 시퀀스로 변환\n",
    "        ''' 텍스트를 정수 시퀀스로 변환'''\n",
    "        sequences = []\n",
    "        for text in texts:\n",
    "            seq = []\n",
    "            for word in text.split():\n",
    "                # 단어가 vocabulary에 있으면 인덱스를 사용하고 없으면 oov\n",
    "                word_index = self.word_index.get(word,1)\n",
    "                seq.append(word_index)\n",
    "            sequences.append(seq)\n",
    "        return sequences\n",
    "    \n",
    "# tokenizer 생성 및 학습\n",
    "tokenizer = SimpleTokenizer(num_words=10, oov_token='UNK')  \n",
    "tokenizer.fit_on_texts(sample_reviews)\n",
    "tokenizer.word_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c895df01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 2, 6, 3, 7, 4], [8, 2, 9, 10, 1], [3, 2, 1, 4]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 텍스트를 시퀀스로 변환\n",
    "sequences = tokenizer.texts_to_sequences(sample_reviews)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9580bb00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  5,  2,  6,  3,  7,  4],\n",
       "       [ 0,  0,  0,  0,  0,  8,  2,  9, 10,  1],\n",
       "       [ 0,  0,  0,  0,  0,  0,  3,  2,  1,  4]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 패딩 구현 - 문자열의 길이를 동일하게 맞춘다\n",
    "def pad_sequence_manual(sequence, max_len = 10, padding='pre',value = 0):\n",
    "    '''패딩구현'''\n",
    "    # 1단계 : 결과물을 담을 빈 리스트 생성\n",
    "    padded = []\n",
    "    # 2단계 : 입력된 모든 시퀀스에 대해 반복 작업 수행\n",
    "    for seq in sequences :\n",
    "        # 3단계 : 현재 시퀀스의 길이가 최대길이보다 긴지 짧은지 확인\n",
    "        if len(seq) >= max_len:\n",
    "            if padded == 'pre':\n",
    "                padded_seq = seq[-max_len:]\n",
    "            else:\n",
    "                padded_seq = seq[:max_len]\n",
    "        else:\n",
    "            pad_length = max_len-len(seq)\n",
    "            if padding == 'pre':\n",
    "                padded_seq = [value]*pad_length + seq\n",
    "            else:\n",
    "                padded_seq = seq + [value]*pad_length\n",
    "        padded.append(padded_seq)\n",
    "    return np.array(padded)\n",
    "padded = pad_sequence_manual(sequences)\n",
    "padded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b16540bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch tensor 변환\n",
    "sequence_tensor = torch.LongTensor(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb4a7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_reviews = [\n",
    "    \"this movie is great and wonderful\",\n",
    "    \"bad movie with poor acting\",\n",
    "    \"great movie absolutely wonderful\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46099e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "패딩된 시퀀스 형태 : torch.Size([3, 10])\n",
      "첫번째 : tensor([0, 0, 0, 0, 5, 2, 6, 3, 7, 4])\n",
      "입력형태 : torch.Size([3, 10])\n",
      "출력형태 : torch.Size([3, 10, 8])\n"
     ]
    }
   ],
   "source": [
    "# 2. 워드 임베딩\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import torch.nn as nn\n",
    "print(f'패딩된 시퀀스 형태 : {sequence_tensor.shape}')\n",
    "print(f'첫번째 : {sequence_tensor[0]}')\n",
    "# pytorch embedding 레이어 생성\n",
    "# num_embeddings 어휘의 크기\n",
    "# embedding_dim 각 단어를 몇차원 벡터로 표현할 건지\n",
    "# padding_idx 길이 맞출 때 채우는 값\n",
    "embedding_layer = nn.Embedding(num_embeddings=1000, embedding_dim=8, padding_idx=0)\n",
    "embedded = embedding_layer(sequence_tensor)\n",
    "print(f'입력형태 : {sequence_tensor.shape}')\n",
    "print(f'출력형태 : {embedded.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61878f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 id 0 : [0. 0. 0. 0.]--8차원 중 처음 4개\n",
      "단어 id 0 : [0. 0. 0. 0.]--8차원 중 처음 4개\n",
      "단어 id 0 : [0. 0. 0. 0.]--8차원 중 처음 4개\n"
     ]
    }
   ],
   "source": [
    "# 임베딩 벡터 상세 분석\n",
    "# 샘플데이터의 첫 3개 단어 임베딩\n",
    "for word_idx in range(3):\n",
    "    embedded_vec = embedded[0,word_idx].detach().numpy()\n",
    "    word_id = sequence_tensor[0,word_idx].item()\n",
    "    print(f'단어 id { word_id} : {embedded_vec[:4]}--8차원 중 처음 4개')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cf8bcbac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임베딩 행령 형태 : (1000, 8)\n",
      "패딩(id=0)의 임베딩 [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "단어(id=5)의 임베딩 [ 0.42097154 -1.3240429   1.4650896   0.05463006 -0.4246347   0.2993056\n",
      " -0.7474072  -0.04265463]\n"
     ]
    }
   ],
   "source": [
    "# 임베딩 행렬\n",
    "embedding_matrix = embedding_layer.weight.detach().numpy()\n",
    "print(f'임베딩 행령 형태 : {embedding_matrix.shape}')\n",
    "print(f'패딩(id=0)의 임베딩 {embedding_matrix[0]}')\n",
    "print(f'단어(id=5)의 임베딩 {embedding_matrix[5]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07153737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rnn 적용\n",
    "class RnnModule(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(RnnModule, self).__init__()\n",
    "        self.embdding = nn.Embedding(vocab_size,embedding_dim,padding_idx=0)\n",
    "        self.rnn  = nn.RNN(embedding_dim,hidden_dim,batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        x_emb =  self.embdding(x)  #(batch,seq_len, embedding_dim)\n",
    "        rnn_out, h_n = self.rnn(x_emb) # rnn_out(batch,seq_len,hidden_dim)\n",
    "                                     # h_n  (1, batch,hidden_dim)\n",
    "        # 마지막 스텝의 출력\n",
    "        last_output = rnn_out[:,-1,:]   # (batch,hidden_dim)\n",
    "        output = self.sigmoid(self.fc(last_output))\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
