{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d56d6067",
   "metadata": {},
   "source": [
    "<span style=\"color: Gold\"> Extractive QA </span> : 책에서 문제의 답을 찾는다\n",
    "- `start-logits` : 각 단어가 답의 시작인 확률 계산\n",
    "- `end-logits` : 각 단어가 답의 끝일 확률 계산\n",
    "- `argmaxV` : 가장 확률이 높은 구간 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2462a157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extractive QA 시각화\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df629443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e05528ca4f334e0b81efbe9fd76ee243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SAMSUNG\\miniconda3\\envs\\llm_env\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\SAMSUNG\\.cache\\huggingface\\hub\\models--distilbert-base-cased-distilled-squad. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6df1220173de4e9d9cdbe60b93c80149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "319cd5c88fa7404e8c3d265cc28ae023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a2afa4cf7884726ac48aa2acb7200ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18c5d859ad4b4760a6c1a42d6e83794d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_NAME = 'distilbert-base-cased-distilled-squad'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff68cea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the capital of France?\"\n",
    "context = \"Paris is the capital and largest city of France. The city has a population of 2.1 million.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fb1b07f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1327, 1110, 1103, 2364, 1104, 1699,  136,  102, 2123, 1110, 1103,\n",
       "         2364, 1105, 2026, 1331, 1104, 1699,  119, 1109, 1331, 1144,  170, 1416,\n",
       "         1104,  123,  119,  122, 1550,  119,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰나이제이션\n",
    "\n",
    "inputs = tokenizer(question, context, return_tensors='pt')\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee43aa49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] What is the capital of France? [SEP] Paris is the capital and largest city of France. The city has a population of 2. 1 million. [SEP]'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(inputs['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a128427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'What',\n",
       " 'is',\n",
       " 'the',\n",
       " 'capital',\n",
       " 'of',\n",
       " 'France',\n",
       " '?',\n",
       " '[SEP]',\n",
       " 'Paris',\n",
       " 'is',\n",
       " 'the',\n",
       " 'capital',\n",
       " 'and',\n",
       " 'largest',\n",
       " 'city',\n",
       " 'of',\n",
       " 'France',\n",
       " '.',\n",
       " 'The',\n",
       " 'city',\n",
       " 'has',\n",
       " 'a',\n",
       " 'population',\n",
       " 'of',\n",
       " '2',\n",
       " '.',\n",
       " '1',\n",
       " 'million',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰리스트\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "434e1d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[-3.0939, -3.7486, -6.0080, -5.7086, -5.8702, -6.6693, -4.2987, -3.1756,\n",
       "         -4.0037,  9.6050, -2.3164, -0.6722, -0.0465, -4.4179, -1.2335, -2.8592,\n",
       "         -4.7219,  0.3473, -2.1641, -0.4413, -2.7191, -5.4559, -4.6687, -4.0404,\n",
       "         -6.9446, -2.4015, -6.9144, -4.8101, -4.1148, -4.0998, -4.0035]]), end_logits=tensor([[-1.6628, -3.1275, -5.5114, -6.3964, -5.4165, -6.5068, -3.2359, -3.9798,\n",
       "         -3.7454,  9.3340, -0.6415, -2.9119,  0.9548, -4.1359, -1.7446, -0.0305,\n",
       "         -4.1837,  3.6947,  1.4507, -3.4107, -3.0052, -5.5297, -5.5761, -3.7925,\n",
       "         -6.3793, -3.9835, -5.3027, -4.0287, -0.2574, -1.0192, -3.7452]]), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예측\n",
    "with torch.no_grad():\n",
    "  outputs = model(**inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0aa3c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 31])\n",
      "모델의 답변 : Paris\n",
      "시작위치 : 9 토큰 Paris\n",
      "종료위치 : 10 토큰 Paris\n",
      "신뢰도 : 0.9952554702758789\n"
     ]
    }
   ],
   "source": [
    "# 로짓을 확률로 변환\n",
    "start_prob = torch.softmax(outputs.start_logits, dim=-1)[0].numpy() # dim은 torch.Size에서 인덱스 1 또는 -1에 값이 오도록 한다\n",
    "print(outputs.start_logits.shape)\n",
    "end_prob = torch.softmax(outputs.end_logits, dim=-1)[0].numpy()\n",
    "\n",
    "# 답변추출\n",
    "answer_start = np.argmax(start_prob)\n",
    "answer_end = np.argmax(end_prob) + 1\n",
    "answer_tokens = tokens[answer_start:answer_end]\n",
    "answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
    "\n",
    "print(f'모델의 답변 : {answer}')\n",
    "print(f'시작위치 : {answer_start} 토큰 {tokens[answer_start]}')\n",
    "print(f'종료위치 : {answer_end} 토큰 {tokens[answer_end-1]}')\n",
    "print(f'신뢰도 : {start_prob[answer_start] * end_prob[answer_end-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d111b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "시작위치 : 1 토큰 Paris 확률: 0.9996962547302246\n",
      "시작위치 : 2 토큰 France 확률: 9.534510172670707e-05\n",
      "시작위치 : 3 토큰 capital 확률: 6.430923531297594e-05\n",
      "시작위치 : 4 토큰 The 확률: 4.333175820647739e-05\n",
      "시작위치 : 5 토큰 the 확률: 3.4399869036860764e-05\n",
      "종료위치 : 1 토큰 Paris 확률: 0.9955578446388245\n",
      "종료위치 : 2 토큰 France 확률: 0.003539448603987694\n",
      "종료위치 : 3 토큰 . 확률: 0.00037531490670517087\n",
      "종료위치 : 4 토큰 capital 확률: 0.00022857278236187994\n",
      "종료위치 : 5 토큰 city 확률: 8.533227082807571e-05\n"
     ]
    }
   ],
   "source": [
    "# 상위 5개 후보 출력\n",
    "# 시작위치 상위 5개\n",
    "top_start = np.argsort(-start_prob)[:5]\n",
    "for i,index in enumerate(top_start,1):\n",
    "  print(f'시작위치 : {i} 토큰 {tokens[index]} 확률: {start_prob[index]}')\n",
    "# 종료위치 상위 5개\n",
    "top_end =  np.argsort(-end_prob)[:5]\n",
    "for i,index in enumerate(top_end,1):\n",
    "  print(f'종료위치 : {i} 토큰 {tokens[index]} 확률: {end_prob[index]}')\n",
    "\n",
    "  # 결론 : 확률이 고르게 분산 -> 답을 못찾은 경우, 특정 구간에 집중되면 확신있는 답을 찾음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5214485",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4be57d",
   "metadata": {},
   "source": [
    "<span style=\"color: Gold\"> offset mattping\n",
    "- 토큰단위로 찾다보면 단어가 분리되어서 자연스럽지 못하기 때문에 문자위치로 변환하는 과정을 거쳐야 자연스러운 문장이 나온다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "910339af",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "문맥 : Paris is the capital of France.\n",
    "        012345678901234 <- 문자 인덱스\n",
    "토큰 : ['Paris', 'is', 'the', 'capital', 'of', 'France','.']\n",
    "          0         1     2     3         <- 토큰인덱스\n",
    "'''\n",
    "#모델은 토큰인덱스로 작동(3번 토큰'capital')\n",
    "#답변위치는 문자인덱스(13번째 문자부터 'capital'시작)\n",
    "offset_mapping = [\n",
    "    (0,5), # 토큰0: 'paris'는 문자 0~5 \n",
    "    (6,8), # 토큰1: 'is'는 문자 6~8\n",
    "    (9,12), \n",
    "    (13,20)\n",
    "]\n",
    "# 활용\n",
    "# 문자위치 - 토큰위치 : Fine-tuning시 답변 레이블 생성\n",
    "# 토큰위치 - 문자위치 : 예측 결과를 원문에 매칭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cae14e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'What', 'is', 'AI', '?', '[SEP]', 'Art', '##ific', '##ial', 'Intelligence', '(', 'AI', ')', 'is', 'the', 'simulation', 'of', 'human', 'intelligence', 'in', 'machines', '.', '[SEP]']\n",
      "인덱스     토큰                       문자위치              원문매칭\n",
      "0        [CLS]                     N/A                '[특수 토큰]'   \n",
      "1        What                      (  0,   4)         'What'   \n",
      "2        is                        (  5,   7)         'is'   \n",
      "3        AI                        (  8,  10)         'AI' \n",
      "4        ?                         ( 10,  11)         '?'   \n",
      "5        [SEP]                     N/A                '[특수 토큰]'   \n",
      "6        Art                       (  0,   3)         'Wha'   \n",
      "7        ##ific                    (  3,   7)         't is'   \n",
      "8        ##ial                     (  7,  10)         ' AI'   \n",
      "9        Intelligence              ( 11,  23)         'Artificial I' \n",
      "10       (                         ( 24,  25)         't'   \n",
      "11       AI                        ( 25,  27)         'el' \n",
      "12       )                         ( 27,  28)         'l'   \n",
      "13       is                        ( 29,  31)         'ge'   \n",
      "14       the                       ( 32,  35)         'ce '   \n",
      "15       simulation                ( 36,  46)         'AI) is the'   \n",
      "16       of                        ( 47,  49)         'si'   \n",
      "17       human                     ( 50,  55)         'ulati'   \n",
      "18       intelligence              ( 56,  68)         'n of human i'   \n",
      "19       in                        ( 69,  71)         'te'   \n",
      "20       machines                  ( 72,  80)         'ligence '   \n",
      "21       .                         ( 80,  81)         'i'   \n",
      "22       [SEP]                     N/A                '[특수 토큰]'   \n"
     ]
    }
   ],
   "source": [
    "# 토큰위치 <-> 문자위치 변환\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "question = \"What is AI?\"\n",
    "context = \"Artificial Intelligence (AI) is the simulation of human intelligence in machines.\"\n",
    "inputs = tokenizer(\n",
    "    question,\n",
    "    context,\n",
    "    return_tensors = 'pt',\n",
    "    return_offsets_mapping = True\n",
    ")\n",
    "3# 토큰리스트 \n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "print(tokens)\n",
    "offset_mapping = inputs['offset_mapping'][0].tolist()\n",
    "# 전체 텍스트(질문+문맥)\n",
    "full_text = question + context\n",
    "# 토큰별로 위치를 매핑\n",
    "print(f\"{'인덱스': <8}{'토큰':<25}{'문자위치':<18}{'원문매칭'}\")\n",
    "for idx, (token, (start, end)) in enumerate(zip(tokens, offset_mapping)):\n",
    "        if start == end == 0:  # 특수 토큰\n",
    "            matched_text = \"[특수 토큰]\"\n",
    "            position = \"N/A\"\n",
    "        else:\n",
    "            # 원래 텍스트에서 추출\n",
    "            matched_text = full_text[start:end]\n",
    "            position = f\"({start:3d}, {end:3d})\"\n",
    "        \n",
    "        # 답변 영역 하이라이트\n",
    "        highlight = \"\" if \"AI\" in token or \"Intelligence\" in token else \"  \"\n",
    "        print(f\"{idx:<8} {token:<25} {position:<18} '{matched_text}' {highlight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0334e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문 길이 보정 + (question_length)\n",
      "절대문자위치 : 12 - 35\n"
     ]
    }
   ],
   "source": [
    "# 실제 답변 위치\n",
    "# 예를 들어, 답변이 하기와 같다면, \n",
    "answer = \"Artificial Intelligence\"\n",
    "# 문맥에서 답변의 문자 위치 찾기\n",
    "answer_start_char = context.index(answer)\n",
    "answer_end_char = answer_start_char + len(answer)\n",
    "\n",
    "# 문자위치 --> 토큰위치로 변환\n",
    "answer_start_token = None\n",
    "answer_end_token = None\n",
    "\n",
    "# 질문부분 건너뛰기 (sequence_idf)\n",
    "sequence_id = inputs.sequence_ids(0)# token_type_ids 를 대체하는게 sequence_ids 라 보면 됨\n",
    "context_start_idx = sequence_id.index(1)# 첫번째 문맥의 시작\n",
    "\n",
    "# 문맥에서 절대 위치를 계산하기 위해 질문 길이 보정\n",
    "question_length = len(question) + 1 # +1은 공백을 의미\n",
    "abs_answer_start = answer_start_char + question_length\n",
    "abs_answer_end = answer_end_char + question_length\n",
    "print(f\"질문 길이 보정 + (question_length)\")\n",
    "print(f\"절대문자위치 : {abs_answer_start} - {abs_answer_end}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97c6e5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "최종 결과\n",
      "================================================================================\n",
      "\n",
      " 변환 성공!\n",
      "   토큰 위치: 18 ~ 19\n",
      "   토큰 리스트: ['intelligence', 'in']\n",
      "   복원된 텍스트: 'intelligence in'\n",
      "   원본 텍스트: 'Artificial Intelligence'\n",
      "\n",
      "   부분 일치 (토크나이제이션으로 인한 차이)\n",
      "\n",
      "================================================================================\n",
      "최종 결과\n",
      "================================================================================\n",
      "\n",
      " 변환 성공!\n",
      "   토큰 위치: 18 ~ 19\n",
      "   토큰 리스트: ['intelligence', 'in']\n",
      "   복원된 텍스트: 'intelligence in'\n",
      "   원본 텍스트: 'Artificial Intelligence'\n",
      "\n",
      "   부분 일치 (토크나이제이션으로 인한 차이)\n",
      "\n",
      "================================================================================\n",
      "최종 결과\n",
      "================================================================================\n",
      "\n",
      " 변환 성공!\n",
      "   토큰 위치: 18 ~ 19\n",
      "   토큰 리스트: ['intelligence', 'in']\n",
      "   복원된 텍스트: 'intelligence in'\n",
      "   원본 텍스트: 'Artificial Intelligence'\n",
      "\n",
      "   부분 일치 (토크나이제이션으로 인한 차이)\n",
      "\n",
      "================================================================================\n",
      "최종 결과\n",
      "================================================================================\n",
      "\n",
      " 변환 성공!\n",
      "   토큰 위치: 18 ~ 19\n",
      "   토큰 리스트: ['intelligence', 'in']\n",
      "   복원된 텍스트: 'intelligence in'\n",
      "   원본 텍스트: 'Artificial Intelligence'\n",
      "\n",
      "   부분 일치 (토크나이제이션으로 인한 차이)\n",
      "\n",
      "================================================================================\n",
      "최종 결과\n",
      "================================================================================\n",
      "\n",
      " 변환 성공!\n",
      "   토큰 위치: 18 ~ 19\n",
      "   토큰 리스트: ['intelligence', 'in']\n",
      "   복원된 텍스트: 'intelligence in'\n",
      "   원본 텍스트: 'Artificial Intelligence'\n",
      "\n",
      "   부분 일치 (토크나이제이션으로 인한 차이)\n",
      "\n",
      "================================================================================\n",
      "최종 결과\n",
      "================================================================================\n",
      "\n",
      " 변환 성공!\n",
      "   토큰 위치: 18 ~ 19\n",
      "   토큰 리스트: ['intelligence', 'in']\n",
      "   복원된 텍스트: 'intelligence in'\n",
      "   원본 텍스트: 'Artificial Intelligence'\n",
      "\n",
      "   부분 일치 (토크나이제이션으로 인한 차이)\n",
      "\n",
      "================================================================================\n",
      "최종 결과\n",
      "================================================================================\n",
      "\n",
      " 변환 성공!\n",
      "   토큰 위치: 18 ~ 19\n",
      "   토큰 리스트: ['intelligence', 'in']\n",
      "   복원된 텍스트: 'intelligence in'\n",
      "   원본 텍스트: 'Artificial Intelligence'\n",
      "\n",
      "   부분 일치 (토크나이제이션으로 인한 차이)\n",
      "\n",
      "================================================================================\n",
      "최종 결과\n",
      "================================================================================\n",
      "\n",
      " 변환 성공!\n",
      "   토큰 위치: 18 ~ 19\n",
      "   토큰 리스트: ['intelligence', 'in']\n",
      "   복원된 텍스트: 'intelligence in'\n",
      "   원본 텍스트: 'Artificial Intelligence'\n",
      "\n",
      "   부분 일치 (토크나이제이션으로 인한 차이)\n",
      "\n",
      "================================================================================\n",
      "최종 결과\n",
      "================================================================================\n",
      "\n",
      " 변환 성공!\n",
      "   토큰 위치: 18 ~ 19\n",
      "   토큰 리스트: ['intelligence', 'in']\n",
      "   복원된 텍스트: 'intelligence in'\n",
      "   원본 텍스트: 'Artificial Intelligence'\n",
      "\n",
      "   부분 일치 (토크나이제이션으로 인한 차이)\n",
      "\n",
      "================================================================================\n",
      "최종 결과\n",
      "================================================================================\n",
      "\n",
      " 변환 성공!\n",
      "   토큰 위치: 18 ~ 19\n",
      "   토큰 리스트: ['intelligence', 'in']\n",
      "   복원된 텍스트: 'intelligence in'\n",
      "   원본 텍스트: 'Artificial Intelligence'\n",
      "\n",
      "   부분 일치 (토크나이제이션으로 인한 차이)\n",
      "\n",
      "================================================================================\n",
      "최종 결과\n",
      "================================================================================\n",
      "\n",
      " 변환 성공!\n",
      "   토큰 위치: 18 ~ 19\n",
      "   토큰 리스트: ['intelligence', 'in']\n",
      "   복원된 텍스트: 'intelligence in'\n",
      "   원본 텍스트: 'Artificial Intelligence'\n",
      "\n",
      "   부분 일치 (토크나이제이션으로 인한 차이)\n",
      "\n",
      "================================================================================\n",
      "최종 결과\n",
      "================================================================================\n",
      "\n",
      " 변환 성공!\n",
      "   토큰 위치: 18 ~ 19\n",
      "   토큰 리스트: ['intelligence', 'in']\n",
      "   복원된 텍스트: 'intelligence in'\n",
      "   원본 텍스트: 'Artificial Intelligence'\n",
      "\n",
      "   부분 일치 (토크나이제이션으로 인한 차이)\n",
      "\n",
      "================================================================================\n",
      "최종 결과\n",
      "================================================================================\n",
      "\n",
      " 변환 성공!\n",
      "   토큰 위치: 18 ~ 19\n",
      "   토큰 리스트: ['intelligence', 'in']\n",
      "   복원된 텍스트: 'intelligence in'\n",
      "   원본 텍스트: 'Artificial Intelligence'\n",
      "\n",
      "   부분 일치 (토크나이제이션으로 인한 차이)\n",
      "\n",
      "================================================================================\n",
      "최종 결과\n",
      "================================================================================\n",
      "\n",
      " 변환 성공!\n",
      "   토큰 위치: 18 ~ 19\n",
      "   토큰 리스트: ['intelligence', 'in']\n",
      "   복원된 텍스트: 'intelligence in'\n",
      "   원본 텍스트: 'Artificial Intelligence'\n",
      "\n",
      "   부분 일치 (토크나이제이션으로 인한 차이)\n",
      "\n",
      "================================================================================\n",
      "최종 결과\n",
      "================================================================================\n",
      "\n",
      " 변환 성공!\n",
      "   토큰 위치: 18 ~ 19\n",
      "   토큰 리스트: ['intelligence', 'in']\n",
      "   복원된 텍스트: 'intelligence in'\n",
      "   원본 텍스트: 'Artificial Intelligence'\n",
      "\n",
      "   부분 일치 (토크나이제이션으로 인한 차이)\n",
      "\n",
      "================================================================================\n",
      "최종 결과\n",
      "================================================================================\n",
      "\n",
      " 변환 성공!\n",
      "   토큰 위치: 18 ~ 19\n",
      "   토큰 리스트: ['intelligence', 'in']\n",
      "   복원된 텍스트: 'intelligence in'\n",
      "   원본 텍스트: 'Artificial Intelligence'\n",
      "\n",
      "   부분 일치 (토크나이제이션으로 인한 차이)\n"
     ]
    }
   ],
   "source": [
    "for idx in range(context_start_idx, len(offset_mapping)):\n",
    "  start, end = offset_mapping[idx]\n",
    "  if start == end == 0:  # 특수 토큰 무시\n",
    "      continue\n",
    "  # 답변 시작 토큰 찾기\n",
    "  if answer_start_token is None and end > abs_answer_start:\n",
    "      answer_start_token = idx\n",
    "      print(f\"    시작 토큰 발견!\")\n",
    "      print(f\"     인덱스: {idx}\")\n",
    "      print(f\"     토큰: '{tokens[idx]}'\")\n",
    "      print(f\"     오프셋: ({start}, {end})\")\n",
    "      print()\n",
    "\n",
    "  # 답변 종료 토큰 찾기\n",
    "  if answer_end_token is None and start >= abs_answer_end:\n",
    "      answer_end_token = idx - 1\n",
    "      print(f\"    종료 토큰 발견!\")\n",
    "      print(f\"      인덱스: {idx - 1}\")\n",
    "      print(f\"      토큰: '{tokens[idx-1]}'\")\n",
    "      print(f\"      오프셋: {offset_mapping[idx-1]}\")\n",
    "      break\n",
    "\n",
    "  print(\"\\n\" + \"=\"*80)\n",
    "  print(\"최종 결과\")\n",
    "  print(\"=\"*80)\n",
    "\n",
    "  if answer_start_token and answer_end_token:\n",
    "    print(f\"\\n 변환 성공!\")\n",
    "    print(f\"   토큰 위치: {answer_start_token} ~ {answer_end_token}\")\n",
    "    print(f\"   토큰 리스트: {tokens[answer_start_token:answer_end_token+1]}\")\n",
    "\n",
    "    # 역변환 확인\n",
    "    reconstructed = tokenizer.convert_tokens_to_string(\n",
    "        tokens[answer_start_token:answer_end_token+1]\n",
    "    )\n",
    "    print(f\"   복원된 텍스트: '{reconstructed}'\")\n",
    "    print(f\"   원본 텍스트: '{answer}'\")\n",
    "\n",
    "    if answer.lower() in reconstructed.lower():\n",
    "        print(f\"\\n   검증 성공! 완벽하게 매칭됩니다.\")\n",
    "    else:\n",
    "        print(f\"\\n   부분 일치 (토크나이제이션으로 인한 차이)\")\n",
    "  else:\n",
    "   print(\"\\n 변환 실패 - 답변을 찾을 수 없습니다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3901a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n{\\n  'input_ids' : ...\\n  'attention_mask' : ...\\n  'token_type_ids' : ...\\n  'start_positions' : 18\\n  'end_positions' : 19\\n}\\n\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "# SQuAD 데이터 전처리\n",
    "# SQuAD 모델이 이해할 수 있는 형식으로 바꾸는 과정에 대한 이해\n",
    "\n",
    "sample = {\n",
    "    \"question\": \"When was the Eiffel Tower built?\",\n",
    "    \"context\": \"The Eiffel Tower was built in 1889 for the Paris World's Fair. It was designed by Gustave Eiffel and remains one of the most iconic structures in the world.\",\n",
    "    \"answers\": {\n",
    "        \"text\": [\"1889\"],\n",
    "        \"answer_start\": [31]  # \"1889\"의 문자 시작 위치\n",
    "    }\n",
    "}\n",
    "# 1. 토크나이제이션\n",
    "#[CLS] When was ...[SEP] the eiffel tower ... [SEP]\n",
    "inputs = tokenizer (\n",
    "  sample['question'],\n",
    "  sample['context'],\n",
    "  max_length = 384,\n",
    "  truncation = 'only_second', # 문맥만 자르기\n",
    "  return_offsets_mapping = True,  # 오프셋 매핑\n",
    "  return_tensors = 'pt',\n",
    "  padding = 'max_length'  # 최대 길이로 패딩\n",
    ")\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "print(f'총 토큰 수 : {len(tokens)}')\n",
    "print(f'실제 : {inputs['attention_mask'][0].tolist().count(1)}')\n",
    "print(f'패딩 : {inputs['attention_mask'][0].tolist().count(0)}')\n",
    "\n",
    "# 2. 답변 문자 위치 확인\n",
    "answer_text = sample['answers']['text'][0]\n",
    "answer_start_char = sample['answers']['answer_start'][0]\n",
    "answer_end_char = answer_start_char + len(answer_text)\n",
    "print(f'답변 텍스트 : {answer_text}')\n",
    "print(f'답변 시작 : {answer_start_char}')\n",
    "print(f'답변 종료 : {answer_end_char}')\n",
    "print(f'검증 : {sample[\"context\"][answer_start_char:answer_end_char]}')\n",
    "\n",
    "# 3. sequence_ids로 문맥 범위 찾기\n",
    "# context_range = [10,45] # 토큰10번~45번이 문맥\n",
    "# token_type_id와 유사 / 문장의 종류 0:질문 1:문장 None:특수토큰 또는 패딩\n",
    "\n",
    "sequence_ids = inputs.sequence_ids(0)\n",
    "# 문맥의 시작과 끝\n",
    "context_start = sequence_ids.index(1)\n",
    "context_end = len(sequence_ids) - sequence_ids[::-1].index(1)-1\n",
    "print(f'특수토큰 : {sequence_ids.count(None)}')\n",
    "print(f'0 질문: {sequence_ids.count(0)}')\n",
    "print(f'1 질문: {sequence_ids.count(1)}')\n",
    "print(f'문맥 시작 : {context_start}')\n",
    "print(f'문맥 종료 : {context_end}')\n",
    "print(f'문맥 첫번째 토큰 : {tokens[context_start]}')\n",
    "print(f'문맥 마지막 토큰 : {tokens[context_end]}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
