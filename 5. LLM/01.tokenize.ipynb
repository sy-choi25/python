{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aacf4884",
   "metadata": {},
   "source": [
    "1. 문장 토큰화 sent_tokenize\n",
    "    - 기본 로직\n",
    "        - 마침표, 느낌표, 물음표를 문장 끝 후보로 인식\n",
    "        - 약어 패턴 학습 (Dr, Mr, U.S.A 등)\n",
    "        - 대문자로 시작하는지\n",
    "    -  통계적 모델을 사용해 진짜 문장 경계인지 판단\n",
    "    - 다국어 지원\n",
    "    - 약어와 실제 문장끝을 구분하는 기게학습 모델 내장\n",
    "\n",
    "2. 단어 토큰화 word_tokenize\n",
    "    - 기본 로직\n",
    "        - 규칙 기준\n",
    "        - 공백기준으로 단어분리\n",
    "        - 구두점을 별도 토큰으로 분리\n",
    "        - 축약형 처리 (it's --> it, s 로 구분)\n",
    "        - 소유격 처리 (Let's --> let, s 로 구분)\n",
    "        - 구두점 기반 WordPunctTokenizer\n",
    "        - It's ---> It, ' , s 로 구분\n",
    "\n",
    "3. 정규 표현식을 이용한 토큰화 regex / regexp\n",
    "\n",
    "4. 노이즈와 불용어 제거\n",
    "    - set자료구조를 사용 : 중복제거\n",
    "    - List Comprehension : 필터링\n",
    "    - NLTK 불용어 사전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b408643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\samsung\\anaconda3\\envs\\deep\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: click in c:\\users\\samsung\\anaconda3\\envs\\deep\\lib\\site-packages (from nltk) (8.3.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\samsung\\anaconda3\\envs\\deep\\lib\\site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\samsung\\anaconda3\\envs\\deep\\lib\\site-packages (from nltk) (2025.10.23)\n",
      "Requirement already satisfied: tqdm in c:\\users\\samsung\\anaconda3\\envs\\deep\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\samsung\\anaconda3\\envs\\deep\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c086e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "084fc5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\SAMSUNG\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package webtext to\n",
      "[nltk_data]     C:\\Users\\SAMSUNG\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package webtext is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SAMSUNG\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\SAMSUNG\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('webtext')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0265f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello everyone.',\n",
       " \"It's good to see to you.\",\n",
       " \"Let's start out text mining class\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장분류 -> 리스트 형태로 출력\n",
    "    # 마침표는 문장을 끝으로 인식\n",
    "    # 대문자로 시작하면 문장의 시작으로 인식\n",
    "sentence = \"Hello everyone. It's good to see to you. Let's start out text mining class\"\n",
    "sent_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99df322f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['안녕하세요, 여러분.', '만나서 반갑습니다.', '이제 학습을 시작해 볼까요?']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 한글\n",
    "sentence_kor = \"안녕하세요, 여러분. 만나서 반갑습니다. 이제 학습을 시작해 볼까요?\"\n",
    "sent_tokenize(sentence_kor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a253b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['안녕하세요', ',', '여러분', '.', '만나서', '반갑습니다', '.', '이제', '학습을', '시작해', '볼까요', '?']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어단위 구분 -> 공백을 기준을 나뉨\n",
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(sentence_kor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59f306e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'everyone',\n",
       " '.',\n",
       " 'It',\n",
       " \"'\",\n",
       " 's',\n",
       " 'good',\n",
       " 'to',\n",
       " 'see',\n",
       " 'to',\n",
       " 'you',\n",
       " '.',\n",
       " 'Let',\n",
       " \"'\",\n",
       " 's',\n",
       " 'start',\n",
       " 'out',\n",
       " 'text',\n",
       " 'mining',\n",
       " 'class']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "WordPunctTokenizer().tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "baa6cbed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정규식 토큰화\n",
    "import re\n",
    "re.findall(\"[abc]\", \"how are you, boy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f83da14e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sorry', ',', 'I', 'could', \"n't\", 'go', 'movie', 'yesterday']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 노이즈와 불용어 제거\n",
    "from nltk.corpus import stopwords\n",
    "english_stops = stopwords.words('english')\n",
    "test1 = \"Sorry, I couldn't go to movie yesterday\"\n",
    "\n",
    "tokens = word_tokenize(test1)\n",
    "[token for token in tokens if token not in english_stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d82f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"[\\\\w]+\")\n",
    "tokens = tokenizer.tokenize(test1.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c381fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RegexpTokenizer(pattern='[a~z]', gaps=False, discard_empty=True, flags=re.UNICODE|re.MULTILINE|re.DOTALL)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 소문자 a~z로 이루어진 문자열에서 4글자이상\n",
    "RegexpTokenizer(\"[a~z]{4,}\")\n",
    "RegexpTokenizer(\"[\\\\w']{3,}\") # 3글자 이상\n",
    "RegexpTokenizer(\"[a~z]\")    # 어포스트로피를 패턴에서 제외 can't can t \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1417f683",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5eca3ae",
   "metadata": {},
   "source": [
    "<span style=\"color: Gold\">어간추출 (stemming)  \n",
    "- 줄기 stem ->  단어에서 불필요한 요소를 제거하고 남는 핵심형태\n",
    "- 단어는 다양한 형태... 복수형 과거형과 같은 시제변환, 복수형\n",
    "- 단어를 통일\n",
    "    - walk(걷다)    walks/ walking/ walked  -> 어간 walk로 통일\n",
    "    - 먹는다       -> '먹-'으로 묶어서 컴퓨터가 같은 단어로 인식"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97f1db0",
   "metadata": {},
   "source": [
    "1. PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f100cc03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('cook', 'cookeri', 'cookbook')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmer.stem('cooking'), stemmer.stem('cookery'), stemmer.stem('cookbooks')\n",
    "\n",
    "# => Porterstemmer 규칙기반이라서 완벽하지 못함 -> 속도가 빠름, 의미가 달라질 수도 있음\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96bbb57",
   "metadata": {},
   "source": [
    "2. LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3a6c09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('cook', 'cookery', 'cookbook')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "stemmer.stem('cooking'), stemmer.stem('cookery'), stemmer.stem('cookbooks')\n",
    "# => 더 많은 규칙이 적용 -> 과도한 츅약 위험"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9416d61a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dad03b",
   "metadata": {},
   "source": [
    "1. 표제어 추출 Lemmatization\n",
    "    - Lemma 단어의 사전 기본형\n",
    "    - 단어의 변형 (시제, 복수, 비교급) 제거하고 사전(headword)에 나오는 <span style=\"color: yellow;\">정확한 원형으로 바꾸는 과정<span>\n",
    "    - 어간처럼 단어줄기가 아니라, 맥락과 품사를 고려한 올바른 형태\n",
    "    - better(더 좋은) -> 표제어 good\n",
    "    - 먹었다 -> 먹다 (동사원형)\n",
    "    - 알고리즘 : 형태소 분석기(<span style=\"color: yellow;\">konlpy</span>)를 사용해 품사(명사, 동사)를 보고 정확히 변환\n",
    "\n",
    "\n",
    "2.  주요 목적\n",
    "    - 어간추출처럼 대충 줄이지 않고 맥락에 맞는 정확한 단어로 만들어서 NLP 품질 향상\n",
    "    - 단점 사전에 의존해서 언어/ 맥락 제한\n",
    "\n",
    "3. 형태 설정  \n",
    "    - n (명사)\n",
    "    - v (동사)\n",
    "    - a (형용사)\n",
    "    - r (부사)\n",
    "    - lemmatizer.lemmatize('cooking',pos = 'v')     # 품사를 동사(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a089b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cook'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('cooking')     # 기본이 명사로 인식\n",
    "lemmatizer.lemmatize('cooking',pos = 'v')     # 품사를 동사(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d25fd70d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello', 'NN'),\n",
       " ('everyone', 'NN'),\n",
       " ('.', '.'),\n",
       " ('It', 'PRP'),\n",
       " (\"'s\", 'VBZ'),\n",
       " ('good', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('see', 'VB'),\n",
       " ('you', 'PRP'),\n",
       " ('.', '.'),\n",
       " ('Let', 'VB'),\n",
       " (\"'s\", 'POS'),\n",
       " ('start', 'VB'),\n",
       " ('out', 'RP'),\n",
       " ('text', 'NN'),\n",
       " ('mining', 'NN'),\n",
       " ('class', 'NN'),\n",
       " ('!', '.')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  품사태깅\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = \"hello everyone. It's good to see you. Let's start out text mining class!\"\n",
    "tokens = word_tokenize(tokens)\n",
    "nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1305fe58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets_json to\n",
      "[nltk_data]     C:\\Users\\SAMSUNG\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping help\\tagsets_json.zip.\n"
     ]
    }
   ],
   "source": [
    "# 품사 태그 정보 확인\n",
    "nltk.download('tagsets_json')\n",
    "nltk.help.upenn_tagset('VB') # 명사 동사 형용사 NN VB JJ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
