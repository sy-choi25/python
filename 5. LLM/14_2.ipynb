{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BpK4bMbFuC0E"
      },
      "outputs": [],
      "source": [
        "# Transformer 는 encoder -> decoder\n",
        "# encoder 를 이용해서 만든 언어모델 BERT : 감성분류, 스펨, 개체명인식, 유사도측정 --> 추출\n",
        "# decoder 를 이용해서 만든 언어모델 GPT : 언어 추론 요약, QA 챗봇  --> 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lBy5odBvyzT"
      },
      "outputs": [],
      "source": [
        "# Bert가 잘하는 것 : 분류, 빈칸 추론, 문장 임베딩"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijLggH6RwPUk"
      },
      "outputs": [],
      "source": [
        "!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmEhOKGRvVXZ",
        "outputId": "c7c744d7-d541-4a19-dfa9-22449e47a868"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"klue/bert-base\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iGnPvyLxAge",
        "outputId": "5860bc84-f991-4ba7-e020-e20a926033a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-5.1105,  6.6659, -4.8473,  ..., -4.9243, -3.7496, -4.6652]])\n",
            "서울\n",
            "광화문\n",
            "부산\n",
            "평양\n",
            "수원\n"
          ]
        }
      ],
      "source": [
        "# 1. 상식 추론\n",
        "import torch\n",
        "text = '한국의 수도는 [MASK]입니다.'\n",
        "inputs = tokenizer(text,return_tensors='pt')\n",
        "mask_token_index = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1]\n",
        "# 추론\n",
        "with torch.no_grad():\n",
        "  outputs = model(**inputs)\n",
        "predictions = outputs.logits\n",
        "print(predictions[0,mask_token_index,:])\n",
        "masked_prediction = predictions[0,mask_token_index,:].topk(5)\n",
        "for i, index_t in enumerate(masked_prediction.indices[0]):\n",
        "  index = index_t.item()\n",
        "  print(tokenizer.decode([index]))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
