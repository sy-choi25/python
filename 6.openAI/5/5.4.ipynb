{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce95eaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG 체인\n",
    "'''\n",
    "사용자질문\n",
    "임베딩 변환    : 벡터로 변환\n",
    "VectorDB 검색  : 유사한 문서 검색\n",
    "문서 포맷팅    : 검색된 문서를 텍스트로 정리\n",
    "프롬프트 구성  : 컨텍스트 + 질문 결합\n",
    "LLM 호출       : 답변생성\n",
    "출력을 파싱    : 문자열로 변환\n",
    "최종답변\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9331b804",
   "metadata": {},
   "source": [
    "invoke : Python용 작업(task) 실행 라이브러리\n",
    "- 주로 프로젝트 관리, 스크립트 실행, 배포 자동화 등에 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09c82601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement langchain_core.prompt (from versions: none)\n",
      "ERROR: No matching distribution found for langchain_core.prompt\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain_core.prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41a48bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프롬프트 템플릿 : 재사용 가능한 프롬프트 구조를 정의\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    ('system', '당신은 {role} 입니다.'),\n",
    "    ('human' , '{question}')\n",
    "])\n",
    "\n",
    "# 변수 채우기\n",
    "prompt = template.invoke({\n",
    "    'role' : 'AI 어시스턴트',\n",
    "    'question' : 'RAG란 무엇인가요?'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fccd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프롬프트 유형\n",
    "# 단일 문자열\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "template = PromptTemplate.from_template('''\n",
    "다음 질문에 답변하세요\n",
    "질문 : {question}\n",
    "답변 : ''')\n",
    "\n",
    "# 채팅 형식\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    ('system' , '시스템 지시사항'),\n",
    "    ('human' , '사용자 질문: {question}'),\n",
    "    (\"assistant\", \"이전 답변 (선택)\"),\n",
    "    (\"human\", \"후속 질문\")\n",
    "])\n",
    "\n",
    "# 프롬프트 설계 원칙\n",
    "'''\n",
    "1. 역할 정의 (Role Definition)\n",
    "     \"당신은 전문적인 기술 문서 Q&A 시스템입니다.\"    \n",
    "2. 컨텍스트 제공 (Context)                          \n",
    "    \"다음은 참조할 문서입니다: {context}\"        \n",
    "3. 명확한 지시 (Instructions)                       \n",
    "    - 컨텍스트 내 정보만 사용                      \n",
    "    - 모르면 모른다고 답변                          \n",
    "    - 한국어로 답변                                  \n",
    "4. 질문 (Question)                                  \n",
    "    \"질문: {question}\"                           \n",
    "5. 출력 형식 (Output Format)                        \n",
    "    \"답변은 구조화된 형태로 작성하세요.\" \n",
    "'''\n",
    "\n",
    "# 효고적인 RAG 프롬프트 작성 예시\n",
    "rag_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', '''당신은 제공된 문맥(Context)를 바탕으로 질문에 답변하는 AI 어시스턴트입니다.\n",
    "    ## 규칙\n",
    "    1. 제공된 문맥 내의 정보만을 사용하여 답변하세요.\n",
    "    2. 문맥에 없는 정보는 추측하지 말고 \"제공된 문서에서 해당 정보를 찾을 수 없습니다.\"라고 답하세요.\n",
    "    3. 답변은 한국어로 명확하고 간결하게 작성하세요.\n",
    "    4. 가능하면 구조화된 형태(목록, 번호 등)로 답변하세요.\n",
    "    5. 확실하지 않은 내용은 그 점을 명시하세요\n",
    "    '''),\n",
    "    ('human', '''## 참조문맥\n",
    "    {Context}\n",
    "    ## 질문\n",
    "    {question}\n",
    "    ## 답변 ''')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f36c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCEL(LangChain Expression Language)\n",
    "# 파이프연산자를 이용해서 직관적으로 연결\n",
    "# 기존 LangChain 방식은 체인을 만들 때 코드가 너무 길고 복잡했다. 프롬프트 만들고 모델 만들고 출력 파서를 붙이고 체인으로 묶고\n",
    "# → 이런 흐름을 한 줄로 만들 수 있게 한 게 LCEL이다.\n",
    "\n",
    "# 전통적인 lagecy 방식\n",
    "# result = parser.parse(llm.invoke(prompt.fromat(question = '질문')))\n",
    "\n",
    "# LECL 방식\n",
    "# chain = prompt | llm | parser\n",
    "# result = chain.invoke({'question' : '질문'})\n",
    "\n",
    "# 핵심 Runnable 컴포넌트\n",
    "from langchain_core.runnable import RunnablePassThrough\n",
    "# 질문을 그대로 전달하면서 context는 별도 처리\n",
    "chain = {\n",
    "    'context' : retriever | format_docs,\n",
    "    'question' : RunnablePassThrough\n",
    "} | prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901088a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
