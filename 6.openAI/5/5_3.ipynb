{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0afb01fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "load_dotenv()\n",
    "\n",
    "# 필수 라이브러리 로드\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43197281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "청크데이터 로드\n"
     ]
    }
   ],
   "source": [
    "# 이전 단계 데이터 로드 또는 재생성\n",
    "print('청크데이터 로드')\n",
    "chunks_file = 'chunk_output_pkl'\n",
    "\n",
    "if os.path.exists(chunks_file):\n",
    "    with open(chunks_file, 'rb') as f:\n",
    "        doc_chunks = pickle.load(f)\n",
    "else :\n",
    "    # 파일이 없으면 새로 생성\n",
    "    print('5_1.ipynb')\n",
    "\n",
    "# openai 임베딩 모델 초기화\n",
    "embedding_model = OpenAIEmbeddings(\n",
    "    model = 'text-embedding-3-small'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "622c086f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력텍스트 : RAG는 검색 증강 생성 기술입니다\n",
      "벡터차원 : 1536\n",
      "벡터일부 : 0.0135, 0.0530... -0.0635\n",
      "소요시간 : 0.328\n"
     ]
    }
   ],
   "source": [
    "# 단일 텍스트 임베딩 텍스트\n",
    "test_text = 'RAG는 검색 증강 생성 기술입니다'\n",
    "\n",
    "start_time = time.time()\n",
    "test_embedding = embedding_model.embed_query(test_text)\n",
    "elapsed = time.time() - start_time\n",
    "print(f'입력텍스트 : {test_text}')\n",
    "print(f'벡터차원 : {len(test_embedding)}')\n",
    "print(f'벡터일부 : {test_embedding[0]:.4f}, {test_embedding[1]:.4f}... {test_embedding[-1]:.4f}')\n",
    "print(f'소요시간 : {elapsed:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "601266fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유사도 계산\n",
    "import numpy as np\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm1*norm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a063023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기존 문장 : ['RAG는 검색 증강 생성 기술입니다.', 'RAG는 문서 검색과 답변 생성을 결합합니다.', '벡터 데이터베이스는 임베딩을 저장합니다.', '오늘 날씨가 매우 좋습니다.']\n",
      "유사도 비교 결과\n",
      " 1 RAG는 문서 검색과 답변 생성을 결합합니다. --> 0.6779\n",
      " 2 벡터 데이터베이스는 임베딩을 저장합니다. --> 0.2335\n",
      " 3 오늘 날씨가 매우 좋습니다. --> 0.0664\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    \"RAG는 검색 증강 생성 기술입니다.\",         # 기준 문장\n",
    "    \"RAG는 문서 검색과 답변 생성을 결합합니다.\",  # 유사한 문장\n",
    "    \"벡터 데이터베이스는 임베딩을 저장합니다.\",   # 관련 있는 문장\n",
    "    \"오늘 날씨가 매우 좋습니다.\",               # 관련 없는 문장\n",
    "]\n",
    "\n",
    "# 모든 문장을 임베딩\n",
    "embeddings = [embedding_model.embed_query(sent) for sent in test_sentences]\n",
    "# 기존 문장과 유사도 비교\n",
    "base_embedding = embeddings[0]\n",
    "print(f'기존 문장 : {test_sentences}')\n",
    "print(f'유사도 비교 결과')\n",
    "for i,(sent, emb) in enumerate(zip(test_sentences[1:],embeddings[1:]), 1):\n",
    "    similarity = cosine_similarity(base_embedding,emb)\n",
    "    print(f' {i} {sent[:30]} --> {similarity:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9268011c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VectorDB 구축완료\n",
      "저장된 청크 수 :10\n",
      "소요시간 :1.02\n"
     ]
    }
   ],
   "source": [
    "# Chroma DB 벡터DB\n",
    "# 동작방식\n",
    "# 저장 : 텍스트(청크) -> 임베딩(벡터) -> VectorDB(저장)\n",
    "# 검색: 질문 -> 임베딩(벡터) -> 유사도검색 -> top-k 문서 반환\n",
    "\n",
    "# chroma DB 청크 저장\n",
    "start_time = time.time()\n",
    "# cromaDB 생성(인메모리 방식)\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents = doc_chunks,\n",
    "    collection_name = '5_1.ipynb',\n",
    "    embedding = embedding_model,\n",
    ")\n",
    "elapsed = time.time() - start_time\n",
    "print(f'VectorDB 구축완료')\n",
    "print(f'저장된 청크 수 :{len(doc_chunks)}')\n",
    "print(f'소요시간 :{elapsed:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e949572a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: RAG란 무엇인가요?\n",
      "1rag_concept.txt (거리:0.8409)\n",
      "   RAG의 핵심 구성요소: Retriever(검색기), Generator(생성기), VectorStore(벡터저장소)\n",
      "2rag_concept.txt (거리:0.8409)\n",
      "   RAG의 핵심 구성요소: Retriever(검색기), Generator(생성기), VectorStore(벡터저장소)\n",
      "질문: VectorDB에는 어떤 종류가 있나요?\n",
      "1vectordb_intro.txt (거리:0.6635)\n",
      "   VectorDB(벡터 데이터베이스)는 고차원 벡터를 효율적으로 저장하고 검색하는 데이터베이스입니다.\n",
      "2vectordb_intro.txt (거리:0.6636)\n",
      "   VectorDB(벡터 데이터베이스)는 고차원 벡터를 효율적으로 저장하고 검색하는 데이터베이스입니다.\n",
      "질문: LangChain의 구성 요소는\n",
      "1langchain_intro.txt (거리:0.6945)\n",
      "   LangChain의 주요 구성 요소:         1. Models: 다양한 LLM 제공자(OpenAI, Anthropic, Google 등)\n",
      "2langchain_intro.txt (거리:0.6945)\n",
      "   LangChain의 주요 구성 요소:         1. Models: 다양한 LLM 제공자(OpenAI, Anthropic, Google 등)\n"
     ]
    }
   ],
   "source": [
    "# 테스트 질문\n",
    "test_queries = [\n",
    "    'RAG란 무엇인가요?',\n",
    "    'VectorDB에는 어떤 종류가 있나요?',\n",
    "    'LangChain의 구성 요소는'\n",
    "]\n",
    "for query in test_queries:\n",
    "    print(f'질문: {query}')\n",
    "    # 유사문서 검색 상위 2개\n",
    "    results = vectorstore.similarity_search_with_score(query,k=2)\n",
    "    for i,(doc,score) in enumerate(results, 1):\n",
    "        source = doc.metadata.get('source','unknown')\n",
    "        preview = doc.page_content.strip()[:80].replace('\\n',' ' )\n",
    "        print(f'{i}{source} (거리:{score:.4f})')\n",
    "        print(f'   {preview}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9338bf66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다양한 검색 옵션\n",
      "기본 유사도 검색(similarity)\n",
      "결과 수 : 3개\n",
      "1None\n",
      "2None\n",
      "3None\n",
      "MMR 검색(다양성 고려)\n",
      "결과 수 : 3개\n",
      "1None\n",
      "2None\n",
      "3None\n"
     ]
    }
   ],
   "source": [
    "# 다양한 검색 옵션\n",
    "print('다양한 검색 옵션')\n",
    "# 리트리버 생성\n",
    "print('기본 유사도 검색(similarity)')\n",
    "retriver_basic = vectorstore.as_retriever(\n",
    "    search_type = 'similarity',\n",
    "    search_kwargs = {'k' :3}\n",
    ")\n",
    "results = retriver_basic.invoke('RAG의 장점')\n",
    "print(f'결과 수 : {len(results)}개')\n",
    "for i,doc in enumerate(results, 1):\n",
    "    print(f'{i}{doc.metadata.get('source''unknown')}')\n",
    "\n",
    "print(F'MMR 검색(다양성 고려)')\n",
    "retriver_basic = vectorstore.as_retriever(\n",
    "    search_type = 'mmr',\n",
    "    search_kwargs = {'k' :3,\n",
    "                  'fetch_k': 6, # 먼저 6개의 후보 검색\n",
    "                  'lambda_mult':0.5 # 다양성 가중치(0 = 다양성, 1 = 관련성)\n",
    "    }\n",
    ")\n",
    "results = retriver_basic.invoke('RAG의 장점')\n",
    "print(f'결과 수 : {len(results)}개')\n",
    "for i,doc in enumerate(results, 1):\n",
    "    print(f'{i}{doc.metadata.get('source''unknown')}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8da86e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "메타데이터 필터링\n",
      "결과 수 : 2개\n",
      "1Nonetopic = technique\n",
      "2Nonetopic = technique\n"
     ]
    }
   ],
   "source": [
    "print('메타데이터 필터링')\n",
    "results = vectorstore.similarity_search(\n",
    "    '기술에 대해 설명해 주세요',\n",
    "    k=2,\n",
    "    filter = {'topic':'technique'}\n",
    ")\n",
    "print(f'결과 수 : {len(results)}개')\n",
    "for i,doc in enumerate(results, 1):\n",
    "    print(f'{i}{doc.metadata.get('source''unknown')}topic = {doc.metadata.get('topic')}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b855f91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectordb 영구저장\n",
      "저장경로 : ./chroma_db_rag2\n",
      "저장된 청크수 : 10\n",
      "설정정보 저장 완료 파일명 : vectordb_config.pkl\n"
     ]
    }
   ],
   "source": [
    "# VectorDB 영구 저장(옵션)\n",
    "persist_dir = './chroma_db_rag2'\n",
    "vectorstore_persistent =  Chroma.from_documents(\n",
    "    documents = doc_chunks,\n",
    "    collection_name = 'persistent_rag',\n",
    "    embedding = embedding_model,\n",
    "    persist_directory = persist_dir)\n",
    "print('vectordb 영구저장')\n",
    "print(f'저장경로 : {persist_dir}')\n",
    "print(f'저장된 청크수 : {len(doc_chunks)}')\n",
    "\n",
    "# 설정정보 저장\n",
    "config = {\n",
    "    'persist_directory' : persist_dir,\n",
    "    'collection_name': \"persistent_rag\",\n",
    "    'embedding_model' : 'text-embedding-',\n",
    "    'chunk_count' : len(doc_chunks)\n",
    "}\n",
    "with open('vectordb_config.pkl','wb')as f:\n",
    "    pickle.dump(config,f)\n",
    "\n",
    "print('설정정보 저장 완료 파일명 : vectordb_config.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba361f72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
