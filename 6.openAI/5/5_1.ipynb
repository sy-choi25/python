{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6a419c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\SAMSUNG\\miniconda3\\envs\\llm_env\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import(\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    CharacterTextSplitter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13aff4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_text_splitters\n",
      "  Using cached langchain_text_splitters-1.0.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from langchain_text_splitters) (1.1.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain_text_splitters) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain_text_splitters) (0.4.48)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain_text_splitters) (25.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain_text_splitters) (2.12.4)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain_text_splitters) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain_text_splitters) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain_text_splitters) (4.15.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain_text_splitters) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain_text_splitters) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain_text_splitters) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain_text_splitters) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain_text_splitters) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain_text_splitters) (0.25.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain_text_splitters) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain_text_splitters) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain_text_splitters) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain_text_splitters) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain_text_splitters) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain_text_splitters) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain_text_splitters) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain_text_splitters) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain_text_splitters) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain_text_splitters) (2.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain_text_splitters) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\samsung\\miniconda3\\envs\\llm_env\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain_text_splitters) (1.3.1)\n",
      "Using cached langchain_text_splitters-1.0.0-py3-none-any.whl (33 kB)\n",
      "Installing collected packages: langchain_text_splitters\n",
      "Successfully installed langchain_text_splitters-1.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain_text_splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dc30158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예제 Document 객체\n",
      "page_content : 이것은 예제 문서의 내용입니다\n",
      "page_content : {'source': 'example.txt', 'page': 1, 'author': '홍길동'}\n"
     ]
    }
   ],
   "source": [
    "# 샘플 도큐먼트 객체 새성\n",
    "example_doc = Document(\n",
    "    page_content = '이것은 예제 문서의 내용입니다',\n",
    "    metadata = {'source' : 'example.txt', 'page' : 1, 'author' : '홍길동'}\n",
    ")\n",
    "print('예제 Document 객체')\n",
    "print(f'page_content : {example_doc.page_content}')\n",
    "print(f'page_content : {example_doc.metadata}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77edd7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 문서 생성(외부문서 시뮬레이션)\n",
    "sample_documents = [\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        LangChain은 대규모 언어 모델(LLM)을 활용한 애플리케이션 개발을 위한 프레임워크입니다.\n",
    "        \n",
    "        LangChain의 주요 구성 요소:\n",
    "        1. Models: 다양한 LLM 제공자(OpenAI, Anthropic, Google 등)와 통합\n",
    "        2. Prompts: 프롬프트 템플릿 관리 및 최적화\n",
    "        3. Chains: 여러 구성 요소를 연결하는 파이프라인\n",
    "        4. Memory: 대화 맥락을 유지하기 위한 메모리 시스템\n",
    "        5. Indexes: 문서 검색을 위한 인덱싱 도구\n",
    "        6. Agents: 도구를 사용하여 복잡한 작업을 수행하는 에이전트\n",
    "        \n",
    "        LangChain Expression Language (LCEL)은 체인을 구성하는 선언적 방식으로,\n",
    "        파이프(|) 연산자를 사용하여 컴포넌트들을 직관적으로 연결할 수 있습니다.\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"langchain_intro.txt\", \"topic\": \"framework\", \"importance\": \"high\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        RAG (Retrieval-Augmented Generation)는 검색 증강 생성 기술입니다.\n",
    "        \n",
    "        RAG의 작동 원리:\n",
    "        1. 사용자 질문을 임베딩 벡터로 변환합니다.\n",
    "        2. 벡터 데이터베이스에서 유사한 문서를 검색합니다.\n",
    "        3. 검색된 문서를 컨텍스트로 사용하여 LLM이 답변을 생성합니다.\n",
    "        \n",
    "        RAG의 장점:\n",
    "        - 최신 정보를 반영할 수 있습니다. LLM의 학습 데이터 이후 정보도 활용 가능합니다.\n",
    "        - 환각(Hallucination)을 감소시킵니다. 실제 문서 기반으로 답변하기 때문입니다.\n",
    "        - 출처를 명시할 수 있습니다. 어떤 문서에서 정보를 가져왔는지 추적 가능합니다.\n",
    "        - 도메인 특화가 가능합니다. 특정 분야의 문서만 사용하여 전문적인 답변을 제공합니다.\n",
    "        \n",
    "        RAG의 핵심 구성요소: Retriever(검색기), Generator(생성기), VectorStore(벡터저장소)\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"rag_concept.txt\", \"topic\": \"technique\", \"importance\": \"high\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        VectorDB(벡터 데이터베이스)는 고차원 벡터를 효율적으로 저장하고 검색하는 데이터베이스입니다.\n",
    "        \n",
    "        주요 VectorDB 솔루션:\n",
    "        - ChromaDB: 로컬 개발에 적합한 오픈소스 솔루션. 파이썬 네이티브로 설치가 간편합니다.\n",
    "        - Pinecone: 완전 관리형 클라우드 서비스. 대규모 프로덕션 환경에 적합합니다.\n",
    "        - Weaviate: 그래프 기반 벡터 데이터베이스. 하이브리드 검색을 지원합니다.\n",
    "        - FAISS: Facebook에서 개발한 고성능 라이브러리. 대용량 벡터 검색에 최적화되어 있습니다.\n",
    "        - Milvus: 분산 환경을 지원하는 오픈소스 솔루션입니다.\n",
    "        \n",
    "        임베딩(Embedding)은 텍스트를 숫자 벡터로 변환하는 과정으로,\n",
    "        의미적으로 유사한 텍스트는 벡터 공간에서 가까운 위치에 배치됩니다.\n",
    "        예를 들어, \"고양이\"와 \"강아지\"는 \"자동차\"보다 벡터 공간에서 더 가깝습니다.\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"vectordb_intro.txt\", \"topic\": \"database\", \"importance\": \"medium\"}\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7fda3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "텍스트 분할기\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 분할기\n",
    "print('텍스트 분할기')\n",
    "# 단순 문자기반 분할기\n",
    "char_splitter = CharacterTextSplitter(\n",
    "    separator = '\\n',\n",
    "    chunk_size = 200,\n",
    "    chunk_overlap = 30,\n",
    "    length_function = len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db73a26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 문서길이 : \n",
      "        LangChain은 대규모 언어 모델(LLM)을 활용한 애플리케이션 개발을 위한 프레임워크입니다.\n",
      "        \n",
      "        LangChain의 주요 구성 요소:\n",
      "        1. Models: 다양한 LLM 제공자(OpenAI, Anthropic, Google 등)와 통합\n",
      "        2. Prompts: 프롬프트 템플릿 관리 및 최적화\n",
      "        3. Chains: 여러 구성 요소를 연결하는 파이프라인\n",
      "        4. Memory: 대화 맥락을 유지하기 위한 메모리 시스템\n",
      "        5. Indexes: 문서 검색을 위한 인덱싱 도구\n",
      "        6. Agents: 도구를 사용하여 복잡한 작업을 수행하는 에이전트\n",
      "        \n",
      "        LangChain Expression Language (LCEL)은 체인을 구성하는 선언적 방식으로,\n",
      "        파이프(|) 연산자를 사용하여 컴포넌트들을 직관적으로 연결할 수 있습니다.\n",
      "        자\n",
      "CharacterTextSolitter 결과 : 3개 청크\n",
      "청크별 미리보기\n",
      " 청크1 ( 156자: LangChain은 대규모 언어 모델(LLM)을 활용한 애플리케이션 개발을 위한 프레임워크입니다.                  LangChai  )\n",
      " 청크2 ( 148자: 2. Prompts: 프롬프트 템플릿 관리 및 최적화         3. Chains: 여러 구성 요소를 연결하는 파이프라인         4.  )\n",
      " 청크3 ( 161자: 6. Agents: 도구를 사용하여 복잡한 작업을 수행하는 에이전트                  LangChain Expression Lang  )\n"
     ]
    }
   ],
   "source": [
    "# 첫번째 문서로 테스트\n",
    "test_doc = sample_documents[0]\n",
    "char_splits = char_splitter.split_documents([test_doc])\n",
    "print(f'원본 문서길이 : {(test_doc.page_content)}자')\n",
    "print(f'CharacterTextSolitter 결과 : {len(char_splits)}개 청크')\n",
    "print('청크별 미리보기')\n",
    "for i , chunk in enumerate(char_splits[:3],1):\n",
    "    preview = chunk.page_content.strip()[:80].replace('\\n',' ')\n",
    "    print(f' 청크{i} ( {len(chunk.page_content)}자: {preview}  )')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6987a919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecursiveCharacterTextSplitter 적용\n",
      "원본 문서 : 3개\n",
      "RecursiveCharacterTextSplitter 결과 : 6개 청크\n",
      "LangChain은 대규모 언어 모델(LLM)을 활용한 애플리케이션 개발을 위한 프레임워크입니다.\n",
      "        \n",
      "        LangChain의 주요 구성 요소:\n",
      "        1. Models: 다양한 LLM 제공자(OpenAI, Anthropic, Google 등)와 통합\n",
      "        2. Prompts: 프롬프트 템플릿 관리 및 최적화\n",
      "        3. Chains: 여러 구성 요소를 연결하는 파이프라인\n",
      "        4. Memory: 대화 맥락을 유지하기 위한 메모리 시스템\n",
      "4. Memory: 대화 맥락을 유지하기 위한 메모리 시스템\n",
      "        5. Indexes: 문서 검색을 위한 인덱싱 도구\n",
      "        6. Agents: 도구를 사용하여 복잡한 작업을 수행하는 에이전트\n",
      "        \n",
      "        LangChain Expression Language (LCEL)은 체인을 구성하는 선언적 방식으로,\n",
      "        파이프(|) 연산자를 사용하여 컴포넌트들을 직관적으로 연결할 수 있습니다.\n",
      "RAG (Retrieval-Augmented Generation)는 검색 증강 생성 기술입니다.\n",
      "        \n",
      "        RAG의 작동 원리:\n",
      "        1. 사용자 질문을 임베딩 벡터로 변환합니다.\n",
      "        2. 벡터 데이터베이스에서 유사한 문서를 검색합니다.\n",
      "        3. 검색된 문서를 컨텍스트로 사용하여 LLM이 답변을 생성합니다.\n",
      "        \n",
      "        RAG의 장점:\n",
      "        - 최신 정보를 반영할 수 있습니다. LLM의 학습 데이터 이후 정보도 활용 가능합니다.\n",
      "- 환각(Hallucination)을 감소시킵니다. 실제 문서 기반으로 답변하기 때문입니다.\n",
      "        - 출처를 명시할 수 있습니다. 어떤 문서에서 정보를 가져왔는지 추적 가능합니다.\n",
      "        - 도메인 특화가 가능합니다. 특정 분야의 문서만 사용하여 전문적인 답변을 제공합니다.\n",
      "        \n",
      "        RAG의 핵심 구성요소: Retriever(검색기), Generator(생성기), VectorStore(벡터저장소)\n",
      "VectorDB(벡터 데이터베이스)는 고차원 벡터를 효율적으로 저장하고 검색하는 데이터베이스입니다.\n",
      "        \n",
      "        주요 VectorDB 솔루션:\n",
      "        - ChromaDB: 로컬 개발에 적합한 오픈소스 솔루션. 파이썬 네이티브로 설치가 간편합니다.\n",
      "        - Pinecone: 완전 관리형 클라우드 서비스. 대규모 프로덕션 환경에 적합합니다.\n",
      "        - Weaviate: 그래프 기반 벡터 데이터베이스. 하이브리드 검색을 지원합니다.\n",
      "- FAISS: Facebook에서 개발한 고성능 라이브러리. 대용량 벡터 검색에 최적화되어 있습니다.\n",
      "        - Milvus: 분산 환경을 지원하는 오픈소스 솔루션입니다.\n",
      "        \n",
      "        임베딩(Embedding)은 텍스트를 숫자 벡터로 변환하는 과정으로,\n",
      "        의미적으로 유사한 텍스트는 벡터 공간에서 가까운 위치에 배치됩니다.\n",
      "        예를 들어, \"고양이\"와 \"강아지\"는 \"자동차\"보다 벡터 공간에서 더 가깝습니다.\n"
     ]
    }
   ],
   "source": [
    "# RecursiveCharacterTextSplitter\n",
    "print(f'RecursiveCharacterTextSplitter 적용')\n",
    "recursive_splitter =  RecursiveCharacterTextSplitter(\n",
    "     chunk_size=300,\n",
    "     chunk_overlap=50,\n",
    "     separators = ['\\n\\n','\\n','.',' ',''],\n",
    "     length_function=len\n",
    ")\n",
    "# 모든 문서를 청크로 분할\n",
    "doc_splits = recursive_splitter.split_documents(sample_documents)\n",
    "print(f'원본 문서 : {len(sample_documents)}개')\n",
    "print(f'RecursiveCharacterTextSplitter 결과 : {len(doc_splits)}개 청크')\n",
    "\n",
    "for d in doc_splits:\n",
    "     print(d.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7067f499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장완료\n",
      "파일명 : chunk_output_pkl\n",
      "청크수 : [Document(metadata={'source': 'langchain_intro.txt', 'topic': 'framework', 'importance': 'high'}, page_content='LangChain은 대규모 언어 모델(LLM)을 활용한 애플리케이션 개발을 위한 프레임워크입니다.'), Document(metadata={'source': 'langchain_intro.txt', 'topic': 'framework', 'importance': 'high'}, page_content='LangChain의 주요 구성 요소:\\n        1. Models: 다양한 LLM 제공자(OpenAI, Anthropic, Google 등)와 통합\\n        2. Prompts: 프롬프트 템플릿 관리 및 최적화\\n        3. Chains: 여러 구성 요소를 연결하는 파이프라인\\n        4. Memory: 대화 맥락을 유지하기 위한 메모리 시스템\\n        5. Indexes: 문서 검색을 위한 인덱싱 도구\\n        6. Agents: 도구를 사용하여 복잡한 작업을 수행하는 에이전트'), Document(metadata={'source': 'langchain_intro.txt', 'topic': 'framework', 'importance': 'high'}, page_content='LangChain Expression Language (LCEL)은 체인을 구성하는 선언적 방식으로,\\n        파이프(|) 연산자를 사용하여 컴포넌트들을 직관적으로 연결할 수 있습니다.'), Document(metadata={'source': 'rag_concept.txt', 'topic': 'technique', 'importance': 'high'}, page_content='RAG (Retrieval-Augmented Generation)는 검색 증강 생성 기술입니다.\\n\\n        RAG의 작동 원리:\\n        1. 사용자 질문을 임베딩 벡터로 변환합니다.\\n        2. 벡터 데이터베이스에서 유사한 문서를 검색합니다.\\n        3. 검색된 문서를 컨텍스트로 사용하여 LLM이 답변을 생성합니다.'), Document(metadata={'source': 'rag_concept.txt', 'topic': 'technique', 'importance': 'high'}, page_content='RAG의 장점:\\n        - 최신 정보를 반영할 수 있습니다. LLM의 학습 데이터 이후 정보도 활용 가능합니다.\\n        - 환각(Hallucination)을 감소시킵니다. 실제 문서 기반으로 답변하기 때문입니다.\\n        - 출처를 명시할 수 있습니다. 어떤 문서에서 정보를 가져왔는지 추적 가능합니다.\\n        - 도메인 특화가 가능합니다. 특정 분야의 문서만 사용하여 전문적인 답변을 제공합니다.'), Document(metadata={'source': 'rag_concept.txt', 'topic': 'technique', 'importance': 'high'}, page_content='RAG의 핵심 구성요소: Retriever(검색기), Generator(생성기), VectorStore(벡터저장소)'), Document(metadata={'source': 'vectordb_intro.txt', 'topic': 'database', 'importance': 'medium'}, page_content='VectorDB(벡터 데이터베이스)는 고차원 벡터를 효율적으로 저장하고 검색하는 데이터베이스입니다.'), Document(metadata={'source': 'vectordb_intro.txt', 'topic': 'database', 'importance': 'medium'}, page_content='주요 VectorDB 솔루션:\\n        - ChromaDB: 로컬 개발에 적합한 오픈소스 솔루션. 파이썬 네이티브로 설치가 간편합니다.\\n        - Pinecone: 완전 관리형 클라우드 서비스. 대규모 프로덕션 환경에 적합합니다.\\n        - Weaviate: 그래프 기반 벡터 데이터베이스. 하이브리드 검색을 지원합니다.\\n        - FAISS: Facebook에서 개발한 고성능 라이브러리. 대용량 벡터 검색에 최적화되어 있습니다.'), Document(metadata={'source': 'vectordb_intro.txt', 'topic': 'database', 'importance': 'medium'}, page_content='- Milvus: 분산 환경을 지원하는 오픈소스 솔루션입니다.'), Document(metadata={'source': 'vectordb_intro.txt', 'topic': 'database', 'importance': 'medium'}, page_content='임베딩(Embedding)은 텍스트를 숫자 벡터로 변환하는 과정으로,\\n        의미적으로 유사한 텍스트는 벡터 공간에서 가까운 위치에 배치됩니다.\\n        예를 들어, \"고양이\"와 \"강아지\"는 \"자동차\"보다 벡터 공간에서 더 가깝습니다.')]\n"
     ]
    }
   ],
   "source": [
    "# 청킹 결과 저장(pickle 사용)\n",
    "import pickle\n",
    "# 최종분할설정(중간크기)\n",
    "final_splitter = RecursiveCharacterTextSplitter(    # RecursiveCharacterTextSplitter\n",
    "    chunk_size = 300,\n",
    "    chunk_overlap = 50,\n",
    "    separators = ['\\n\\n','\\n','.',' ',''],\n",
    ")\n",
    "\n",
    "final_chunks = final_splitter.split_documents(sample_documents)\n",
    "# 파일로 저장\n",
    "output_path = 'chunk_output_pkl'\n",
    "with open(output_path, 'wb') as f:\n",
    "    pickle.dump(final_chunks,f)\n",
    "print(f'저장완료')\n",
    "print(f'파일명 : {output_path}')\n",
    "print(f'청크수 : {final_chunks}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefb43e8",
   "metadata": {},
   "source": [
    "pickle : Python 객체를 파일이나 바이트 형태로 저장/불러오는 라이브러리"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
