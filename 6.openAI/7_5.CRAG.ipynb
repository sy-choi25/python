{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b29e58e",
   "metadata": {},
   "source": [
    "<span style=\"color: Gold\"> CRAG\n",
    "\n",
    "<span style=\"font-size:12px;\">\n",
    "\n",
    "- ê²€ìƒ‰ëœ ë¬¸ì„œì˜ í’ˆì§ˆì„ ìì²´ì ìœ¼ë¡œ í‰ê°€í•˜ê³ , í•„ìš”ì‹œ **ìê¸° êµì •(Self-Correction)**ì„ ìˆ˜í–‰í•˜ëŠ” ê³ ê¸‰ RAG íŒ¨í„´\n",
    "\n",
    "- ì „ì²´ ê³¼ì •\n",
    "```text\n",
    "START\n",
    "  â”‚\n",
    "retrieve (ë‚´ë¶€ ë¬¸ì„œ ê²€ìƒ‰)\n",
    "  â”‚\n",
    "grade_documents (ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€ + web_search_needed ê²°ì •)\n",
    "  â”œâ”€â”€> generate (web_search_needed = \"No\" â†’ ë‹µë³€ ìƒì„±)\n",
    "  â””â”€â”€> web_search (web_search_needed = \"Yes\" â†’ ì›¹ ê²€ìƒ‰ ìˆ˜í–‰)\n",
    "          â”‚\n",
    "       generate (í•„í„°ë§ ë¬¸ì„œ + ì›¹ ê²€ìƒ‰ ê²°ê³¼ â†’ ë‹µë³€ ìƒì„±)\n",
    "          â”‚\n",
    "         END\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32241fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from typing import List, Literal\n",
    "from typing_extensions import TypedDict\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain ê´€ë ¨ ì„í¬íŠ¸\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "\n",
    "# LangGraph ê´€ë ¨ ì„í¬íŠ¸\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# í™˜ê²½ì„¤ì •\n",
    "load_dotenv()\n",
    "\n",
    "if not os.environ.get('OPENAI_API_KEY'):\n",
    "    raise ValueError('key check....')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "575a2ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGRAState(TypedDict):\n",
    "    question : str\n",
    "    documents : List[Document]\n",
    "    filtered_documents: List[Document] \n",
    "    web_search_needed : str     # ì›¹ê²€ìƒ‰ ì—¬ë¶€ (yes/ no)\n",
    "    context : str\n",
    "    answer : str\n",
    "    grade_results : List[str]   # ê° ë¬¸ì„œì˜ í‰ê°€ ê²°ê³¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59bf812b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs path : c:\\python_src\\6.openAI\n"
     ]
    }
   ],
   "source": [
    "# ë¬¸ì„œ ë¡œë“œ\n",
    "script_dir = os.getcwd()\n",
    "docs_path = os.path.join(script_dir)\n",
    "print(f'docs path : {docs_path}')\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    docs_path,\n",
    "    glob = '**/*.txt',\n",
    "    loader_cls = TextLoader,\n",
    "    loader_kwargs={'encoding':'utf-8'},  # í•œêµ­ì–´ë©´ ê¼­ ì¨ì¤„ ê²ƒ\n",
    ")\n",
    "docs= loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7bc8b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VectorDB êµ¬ì¶• ì™„ë£Œ ì²­í¬ê°œìˆ˜ : 11\n",
      " 11ê°œ ì²­í¬ë¡œ VectorDB êµ¬ì¶• ì™„ë£Œ\n",
      "ì¡°ê±´ë¶€ ì—£ì§€ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\n",
      "\n",
      " CRAG StateGraph êµ¬ì„± ë° ì»´íŒŒì¼ ì¤‘...\n",
      "  3ê°œ ê´€ë ¨ ë¬¸ì„œ í™•ë³´!\n",
      "\n",
      "   [DECISION] ë‹¤ìŒ ë‹¨ê³„ ê²°ì • ì¤‘...\n",
      "   ê²°ì •: ë‹µë³€ ìƒì„±ìœ¼ë¡œ ì´ë™\n",
      "\n",
      "   ğŸ’¬ [GENERATE ë…¸ë“œ] ë‹µë³€ ìƒì„± ì¤‘...\n",
      "   ë‹µë³€ ìƒì„± ì™„ë£Œ!\n",
      "\n",
      "======================================\n",
      "ì§ˆë¬¸: LangGraphì˜ í•µì‹¬ ê°œë…ì„ ì„¤ëª…í•´ ì£¼ì„¸ìš”\n",
      "--------------------------------------\n",
      "\n",
      "ğŸ“Œ ë‹µë³€:\n",
      "LangGraphì˜ í•µì‹¬ ê°œë…ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
      "\n",
      "1. **State(ìƒíƒœ)**: ì—ì´ì „íŠ¸ì˜ í˜„ì¬ ìƒíƒœë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë°ì´í„° êµ¬ì¡°ë¡œ, ì—ì´ì „íŠ¸ê°€ ìˆ˜í–‰í•˜ëŠ” ì‘ì—…ì˜ ë§¥ë½ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
      "\n",
      "2. **Node(ë…¸ë“œ)**: ì‹¤ì œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜ë¡œ, ì—ì´ì „íŠ¸ê°€ íŠ¹ì • ì‘ì—…ì„ ì‹¤í–‰í•  ë•Œ ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
      "\n",
      "3. **Edge(ì—£ì§€)**: ë…¸ë“œ ê°„ì˜ ì œì–´ íë¦„ì„ ì •ì˜í•˜ì—¬, ì–´ë–¤ ë…¸ë“œê°€ ë‹¤ìŒì— ì‹¤í–‰ë ì§€ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.\n",
      "\n",
      "4. **Conditional Edge**: íŠ¹ì • ì¡°ê±´ì— ë”°ë¼ ë‹¤ë¥¸ ë…¸ë“œë¡œ ë¶„ê¸°í•  ìˆ˜ ìˆëŠ” ì—£ì§€ë¡œ, ì—ì´ì „íŠ¸ì˜ ì˜ì‚¬ê²°ì • ê³¼ì •ì„ ìœ ì—°í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤.\n",
      "\n",
      "LangGraphëŠ” ì´ëŸ¬í•œ ê°œë…ë“¤ì„ í†µí•´ ìˆœí™˜(Cycle)ì„ ì§€ì›í•˜ë©°, ë³µì¡í•œ ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš°ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ğŸ“„ ì°¸ì¡° ë¬¸ì„œ ìˆ˜: 3ê°œ\n",
      "ğŸŒ ì›¹ ê²€ìƒ‰ ì—¬ë¶€: No\n",
      "======================================\n",
      "\n",
      "  3ê°œ ê´€ë ¨ ë¬¸ì„œ í™•ë³´!\n",
      "\n",
      "   [DECISION] ë‹¤ìŒ ë‹¨ê³„ ê²°ì • ì¤‘...\n",
      "   ê²°ì •: ë‹µë³€ ìƒì„±ìœ¼ë¡œ ì´ë™\n",
      "\n",
      "   ğŸ’¬ [GENERATE ë…¸ë“œ] ë‹µë³€ ìƒì„± ì¤‘...\n",
      "   ë‹µë³€ ìƒì„± ì™„ë£Œ!\n",
      "\n",
      "======================================\n",
      "ì§ˆë¬¸: RAGë€ ë¬´ì—‡ì¸ê°€ìš”?\n",
      "--------------------------------------\n",
      "\n",
      "ğŸ“Œ ë‹µë³€:\n",
      "RAG(ê²€ìƒ‰ ì¦ê°• ìƒì„±, Retrieval-Augmented Generation)ëŠ” ì •ë³´ ê²€ìƒ‰ê³¼ ìì—°ì–´ ìƒì„± ê¸°ìˆ ì„ ê²°í•©í•œ ë°©ë²•ì…ë‹ˆë‹¤. ì´ ê¸°ìˆ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ë³´ë‹¤ ì •í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë‹µë³€ì„ ì œê³µí•˜ê¸° ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤:\n",
      "\n",
      "1. **ì„ë² ë”© ë²¡í„° ë³€í™˜**: ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
      "2. **ë¬¸ì„œ ê²€ìƒ‰**: ë³€í™˜ëœ ë²¡í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.\n",
      "3. **ë‹µë³€ ìƒì„±**: ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ í™œìš©í•˜ì—¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
      "\n",
      "RAGì˜ ì£¼ìš” ì¥ì ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
      "- **ìµœì‹  ì •ë³´ ë°˜ì˜**: ìµœì‹  ì •ë³´ë¥¼ í¬í•¨í•  ìˆ˜ ìˆì–´, ë³´ë‹¤ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
      "- **í™˜ê° ê°ì†Œ**: ìƒì„±ëœ ë‹µë³€ì˜ ì‹ ë¢°ì„±ì„ ë†’ì—¬ í™˜ê° í˜„ìƒì„ ì¤„ì…ë‹ˆë‹¤.\n",
      "- **ì¶œì²˜ ëª…ì‹œ**: ì œê³µëœ ì •ë³´ì˜ ì¶œì²˜ë¥¼ ëª…í™•íˆ í•  ìˆ˜ ìˆì–´, ì‚¬ìš©ìê°€ ì •ë³´ë¥¼ ê²€ì¦í•  ìˆ˜ ìˆë„ë¡ ë•ìŠµë‹ˆë‹¤. \n",
      "\n",
      "ì´ëŸ¬í•œ íŠ¹ì„± ë•ë¶„ì— RAGëŠ” ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ìœ ìš©í•˜ê²Œ í™œìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ğŸ“„ ì°¸ì¡° ë¬¸ì„œ ìˆ˜: 3ê°œ\n",
      "ğŸŒ ì›¹ ê²€ìƒ‰ ì—¬ë¶€: No\n",
      "======================================\n",
      "\n",
      "   ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ â†’ ì›¹ ê²€ìƒ‰ í•„ìš”!\n",
      "\n",
      "   [DECISION] ë‹¤ìŒ ë‹¨ê³„ ê²°ì • ì¤‘...\n",
      "   ê²°ì •: ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ì´ë™\n",
      "\n",
      "   [WEB SEARCH ë…¸ë“œ] ì™¸ë¶€ ì›¹ ê²€ìƒ‰ ìˆ˜í–‰ ì¤‘...\n",
      "   ì›¹ ê²€ìƒ‰ ì™„ë£Œ! ê²°ê³¼ê°€ ë¬¸ì„œì— ì¶”ê°€ë¨\n",
      "\n",
      "   ğŸ’¬ [GENERATE ë…¸ë“œ] ë‹µë³€ ìƒì„± ì¤‘...\n",
      "   ë‹µë³€ ìƒì„± ì™„ë£Œ!\n",
      "\n",
      "======================================\n",
      "ì§ˆë¬¸: ì˜¤ëŠ˜ ì„œìš¸ ë‚ ì”¨ëŠ” ì–´ë–¤ê°€ìš”?\n",
      "--------------------------------------\n",
      "\n",
      "ğŸ“Œ ë‹µë³€:\n",
      "ì£„ì†¡í•˜ì§€ë§Œ, ì œê³µëœ ë¬¸ë§¥ì—ëŠ” ì˜¤ëŠ˜ ì„œìš¸ ë‚ ì”¨ì— ëŒ€í•œ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ì„œìš¸ ë‚ ì”¨ì— ëŒ€í•œ ì •í™•í•œ ì •ë³´ë¥¼ ì›í•˜ì‹ ë‹¤ë©´, ê¸°ìƒì²­ ì›¹ì‚¬ì´íŠ¸ë‚˜ ë‚ ì”¨ ê´€ë ¨ ì•±ì„ í™•ì¸í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.\n",
      "\n",
      "ğŸ“„ ì°¸ì¡° ë¬¸ì„œ ìˆ˜: 1ê°œ\n",
      "ğŸŒ ì›¹ ê²€ìƒ‰ ì—¬ë¶€: Yes\n",
      "======================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# í…ìŠ¤íŠ¸ ë¶„í• \n",
    "text_spliter =  RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50,separators= ['\\n\\n','\\n','.',' ',''])\n",
    "doc_splits = text_spliter.split_documents(docs)\n",
    "\n",
    "# ì„ë² ë”© ë° Vector DB\n",
    "\n",
    "vectorstores = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=OpenAIEmbeddings(model='text-embedding-3-small'),\n",
    "    collection_name='crag_collection'\n",
    ")\n",
    "print(f'VectorDB êµ¬ì¶• ì™„ë£Œ ì²­í¬ê°œìˆ˜ : {len(doc_splits)}')\n",
    "\n",
    "# ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì •\n",
    "retriever = vectorstores.as_retriever(search_kwargs={'k':3})\n",
    "print(f' {len(doc_splits)}ê°œ ì²­í¬ë¡œ VectorDB êµ¬ì¶• ì™„ë£Œ')\n",
    "\n",
    "# ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€ë¥¼ ìœ„í•œ Grader ì •ì˜\n",
    "from pydantic import BaseModel, Field\n",
    "class GradeDocuments(BaseModel):\n",
    "    '''ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€ ê²°ê³¼ë¥¼ ìœ„í•˜ pydantic ëª¨ë¸'''\n",
    "    binary_score :str = Field(description = \"ë¬¸ì„œê°€ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆìœ¼ë©´ 'yes', ì—†ìœ¼ë©´ 'no'\")\n",
    "\n",
    "# llm\n",
    "grader_llm = ChatOpenAI(model = 'gpt-4o-mini', temperature=0)\n",
    "structured_grader = grader_llm.with_structured_output(GradeDocuments)   # with_structured_output: LLM ì¶œë ¥ê°’ì„ êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ë°›ë„ë¡ ì„¤ì •í•˜ëŠ” í•¨ìˆ˜\n",
    "grade_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system','''ë‹¹ì‹ ì€ ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ”ë° ê´€ë ¨ì´ ìˆëŠ”ì§€ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ ì…ë‹ˆë‹¤.\n",
    "     \n",
    "     í‰ê°€ê¸°ì¤€:\n",
    "     - ë¬¸ì„œê°€ ì§ˆë¬¸ì˜ í‚¤ì›Œë“œë‚˜ ì˜ë¯¸ì™€ ì—°ê´€ë˜ì–´ ìˆë‹¤ë©´ 'ê´€ë ¨ìˆìŒ'ìœ¼ë¡œ í‰ê°€\n",
    "     - ë‹µë³€ì— ë„ì›€ì´ ë  ê°€ëŠ¥ì„±ì´ ì¡°ê¸ˆì´ë¼ë„ ìˆë‹¤ë©´ 'ê´€ë ¨ìˆìŒ'\n",
    "     - ì™€ì „íˆ ë¬´ê´€í•œ ë‚´ìš©ì´ë©´ 'ê´€ë ¨ì—†ìŒ'\n",
    "\n",
    "     ì—„ê²©í•˜ê²Œ í‰ê°€í•˜ì§€ ë§ê³ , ì•½ê°„ì˜ ì—°ê´€ì„±ì´ë¼ë„ ìˆìœ¼ë©´ 'yes'ë¥¼ ë°˜í™˜í•˜ì„¸ìš”     \n",
    "'''),\n",
    "('human','''ì§ˆë¬¸:{question}\n",
    " \n",
    " ë¬¸ì„œë‚´ìš©:\n",
    " {document}\n",
    "\n",
    " ì´ ë¬¸ì„œê°€ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆìŠµë‹ˆê¹Œ? 'yes' ë˜ëŠ” 'no'ë¡œë§Œ ë‹µí•˜ì„¸ìš”\n",
    " ''')\n",
    "])\n",
    "\n",
    "document_grader = grade_prompt | structured_grader\n",
    "\n",
    "def retrieve_node(state:CGRAState) -> dict:\n",
    "    '''ë‚´ë¶€ ë¬¸ì„œ ê²€ìƒ‰ ë…¸ë“œ'''\n",
    "    question = state['question']\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\n",
    "        'documents' : documents,\n",
    "        'question' : question\n",
    "    }\n",
    "def grade_documents_node(state:CGRAState) -> dict :\n",
    "    '''ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€ ë…¸ë“œ\n",
    "    ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ê´€ë ¨ì„± ì—¬ë¶€ë¥¼ llm í‰ê°€\n",
    "    ê´€ë ¨ ì—†ìœ¼ë©´ ì›¹ ê²€ìƒ‰ í”Œë ˆê·¸ë¥¼ í™œì„±í™”\n",
    "    '''\n",
    "    question = state['question']\n",
    "    documents = state['documents']\n",
    "    filtered_docs, grade_results = [],[]\n",
    "    for i,doc in enumerate(documents,1):\n",
    "        # ê° ë¬¸ì„œì˜ ê´€ë ¨ì„± í‰ê°€\n",
    "        score = document_grader.invoke ({\n",
    "        'question' : question,\n",
    "        'document' : doc.page_content\n",
    "        })\n",
    "        grade = score.binary_score.lower()\n",
    "        if grade == 'yes':\n",
    "            filtered_docs.append(doc)\n",
    "            grade_results.append(\"relevant\")\n",
    "        else:\n",
    "            grade_results.append(\"not_relevant\")\n",
    "     # ê´€ë ¨ ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ì›¹ ê²€ìƒ‰ í•„ìš”\n",
    "    if len(filtered_docs) == 0:\n",
    "        web_search_needed = \"Yes\"\n",
    "        print(\"   ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ â†’ ì›¹ ê²€ìƒ‰ í•„ìš”!\")\n",
    "    else:\n",
    "        web_search_needed = \"No\"\n",
    "        print(f\"  {len(filtered_docs)}ê°œ ê´€ë ¨ ë¬¸ì„œ í™•ë³´!\")\n",
    "    \n",
    "    return {\n",
    "        \"filtered_documents\": filtered_docs,\n",
    "        \"web_search_needed\": web_search_needed,\n",
    "        \"grade_results\": grade_results\n",
    "    }\n",
    "\n",
    "def web_search_node(state: CGRAState) -> dict:\n",
    "    \"\"\"\n",
    "    ì›¹ ê²€ìƒ‰ ë…¸ë“œ (ì‹œë®¬ë ˆì´ì…˜)\n",
    "    \n",
    "    ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” Tavily APIë‚˜ ë‹¤ë¥¸ ê²€ìƒ‰ APIë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "    ì—¬ê¸°ì„œëŠ” í•™ìŠµ ëª©ì ìœ¼ë¡œ ì‹œë®¬ë ˆì´ì…˜í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    print(\"\\n   [WEB SEARCH ë…¸ë“œ] ì™¸ë¶€ ì›¹ ê²€ìƒ‰ ìˆ˜í–‰ ì¤‘...\")\n",
    "    \n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    # ì›¹ ê²€ìƒ‰ ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œë¡œëŠ” Tavily API ë“± ì‚¬ìš©)\n",
    "    # ì‹¤ì œ êµ¬í˜„ ì˜ˆì‹œ:\n",
    "    # from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "    # web_search = TavilySearchResults(k=3)\n",
    "    # web_results = web_search.invoke({\"query\": question})\n",
    "    \n",
    "    # ì‹œë®¬ë ˆì´ì…˜ëœ ì›¹ ê²€ìƒ‰ ê²°ê³¼\n",
    "    simulated_web_results = f\"\"\"\n",
    "    [ì›¹ ê²€ìƒ‰ ê²°ê³¼ - ì‹œë®¬ë ˆì´ì…˜]\n",
    "    \n",
    "    ì§ˆë¬¸ '{question}'ì— ëŒ€í•œ ì›¹ ê²€ìƒ‰ ê²°ê³¼:\n",
    "    \n",
    "    1. LLM(Large Language Model) ê´€ë ¨ ìµœì‹  ì •ë³´:\n",
    "       - LLMì€ ìì—°ì–´ ì²˜ë¦¬ì—ì„œ í˜ì‹ ì ì¸ ë°œì „ì„ ì´ë£¨ê³  ìˆìŠµë‹ˆë‹¤.\n",
    "       - OpenAI, Anthropic, Google ë“±ì´ ì£¼ìš” ì œê³µìì…ë‹ˆë‹¤.\n",
    "       - RAG, Fine-tuning, Prompt Engineeringì´ ì£¼ìš” í™œìš© ê¸°ë²•ì…ë‹ˆë‹¤.\n",
    "    \n",
    "    2. AI ì—ì´ì „íŠ¸ íŠ¸ë Œë“œ:\n",
    "       - ììœ¨ì ì¸ AI ì—ì´ì „íŠ¸ê°€ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤.\n",
    "       - LangGraph, AutoGPT ë“±ì´ ëŒ€í‘œì ì¸ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\n",
    "       - ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì´ ë³µì¡í•œ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    ì¶œì²˜: ì‹œë®¬ë ˆì´ì…˜ëœ ì›¹ ê²€ìƒ‰ (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” Tavily API ì‚¬ìš©)\n",
    "    \"\"\"\n",
    "    \n",
    "    # ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ Document í˜•íƒœë¡œ ë³€í™˜\n",
    "    web_doc = Document(\n",
    "        page_content=simulated_web_results,\n",
    "        metadata={\"source\": \"web_search\", \"type\": \"external\"}\n",
    "    )\n",
    "    \n",
    "    # ê¸°ì¡´ í•„í„°ë§ëœ ë¬¸ì„œì— ì›¹ ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€\n",
    "    filtered_docs = state.get(\"filtered_documents\", [])\n",
    "    filtered_docs.append(web_doc)\n",
    "    \n",
    "    print(\"   ì›¹ ê²€ìƒ‰ ì™„ë£Œ! ê²°ê³¼ê°€ ë¬¸ì„œì— ì¶”ê°€ë¨\")\n",
    "    \n",
    "    return {\n",
    "        \"filtered_documents\": filtered_docs\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_node(state: CGRAState) -> dict:\n",
    "    \"\"\"\n",
    "    ë‹µë³€ ìƒì„± ë…¸ë“œ\n",
    "    í•„í„°ë§ëœ ë¬¸ì„œ(ë‚´ë¶€ ë¬¸ì„œ + ì›¹ ê²€ìƒ‰ ê²°ê³¼)ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    print(\"\\n   ğŸ’¬ [GENERATE ë…¸ë“œ] ë‹µë³€ ìƒì„± ì¤‘...\")\n",
    "    \n",
    "    question = state[\"question\"]\n",
    "    filtered_documents = state[\"filtered_documents\"]\n",
    "    \n",
    "    # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±\n",
    "    context = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in filtered_documents])\n",
    "    \n",
    "    # ë‹µë³€ ìƒì„± LLM\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"ë‹¹ì‹ ì€ ì œê³µëœ ë¬¸ë§¥ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n",
    "\n",
    "ê·œì¹™:\n",
    "1. ì œê³µëœ ë¬¸ë§¥ ë‚´ì˜ ì •ë³´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì„¸ìš”.\n",
    "2. ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ëª…í™•í•˜ê³  êµ¬ì¡°í™”ë˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.\n",
    "3. ì›¹ ê²€ìƒ‰ ê²°ê³¼ê°€ í¬í•¨ëœ ê²½ìš°, í•´ë‹¹ ì •ë³´ë„ ì ì ˆíˆ í™œìš©í•˜ì„¸ìš”.\n",
    "4. í™•ì‹¤í•˜ì§€ ì•Šì€ ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.\"\"\"),\n",
    "        (\"human\", \"\"\"ë¬¸ë§¥(Context):\n",
    "{context}\n",
    "\n",
    "ì§ˆë¬¸: {question}\n",
    "\n",
    "ë‹µë³€:\"\"\")\n",
    "    ])\n",
    "    \n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    answer = chain.invoke({\"context\": context, \"question\": question})\n",
    "    \n",
    "    print(\"   ë‹µë³€ ìƒì„± ì™„ë£Œ!\")\n",
    "    \n",
    "    return {\n",
    "        \"context\": context,\n",
    "        \"answer\": answer\n",
    "    }\n",
    "\n",
    "\n",
    "# ì¡°ê±´ë¶€ ì—£ì§€ í•¨ìˆ˜ ì •ì˜\n",
    "\n",
    "def decide_to_generate(state: CGRAState) -> Literal[\"generate\", \"web_search\"]:\n",
    "    \"\"\"\n",
    "    ë¬¸ì„œ í‰ê°€ ê²°ê³¼ì— ë”°ë¼ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    - ê´€ë ¨ ë¬¸ì„œê°€ ìˆìœ¼ë©´ â†’ generate (ë‹µë³€ ìƒì„±)\n",
    "    - ê´€ë ¨ ë¬¸ì„œê°€ ì—†ìœ¼ë©´ â†’ web_search (ì›¹ ê²€ìƒ‰)\n",
    "    \n",
    "    Returns:\n",
    "        \"generate\" ë˜ëŠ” \"web_search\"\n",
    "    \"\"\"\n",
    "    print(\"\\n   [DECISION] ë‹¤ìŒ ë‹¨ê³„ ê²°ì • ì¤‘...\")\n",
    "    \n",
    "    web_search_needed = state[\"web_search_needed\"]\n",
    "    \n",
    "    if web_search_needed == \"Yes\":\n",
    "        print(\"   ê²°ì •: ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ì´ë™\")\n",
    "        return \"web_search\"\n",
    "    else:\n",
    "        print(\"   ê²°ì •: ë‹µë³€ ìƒì„±ìœ¼ë¡œ ì´ë™\")\n",
    "        return \"generate\"\n",
    "\n",
    "\n",
    "print(\"ì¡°ê±´ë¶€ ì—£ì§€ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\")\n",
    "\n",
    "print(\"\\n CRAG StateGraph êµ¬ì„± ë° ì»´íŒŒì¼ ì¤‘...\")\n",
    "\n",
    "# StateGraph ìƒì„±\n",
    "workflow = StateGraph(CGRAState)\n",
    "\n",
    "# ë…¸ë“œ ì¶”ê°€\n",
    "workflow.add_node(\"retrieve\", retrieve_node)\n",
    "workflow.add_node(\"grade_documents\", grade_documents_node)\n",
    "workflow.add_node(\"web_search\", web_search_node)\n",
    "workflow.add_node(\"generate\", generate_node)\n",
    "\n",
    "# ì—£ì§€ ì¶”ê°€\n",
    "# START -> retrieve -> grade_documents\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "\n",
    "# ì¡°ê±´ë¶€ ì—£ì§€: grade_documents ì´í›„ ë¶„ê¸°\n",
    "# - ê´€ë ¨ ë¬¸ì„œ ìˆìŒ â†’ generate\n",
    "# - ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ â†’ web_search\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",      # ì‹œì‘ ë…¸ë“œ\n",
    "    decide_to_generate,     # ì¡°ê±´ í•¨ìˆ˜\n",
    "    {\n",
    "        \"generate\": \"generate\",      # \"generate\" ë°˜í™˜ ì‹œ\n",
    "        \"web_search\": \"web_search\"   # \"web_search\" ë°˜í™˜ ì‹œ\n",
    "    }\n",
    ")\n",
    "\n",
    "# web_search ì´í›„ generateë¡œ ì´ë™\n",
    "workflow.add_edge(\"web_search\", \"generate\")\n",
    "\n",
    "# generate ì´í›„ ì¢…ë£Œ\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "# ê·¸ë˜í”„ ì»´íŒŒì¼\n",
    "app = workflow.compile()\n",
    "\n",
    "test_qeustion = [\n",
    "    'LangGraphì˜ í•µì‹¬ ê°œë…ì„ ì„¤ëª…í•´ ì£¼ì„¸ìš”',\n",
    "    'RAGë€ ë¬´ì—‡ì¸ê°€ìš”?',\n",
    "    'ì˜¤ëŠ˜ ì„œìš¸ ë‚ ì”¨ëŠ” ì–´ë–¤ê°€ìš”?'  # ë‚´ë¶€ ë¬¸ì„œì— ì—†ìŒ\n",
    "]\n",
    "\n",
    "# ê° ì§ˆë¬¸ì— ëŒ€í•œ ì¶œë ¥\n",
    "for question in test_qeustion:        \n",
    "    result = app.invoke({\n",
    "        'question': question,\n",
    "        'documents': [],\n",
    "        'doc_scores': [],\n",
    "        'search_type': \"\",\n",
    "        'answer': \"\",\n",
    "        \n",
    "        # ğŸ”¥ í•„ìˆ˜ ì´ˆê¸°ê°’ ì¶”ê°€\n",
    "        'filtered_documents': [],\n",
    "        'web_search_needed': \"No\"\n",
    "    })\n",
    "\n",
    "    print(\"\\n======================================\")\n",
    "    print(f\"ì§ˆë¬¸: {question}\")\n",
    "    print(\"--------------------------------------\")\n",
    "\n",
    "    # 1) ë‹µë³€\n",
    "    print(f\"\\nğŸ“Œ ë‹µë³€:\\n{result.get('answer', '(ë‹µë³€ ì—†ìŒ)')}\")\n",
    "\n",
    "    # 2) ì°¸ì¡° ë¬¸ì„œ ìˆ˜\n",
    "    filtered_docs = result.get(\"filtered_documents\", [])\n",
    "    print(f\"\\nğŸ“„ ì°¸ì¡° ë¬¸ì„œ ìˆ˜: {len(filtered_docs)}ê°œ\")\n",
    "\n",
    "    # 3) ì›¹ ê²€ìƒ‰ ì—¬ë¶€ \n",
    "    web_flag = result.get(\"web_search_needed\", \"Unknown\")\n",
    "    print(f\"ğŸŒ ì›¹ ê²€ìƒ‰ ì—¬ë¶€: {web_flag}\")\n",
    "\n",
    "    print(\"======================================\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
