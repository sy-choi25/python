# 한국어 특화 임베딩 (BGE-M3) 이론

## 목차

1. [한국어 임베딩의 필요성](#1-한국어-임베딩의-필요성)
2. [BGE-M3 모델 소개](#2-bge-m3-모델-소개)
3. [Dense vs Sparse 임베딩](#3-dense-vs-sparse-임베딩)
4. [임베딩 모델 비교](#4-임베딩-모델-비교)
5. [하이브리드 검색](#5-하이브리드-검색)
6. [적용 가이드](#6-적용-가이드)

---

## 1. 한국어 임베딩의 필요성

### 1.1 한국어의 특수성

```text
영어:                         한국어:
"I love you"                 "나는 너를 사랑한다"
  │  │   │                      │    │     │
  3 단어                        조사, 어미가 붙은 복합 형태
```

한국어 특성:

- **교착어**: 조사, 어미가 어근에 붙음
- **띄어쓰기 유연**: 붙여쓰기도 허용되는 경우 多
- **한자어**: 같은 한자가 다양한 의미로 사용
- **신조어/줄임말**: 빠르게 변화하는 어휘

### 1.2 영어 중심 모델의 한계

| 문제 | 예시 | 영향 |
|------|------|------|
| **토큰화 비효율** | "사랑합니다" → 5-6 토큰 | 임베딩 품질 저하 |
| **의미 인식 오류** | "배" (과일/선박/신체) | 검색 정확도 하락 |
| **신조어 미인식** | "갑분싸", "점메추" | 검색 실패 |

### 1.3 한국어 임베딩 벤치마크

```text
KorSTS (한국어 의미 유사도 벤치마크):

BGE-M3          ████████████████████ 85.2%
KoSimCSE        ███████████████████  83.1%
OpenAI Ada-002  █████████████████    79.5%
Multilingual-E5 ████████████████     78.8%
```

---

## 2. BGE-M3 모델 소개

### 2.1 BGE-M3란?

**BGE-M3**는 BAAI(Beijing Academy of Artificial Intelligence)에서 개발한 다국어 임베딩 모델입니다.

```text
BGE-M3의 의미:
├── M: Multi-lingual (다국어: 100+ 언어)
├── M: Multi-functionality (다기능: Dense + Sparse + ColBERT)
└── M: Multi-granularity (다중 세분화: 문장/문단/문서)
```

### 2.2 주요 특징

| 특징 | 설명 | 장점 |
|------|------|------|
| **다국어 지원** | 100+ 언어 (한국어 포함) | 별도 모델 불필요 |
| **긴 컨텍스트** | 최대 8,192 토큰 | 긴 문서 처리 가능 |
| **하이브리드** | Dense + Sparse 동시 지원 | 검색 정확도 향상 |
| **오픈소스** | HuggingFace 공개 | 비용 절감 |

### 2.3 사용법

```python
from langchain_huggingface import HuggingFaceEmbeddings

# BGE-M3 모델 로드
embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-m3",
    model_kwargs={
        'device': 'cuda',  # GPU 사용 (없으면 'cpu')
        'trust_remote_code': True
    },
    encode_kwargs={
        'normalize_embeddings': True,  # 정규화
        'batch_size': 32
    }
)

# 임베딩 생성
vector = embeddings.embed_query("한국어 텍스트입니다.")
print(f"벡터 차원: {len(vector)}")  # 1024
```

### 2.4 BGE-M3 vs OpenAI 임베딩

| 항목 | BGE-M3 | OpenAI text-embedding-3 |
|------|--------|-------------------------|
| **비용** | 무료 (로컬) | API 호출당 과금 |
| **한국어 성능** | 우수 | 보통 |
| **벡터 차원** | 1024 | 1536/3072 |
| **실행 위치** | 로컬/서버 | OpenAI 서버 |
| **인터넷** | 불필요 (다운로드 후) | 필수 |
| **속도** | GPU 시 빠름 | 네트워크 의존 |

---

## 3. Dense vs Sparse 임베딩

### 3.1 Dense 임베딩

**Dense(밀집) 임베딩**은 모든 차원에 값이 있는 벡터입니다.

```text
"한국어 임베딩"의 Dense 벡터:
[0.23, -0.45, 0.12, 0.78, -0.33, 0.56, ...]
 ↑     ↑     ↑     ↑     ↑     ↑
 모든 차원에 값 존재 (1024차원)
```

**특징:**

- 의미적 유사성 캡처
- "유사한 의미 = 가까운 벡터"
- 동의어, 유사어 검색에 강함

### 3.2 Sparse 임베딩
 
**Sparse(희소) 임베딩**은 대부분의 값이 0인 벡터입니다.  
<span style="color: Gold"> -> 단어 전체 사전을 기준으로 one-hot + 가중치 </span> 
```text
"한국어 임베딩"의 Sparse 벡터:
[0, 0, 0.8, 0, 0, 0, 0.5, 0, 0, 0.3, 0, 0, ...]
       ↑           ↑           ↑
    "한국어"     "임베딩"      "기술"
    (특정 단어 위치에만 값)
```

**특징:**

- 키워드 매칭 기반 (BM25와 유사)
- 정확한 단어 매칭에 강함
- 특정 용어 검색에 효과적

### 3.3 Dense vs Sparse 비교

| 검색 유형 | Dense | Sparse |
|-----------|-------|--------|
| "행복한 감정" → "기쁜 마음" |  우수 |  단어 불일치 |
| "Python 3.12" → "Python 3.12" | △ 보통 |  정확 매칭 |
| "LangChain" → "랭체인" |  의미 유사 |  완전 다른 단어 |
| "에러 코드 E001" | △ 의미 흐릿 | 정확 매칭 |

### 3.4 BGE-M3의 다중 출력

```python
from FlagEmbedding import BGEM3FlagModel

model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)

# 문장 인코딩
output = model.encode(
    ["한국어 임베딩 테스트"],
    return_dense=True,
    return_sparse=True,
    return_colbert_vecs=True
)

# 세 가지 출력
dense_vecs = output['dense_vecs']      # (1, 1024)
sparse_vecs = output['lexical_weights'] # {토큰: 가중치}
colbert_vecs = output['colbert_vecs']  # (1, seq_len, 1024)
```

---

## 4. 임베딩 모델 비교

### 4.1 주요 한국어 임베딩 모델

| 모델 | 차원 | 한국어 | 특징 | 라이선스 |
|------|------|--------|------|----------|
| **BGE-M3** | 1024 | ◎ | 다국어, 하이브리드 | MIT |
| **KoSimCSE** | 768 | ◎ | 한국어 전용 | Apache 2.0 |
| **multilingual-e5** | 1024 | ○ | MS 개발 | MIT |
| **paraphrase-multilingual** | 768 | ○ | 범용 | Apache 2.0 |
| **OpenAI ada-002** | 1536 | △ | API 기반 | 상용 |

### 4.2 성능 비교 (한국어 벤치마크)

```text
KorSTS (의미 유사도):
BGE-M3          ████████████████████ 85.2
KoSimCSE        ███████████████████  83.1
multilingual-e5 ████████████████     78.8
OpenAI ada-002  █████████████████    79.5

KorNLI (자연어 추론):
BGE-M3          ████████████████████ 82.5
KoSimCSE        ██████████████████   80.3
multilingual-e5 █████████████████    78.1
```

### 4.3 모델 선택 가이드

```text
한국어 문서 RAG
├── 높은 품질 필요 → BGE-M3 (권장)
├── 경량화 필요 → KoSimCSE
└── API 선호 → OpenAI text-embedding-3-small

영어 + 한국어 혼합
├── BGE-M3 (다국어)
└── multilingual-e5

비용 최소화
├── 로컬: BGE-M3, KoSimCSE
└── 클라우드: text-embedding-3-small
```

---

## 5. 하이브리드 검색

### 5.1 하이브리드 검색이란?

**Dense + Sparse 검색**을 결합하여 각각의 장점을 활용합니다.

```text
질문: "Python 에러 해결 방법"

Dense 검색:
├── "파이썬 오류 수정하기"  (의미 유사)
├── "코드 버그 해결"  (의미 유사)
└── "Python error fix"  (언어 달라도 유사)

Sparse 검색:
├── "Python 에러 해결 가이드"  (키워드 매칭)
├── "Python error handling"  (정확한 키워드)
└── "파이썬 오류"  (키워드 불일치)

하이브리드 (결합):
├── "Python 에러 해결 가이드" 
├── "파이썬 오류 수정하기" 
└── "코드 버그 해결" 
```

### 5.2 하이브리드 검색 구현

```python
from langchain.retrievers import EnsembleRetriever
from langchain_community.retrievers import BM25Retriever

# Dense 리트리버 (의미 기반)
dense_retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

# Sparse 리트리버 (키워드 기반)
bm25_retriever = BM25Retriever.from_documents(documents)
bm25_retriever.k = 5

# 앙상블 (하이브리드)
ensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, dense_retriever],
    weights=[0.3, 0.7]  # Sparse 30%, Dense 70%
)

# 검색
results = ensemble_retriever.invoke("Python 에러 해결")
```

### 5.3 가중치 튜닝

| 상황 | Sparse 비중 | Dense 비중 | 이유 |
|------|------------|-----------|------|
| **기술 문서** | 0.4 | 0.6 | 정확한 용어 중요 |
| **일반 Q&A** | 0.2 | 0.8 | 의미 이해 중요 |
| **법률/의료** | 0.5 | 0.5 | 정확성과 의미 모두 |
| **코드 검색** | 0.6 | 0.4 | 정확한 매칭 필수 |

### 5.4 Reciprocal Rank Fusion (RRF)

여러 검색 결과를 병합하는 알고리즘입니다.

```python
def reciprocal_rank_fusion(results_list, k=60):
    """
    여러 검색 결과를 RRF로 병합
    
    RRF 점수 = Σ 1/(k + rank)
    """
    fused_scores = {}
    
    for results in results_list:
        for rank, doc in enumerate(results):
            doc_id = doc.metadata.get("id", doc.page_content[:50])
            if doc_id not in fused_scores:
                fused_scores[doc_id] = {"doc": doc, "score": 0}
            fused_scores[doc_id]["score"] += 1 / (k + rank + 1)
    
    # 점수 순 정렬
    sorted_docs = sorted(
        fused_scores.values(), 
        key=lambda x: x["score"], 
        reverse=True
    )
    
    return [item["doc"] for item in sorted_docs]
```

---

## 6. 적용 가이드

### 6.1 BGE-M3 배포 옵션

```text
옵션 1: 직접 로드 (개발/소규모)
├── HuggingFace에서 다운로드
├── 메모리: ~2GB
└── GPU 권장

옵션 2: API 서버 (프로덕션)
├── FastAPI + 모델 서빙
├── 로드 밸런싱
└── 캐싱

옵션 3: 관리형 서비스
├── HuggingFace Inference Endpoints
├── AWS SageMaker
└── GCP Vertex AI
```

### 6.2 성능 최적화

```python
# 1. 배치 처리로 속도 향상
texts = ["문장1", "문장2", "문장3", ...]
embeddings = model.embed_documents(texts)  # 한 번에 처리

# 2. GPU 메모리 최적화
model_kwargs = {
    'device': 'cuda',
    'torch_dtype': torch.float16  # FP16으로 메모리 절약
}

# 3. 캐싱
from langchain.embeddings import CacheBackedEmbeddings
from langchain.storage import LocalFileStore

store = LocalFileStore("./embedding_cache")
cached_embeddings = CacheBackedEmbeddings.from_bytes_store(
    embeddings, store, namespace="bge-m3"
)
```

### 6.3 품질 평가 방법

```python
def evaluate_embedding_quality(model, test_pairs):
    """
    임베딩 품질 평가
    test_pairs: [(query, positive_doc, negative_doc), ...]
    """
    correct = 0
    for query, pos, neg in test_pairs:
        q_emb = model.embed_query(query)
        p_emb = model.embed_query(pos)
        n_emb = model.embed_query(neg)
        
        pos_sim = cosine_similarity(q_emb, p_emb)
        neg_sim = cosine_similarity(q_emb, n_emb)
        
        if pos_sim > neg_sim:
            correct += 1
    
    accuracy = correct / len(test_pairs)
    return accuracy
```

### 6.4 마이그레이션 체크리스트

OpenAI → BGE-M3 전환 시:

- [ ] 벡터 차원 변경 (1536 → 1024)
- [ ] VectorDB 재인덱싱
- [ ] 유사도 임계값 재조정
- [ ] 배치 크기 최적화
- [ ] GPU/CPU 리소스 확인

---

## 핵심 요약

###  체크리스트

- [ ] 한국어 비중 파악 → 모델 선택
- [ ] 로컬 vs API 결정
- [ ] 하이브리드 검색 필요 여부
- [ ] 리소스 (GPU/메모리) 확인
- [ ] 벤치마크 테스트

### 포인트

1. **한국어는 BGE-M3**: 다국어 성능 우수
2. **하이브리드 검색**: Dense + Sparse 조합
3. **로컬 = 비용 절감**: API 비용 없음
4. **GPU 권장**: CPU는 느림