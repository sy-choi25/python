{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b92c5221",
   "metadata": {},
   "source": [
    "1. 환경 설정 로드 (API키, 경고 제거)\n",
    "2. 필요한 라이브러리 import\n",
    "3. 임베딩 모델 Factory 클래스 정의\n",
    "4. 예시 한글 문서(Document)들 준비\n",
    "5. 테스트용 문장(test_texts) 준비\n",
    "6. 임베딩 모델별 성능 테스트 (OpenAI → BGE-M3 → 다른 모델)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66ec768b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai 임베딩\n",
      "벡터차원 : 3\n",
      "처리시간 : 0.49\n",
      "hf_embeddings 임베딩\n",
      "벡터차원 : 3\n",
      "처리시간 : 0.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name BM-K/KoSimCSE-roberta-multitask. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ko_orberta_vectors 임베딩\n",
      "벡터차원 : 3\n",
      "처리시간 : 0.67\n",
      "문서분할 완료: 6개 청크\n",
      "검색 테스트 결과\n",
      "질문: 한국어 자연어처리란 무엇인가요?\n",
      "걈색결과: 한국어\n",
      "걈색결과: 한국어 자연어 처리는 영어와 다른 특성을 가집니다. 한국어는 교착어로서\n",
      "        조사와 어미가 단어에 붙어 문장의 의미를 결정합니다. 따라서 \n",
      "        형태소 분석, 적절한 토큰화, 다국어 지원 임베딩 모델 사용이 중요합니다.\n",
      "        KoNLPy, Mecab 등의 한국어 특화 도구를 활용할 수 있습니다.\n",
      "질문: RAG 시스템의 장점을 설명해 주세요\n",
      "걈색결과: RAG\n",
      "걈색결과: RAG(Retrieval-Augmented Generation)는 검색 증강 생성 기술로,\n",
      "        LLM의 한계를 보완합니다. 기업의 내부 문서나 최신 정보를 벡터 \n",
      "        데이터베이스에 저장하고, 사용자 질문과 관련된 문서를 검색하여\n",
      "        답변의 정확성을 높입니다. 이를 통해 환각(Hallucination) 현상을 \n",
      "        줄일 수 있습니다.\n",
      "질문: 벡터 데이터베이스의 종류\n",
      "걈색결과: 데이터베이스\n",
      "걈색결과: 벡터 데이터베이스는 고차원 벡터를 효율적으로 저장하고 검색하는\n",
      "        데이터베이스입니다. 텍스트, 이미지, 오디오 등을 임베딩 벡터로 \n",
      "        변환하여 저장하면, 의미적으로 유사한 항목을 빠르게 찾을 수 있습니다.\n",
      "        ChromaDB, Pinecone, Weaviate, FAISS 등이 대표적입니다.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "from typing import  List, Tuple\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# langchain\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "#BS25\n",
    "from rank_bm25 import BM25Okapi\n",
    "# 환경설정\n",
    "load_dotenv()\n",
    "\n",
    "if not os.getenv('OPENAI_API_KEY'):\n",
    "    raise ValueError('check openai key in .env')\n",
    "\n",
    "# HuggingFace 임베딩 시도\n",
    "try:\n",
    "    from langchain_huggingface import HuggingFaceEmbeddings\n",
    "except:\n",
    "    print('pip install langchain-hggingface sentence-transformers')\n",
    "\n",
    "# 임베딩 모델 정의\n",
    "class KoreanEmbeddingModels:\n",
    "    '''한국어 임베딩 모델 팩토리 클래스\n",
    "    다양한 임베딩 모델을 쉽게 교체할 수 있도록 추상화\n",
    "    '''\n",
    "    @staticmethod\n",
    "    def get_bge_m3(device: str = 'cpu'):\n",
    "        '''BGE-M3 모델 반환\n",
    "         - Dense + Sparse 임베딩을 지원\n",
    "         - 다국어 지원(한국어 우수)\n",
    "        '''\n",
    "        return HuggingFaceEmbeddings(\n",
    "            model_name = 'BAAI/bge-m3',\n",
    "            model_kwargs = {\n",
    "                'device':device,\n",
    "                'trust_remote_code':True\n",
    "            },\n",
    "            encode_kwargs = {\n",
    "                'normalize_embeddings':True,  # 정규화를 해서 코사인 유사도 계산을 용이하게..\n",
    "                'batch_size':32\n",
    "            }\n",
    "        )\n",
    "    @staticmethod\n",
    "    def get_multilingual_e5(device:str='cpu'):\n",
    "        '''Multilingual-E5 모델 반환\n",
    "        - 경량화\n",
    "        - 다국어지원\n",
    "        '''\n",
    "        return HuggingFaceEmbeddings(\n",
    "            model_name = 'intfloat/multilingual-e5-large',\n",
    "            model_kwargs = {'device':device},\n",
    "            encode_kwargs= {'normalize_embeddings':True}\n",
    "        )\n",
    "    @staticmethod\n",
    "    def get_korean_roberta(device:str = 'cpu'):\n",
    "        '''BM-K/KoSimCSE-roberta-multitask'''\n",
    "        return HuggingFaceEmbeddings(\n",
    "            model_name = 'BM-K/KoSimCSE-roberta-multitask',\n",
    "            model_kwargs = {'device':device},\n",
    "            encode_kwargs= {'normalize_embeddings':True}\n",
    "        )\n",
    "    @staticmethod\n",
    "    def get_openai(model:str = 'text-embedding-3-small'):\n",
    "        '''OpenAI 임베딩 모델'''\n",
    "        return OpenAIEmbeddings(model=model)\n",
    "\n",
    "# 데이터 로드\n",
    "korean_documents = [\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        인공지능(AI)은 기계가 인간의 지능을 모방하여 학습하고, 추론하며, \n",
    "        문제를 해결할 수 있도록 하는 기술입니다. 최근 대규모 언어 모델(LLM)의 \n",
    "        발전으로 AI는 자연어 처리, 번역, 요약 등 다양한 분야에서 활용되고 있습니다.\n",
    "        특히 GPT-4, Claude, Gemini 등의 모델이 주목받고 있습니다.\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"ai_intro\", \"topic\": \"인공지능\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        RAG(Retrieval-Augmented Generation)는 검색 증강 생성 기술로,\n",
    "        LLM의 한계를 보완합니다. 기업의 내부 문서나 최신 정보를 벡터 \n",
    "        데이터베이스에 저장하고, 사용자 질문과 관련된 문서를 검색하여\n",
    "        답변의 정확성을 높입니다. 이를 통해 환각(Hallucination) 현상을 \n",
    "        줄일 수 있습니다.\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"rag_intro\", \"topic\": \"RAG\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        LangChain은 LLM 애플리케이션 개발을 위한 프레임워크입니다.\n",
    "        프롬프트 관리, 체인 구성, 메모리 시스템, 에이전트 등\n",
    "        다양한 기능을 제공합니다. Python과 JavaScript 버전이 있으며,\n",
    "        OpenAI, Anthropic, Hugging Face 등 다양한 모델과 통합됩니다.\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"langchain_intro\", \"topic\": \"프레임워크\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        벡터 데이터베이스는 고차원 벡터를 효율적으로 저장하고 검색하는\n",
    "        데이터베이스입니다. 텍스트, 이미지, 오디오 등을 임베딩 벡터로 \n",
    "        변환하여 저장하면, 의미적으로 유사한 항목을 빠르게 찾을 수 있습니다.\n",
    "        ChromaDB, Pinecone, Weaviate, FAISS 등이 대표적입니다.\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"vectordb_intro\", \"topic\": \"데이터베이스\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        한국어 자연어 처리는 영어와 다른 특성을 가집니다. 한국어는 교착어로서\n",
    "        조사와 어미가 단어에 붙어 문장의 의미를 결정합니다. 따라서 \n",
    "        형태소 분석, 적절한 토큰화, 다국어 지원 임베딩 모델 사용이 중요합니다.\n",
    "        KoNLPy, Mecab 등의 한국어 특화 도구를 활용할 수 있습니다.\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"korean_nlp\", \"topic\": \"한국어\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        프롬프트 엔지니어링은 LLM에게 효과적인 지시를 내리는 기술입니다.\n",
    "        Zero-shot, Few-shot, Chain-of-Thought 등의 기법이 있습니다.\n",
    "        좋은 프롬프트는 명확하고 구체적이며, 충분한 문맥을 제공해야 합니다.\n",
    "        시스템 프롬프트를 통해 AI의 역할과 규칙을 정의할 수 있습니다.\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"prompt_engineering\", \"topic\": \"프롬프트\"}\n",
    "    )\n",
    "]\n",
    "test_texts = (\n",
    "    '한국어 자연어처리란 무엇인가요?',\n",
    "    'RAG 시스템의 장점을 설명해 주세요',\n",
    "    '벡터 데이터베이스의 종류'\n",
    ")\n",
    "from time import time\n",
    "# 임베딩 모델 테스트\n",
    "# openai 임베딩(base)\n",
    "openai_embeddings = KoreanEmbeddingModels.get_openai()\n",
    "start_time = time()\n",
    "openai_vectors = openai_embeddings.embed_documents(test_texts)\n",
    "elapsed = time() - start_time\n",
    "print('openai 임베딩')\n",
    "print(f'벡터차원 : {len(openai_vectors)}')\n",
    "print(f'처리시간 : {elapsed:.2f}')\n",
    "\n",
    "# BGE-M3 모델 테스트\n",
    "hf_embeddings = KoreanEmbeddingModels.get_bge_m3()\n",
    "start_time = time()\n",
    "bgem3_vectors = openai_embeddings.embed_documents(test_texts)\n",
    "elapsed = time() - start_time\n",
    "print('hf_embeddings 임베딩')\n",
    "print(f'벡터차원 : {len(bgem3_vectors)}')\n",
    "print(f'처리시간 : {elapsed:.2f}')\n",
    "\n",
    "ko_orberta = KoreanEmbeddingModels.get_korean_roberta()\n",
    "start_time = time()\n",
    "ko_orberta_vectors = ko_orberta.embed_documents(test_texts)\n",
    "elapsed = time() - start_time\n",
    "print('ko_orberta_vectors 임베딩')\n",
    "print(f'벡터차원 : {len(ko_orberta_vectors)}')\n",
    "print(f'처리시간 : {elapsed:.2f}')\n",
    "\n",
    "# 벡터DB 구축, 검색 테스트\n",
    "# 청킹(텍스트 분할)\n",
    "text_spliter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 300,\n",
    "    chunk_overlap = 50,\n",
    ")\n",
    "doc_chunks = text_spliter.split_documents(korean_documents)\n",
    "print(f'문서분할 완료: {len(doc_chunks)}개 청크')\n",
    "\n",
    "# VectorDB 구축\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents = doc_chunks,\n",
    "    collection_name = 'korean_docs',\n",
    "    embedding=hf_embeddings # BGM-M3 모델\n",
    ")\n",
    "print(f'검색 테스트 결과')\n",
    "for query in test_texts:\n",
    "    results = vectorstore.similarity_search(query)\n",
    "    print(f'질문: {query}')\n",
    "    print(f'걈색결과: {results[0].metadata.get('topic','N/A')}')\n",
    "    print(f'걈색결과: {results[0].page_content}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "965a8d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name BM-K/KoSimCSE-roberta-multitask. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "from typing import  List, Tuple\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# langchain\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "#BS25\n",
    "from rank_bm25 import BM25Okapi\n",
    "# 환경설정\n",
    "load_dotenv()\n",
    "\n",
    "if not os.getenv('OPENAI_API_KEY'):\n",
    "    raise ValueError('check openai key in .env')\n",
    "\n",
    "# HuggingFace 임베딩 시도\n",
    "try:\n",
    "    from langchain_huggingface import HuggingFaceEmbeddings\n",
    "except:\n",
    "    print('pip install langchain-hggingface sentence-transformers')\n",
    "\n",
    "# 임베딩 모델 정의\n",
    "class KoreanEmbeddingModels:\n",
    "    '''한국어 임베딩 모델 팩토리 클래스\n",
    "    다양한 임베딩 모델을 쉽게 교체할 수 있도록 추상화\n",
    "    '''\n",
    "    @staticmethod\n",
    "    def get_bge_m3(device: str = 'cpu'):\n",
    "        '''BGE-M3 모델 반환\n",
    "         - Dense + Sparse 임베딩을 지원\n",
    "         - 다국어 지원(한국어 우수)\n",
    "        '''\n",
    "        return HuggingFaceEmbeddings(\n",
    "            model_name = 'BAAI/bge-m3',\n",
    "            model_kwargs = {\n",
    "                'device':device,\n",
    "                'trust_remote_code':True\n",
    "            },\n",
    "            encode_kwargs = {\n",
    "                'normalize_embeddings':True,  # 정규화를 해서 코사인 유사도 계산을 용이하게..\n",
    "                'batch_size':32\n",
    "            }\n",
    "        )\n",
    "    @staticmethod\n",
    "    def get_multilingual_e5(device:str='cpu'):\n",
    "        '''Multilingual-E5 모델 반환\n",
    "        - 경량화\n",
    "        - 다국어지원\n",
    "        '''\n",
    "        return HuggingFaceEmbeddings(\n",
    "            model_name = 'intfloat/multilingual-e5-large',\n",
    "            model_kwargs = {'device':device},\n",
    "            encode_kwargs= {'normalize_embeddings':True}\n",
    "        )\n",
    "    @staticmethod\n",
    "    def get_korean_roberta(device:str = 'cpu'):\n",
    "        '''BM-K/KoSimCSE-roberta-multitask'''\n",
    "        return HuggingFaceEmbeddings(\n",
    "            model_name = 'BM-K/KoSimCSE-roberta-multitask',\n",
    "            model_kwargs = {'device':device},\n",
    "            encode_kwargs= {'normalize_embeddings':True}\n",
    "        )\n",
    "    @staticmethod\n",
    "    def get_openai(model:str = 'text-embedding-3-small'):\n",
    "        '''OpenAI 임베딩 모델'''\n",
    "        return OpenAIEmbeddings(model=model)\n",
    "\n",
    "# 데이터 로드\n",
    "korean_documents = [\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        인공지능(AI)은 기계가 인간의 지능을 모방하여 학습하고, 추론하며, \n",
    "        문제를 해결할 수 있도록 하는 기술입니다. 최근 대규모 언어 모델(LLM)의 \n",
    "        발전으로 AI는 자연어 처리, 번역, 요약 등 다양한 분야에서 활용되고 있습니다.\n",
    "        특히 GPT-4, Claude, Gemini 등의 모델이 주목받고 있습니다.\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"ai_intro\", \"topic\": \"인공지능\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        RAG(Retrieval-Augmented Generation)는 검색 증강 생성 기술로,\n",
    "        LLM의 한계를 보완합니다. 기업의 내부 문서나 최신 정보를 벡터 \n",
    "        데이터베이스에 저장하고, 사용자 질문과 관련된 문서를 검색하여\n",
    "        답변의 정확성을 높입니다. 이를 통해 환각(Hallucination) 현상을 \n",
    "        줄일 수 있습니다.\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"rag_intro\", \"topic\": \"RAG\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        LangChain은 LLM 애플리케이션 개발을 위한 프레임워크입니다.\n",
    "        프롬프트 관리, 체인 구성, 메모리 시스템, 에이전트 등\n",
    "        다양한 기능을 제공합니다. Python과 JavaScript 버전이 있으며,\n",
    "        OpenAI, Anthropic, Hugging Face 등 다양한 모델과 통합됩니다.\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"langchain_intro\", \"topic\": \"프레임워크\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        벡터 데이터베이스는 고차원 벡터를 효율적으로 저장하고 검색하는\n",
    "        데이터베이스입니다. 텍스트, 이미지, 오디오 등을 임베딩 벡터로 \n",
    "        변환하여 저장하면, 의미적으로 유사한 항목을 빠르게 찾을 수 있습니다.\n",
    "        ChromaDB, Pinecone, Weaviate, FAISS 등이 대표적입니다.\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"vectordb_intro\", \"topic\": \"데이터베이스\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        한국어 자연어 처리는 영어와 다른 특성을 가집니다. 한국어는 교착어로서\n",
    "        조사와 어미가 단어에 붙어 문장의 의미를 결정합니다. 따라서 \n",
    "        형태소 분석, 적절한 토큰화, 다국어 지원 임베딩 모델 사용이 중요합니다.\n",
    "        KoNLPy, Mecab 등의 한국어 특화 도구를 활용할 수 있습니다.\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"korean_nlp\", \"topic\": \"한국어\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        프롬프트 엔지니어링은 LLM에게 효과적인 지시를 내리는 기술입니다.\n",
    "        Zero-shot, Few-shot, Chain-of-Thought 등의 기법이 있습니다.\n",
    "        좋은 프롬프트는 명확하고 구체적이며, 충분한 문맥을 제공해야 합니다.\n",
    "        시스템 프롬프트를 통해 AI의 역할과 규칙을 정의할 수 있습니다.\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"prompt_engineering\", \"topic\": \"프롬프트\"}\n",
    "    )\n",
    "]\n",
    "\n",
    "test_texts = [\n",
    "    '한국어 자연어 처리란 무엇인가요',\n",
    "    'RAG 시스템의 장점을 설명해 주세요',\n",
    "    '벡터 데이터베이스의 종류'\n",
    "]\n",
    "from time import time\n",
    "# 임베딩 모델 테스트\n",
    "# openai 임베딩(base)\n",
    "embeding_models = (\n",
    "    KoreanEmbeddingModels.get_openai(), KoreanEmbeddingModels.get_bge_m3(), \n",
    "    KoreanEmbeddingModels.get_korean_roberta(),KoreanEmbeddingModels.get_multilingual_e5()\n",
    ")\n",
    "embedding_model_name = ('openai','bge_m3','korean_roberta','multilingual_e5')\n",
    "\n",
    "def evaluate_embeding_models(embeingmodel:KoreanEmbeddingModels, test_texts:List[str],model_name:str):\n",
    "    start_time = time()\n",
    "    vectors = embeingmodel.embed_documents(test_texts)\n",
    "    elapsed = time() - start_time\n",
    "    print('openai 임베딩')\n",
    "    print(f'벡터차원 : {len(vectors)}')\n",
    "    print(f'처리시간 : {elapsed:.2f}')\n",
    "\n",
    "# 각 임베딩 모델 평가\n",
    "for idx, model in enumerate( embeding_models):\n",
    "    evaluate_embeding_models(model, test_texts, embeding_model_names[idx])\n",
    "\n",
    "# VectorDB 구축 , 검색 테스트\n",
    "# 청킹(텍스트 분할)\n",
    "text_spliter =  RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "doc_chunks = text_spliter.split_documents(korean_documents)\n",
    "print(f'문서분할 완료 : {len(doc_chunks)}개 청크')\n",
    "\n",
    "\n",
    "# 각 모델별 VectorDB구축 및 테스트\n",
    "for model in embeding_models:\n",
    "    # VectorDB 구축\n",
    "    vectorStore =  Chroma.from_documents(\n",
    "        documents=doc_chunks,\n",
    "        collection_name='korean_docs',\n",
    "        embedding=model\n",
    "    )\n",
    "    print('검색 테스트 결과')\n",
    "    for query in test_texts:\n",
    "        results = vectorStore.similarity_search(query)\n",
    "        print(f'\\n\\n질문 : {query}')\n",
    "        print(f'검색결과 : {results[0].metadata.get('topic', 'N/A')}')\n",
    "        print(f'찾은 문장 : {results[0].page_content}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3595def4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
