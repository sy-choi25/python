{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7231297",
   "metadata": {},
   "source": [
    "Sentence-Transformers\n",
    "\n",
    "  - BERT 계열 모델의 토큰 출력을 Pooling하여 고정 크기 문장 벡터를 생성하는 시스템\n",
    "  - 두 가지 인코딩 방식\n",
    "    - **Sentence Encoder (Bi-Encoder)**\n",
    "      - 각 문장을 독립적으로 인코딩\n",
    "      - 빠름: O(n)\n",
    "      - 벡터끼리 코사인 유사도만 계산하면 됨\n",
    "      - Sentence-BERT가 사용하는 방식\n",
    "      - 용도: 검색, 유사도 비교, 군집화\n",
    "    - **Cross-Encoder**\n",
    "      - 문장 2개를 동시에 BERT에 넣어 분류”하는 구조\n",
    "      - 느림: O(n²)\n",
    "      - 정확도는 높음\n",
    "\n",
    "Pooling 종류 (Sentence-BERT의 핵심 단계) \n",
    "- Pooling의 역할 — “토큰 벡터를 문장 벡터로 압축하는 과정”\n",
    "  1) [CLS] Pooling  \n",
    "  첫 번째 토큰([CLS])만 사용  \n",
    "  성능: 보통  \n",
    "\n",
    "  2) Mean Pooling  \n",
    "  모든 토큰의 임베딩 평균 → 문장 벡터  \n",
    "  성능: 가장 좋음 (SBERT 기본 추천)  \n",
    "\n",
    "  3) Max Pooling  \n",
    "  각 차원별로 최대값을 선택  \n",
    "  성능: 보통  \n",
    "\n",
    "한국어 모델 활용(KR-SBERT)\n",
    "- 한국어 데이터로 파인튜닝된 Sentence-BERT 모델\n",
    "- KorSBERT / KoSentenceBERT 등\n",
    "- 한국어 유사도 평가나 문장 검색에 효과적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d1a641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\SAMSUNG\\miniconda3\\envs\\llm_env\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "임베딩 크기 : (3, 768)\n",
      "유사도 행렬 : [[1.         0.80070263 0.24432442]\n",
      " [0.80070263 1.         0.178646  ]\n",
      " [0.24432442 0.178646   1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Sentence Transformer\n",
    "# 1) 모델 로드\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('snunlp/KR-SBERT-V40K-klueNLI-augSTS')\n",
    "\n",
    "# 2) 문장 목록과 임베딩\n",
    "sentences = [\n",
    "    '오늘 날씨가 좋아요',\n",
    "    '오늘 하늘이 맑아요',\n",
    "    '프로그램을 배우고 싶습니다.'\n",
    "]\n",
    "embeddings = model.encode(sentences)       \n",
    "# shape -> (3, D)/ 문장을 의미 기반의 벡터 표현으로 바꾸는 표정\n",
    "# 문장을 토큰화/ 토큰을 BERT로 인코딩/ 토큰 임베딩을 문장 임베딩함\n",
    "# BERT로 인코딩 -> 문맥을 이해한 벡터로 변환한다는 뜻 -> 입력 문장을 토큰으로 쪼개고 앞뒤 문장을 고려해서 각 토큰을 숫자벡터로 바꾸는 과정\n",
    "# => 단순 숫자가 아닌 문장의 의미 관계 문맥을 반영한 표현으로 바꾸는 것 \n",
    "print(f'임베딩 크기 : {embeddings.shape}')\n",
    "\n",
    "# 3) 유사도 계산 (코사인 유사도) -> 벡터 방향 비교\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity = cosine_similarity(embeddings)  # 모든 문장 쌍을 서로 비교\n",
    "print(f'유사도 행렬 : {similarity}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14979663",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, models \n",
    "transformer = models.Transformer('klue/roberta-base')\n",
    "# 레이어 추가\n",
    "pooling = models.Pooling(\n",
    "    transformer.get_word_embedding_dimension(),\n",
    "    pooling_mode_mean_tokens=True,  # mean pooling 사용\n",
    "    pooling_mode_cls_token=False,\n",
    "    pooling_mode_max_tokens=False\n",
    ")\n",
    "# 모델 조립\n",
    "model = SentenceTransformer(modules=[transformer, pooling])\n",
    "# 문장 임베딩 생성\n",
    "sentence = ['안녕하세요','반갑습니다']\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
