{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a27b215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sqllite 데이터베이스 초기화 중\n",
      "sqllite 데이터베이스 초기화 완료\n",
      "질문 : 로컬 모니터링의 장점은 무엇인가요? \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Completions.create() got an unexpected keyword argument 'callback'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 495\u001b[39m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m quesiton \u001b[38;5;129;01min\u001b[39;00m test_questions:\n\u001b[32m    494\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m질문 : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquesiton\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m495\u001b[39m     answer = \u001b[43mrag_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquesiton\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    496\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m답변 : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manswer\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    498\u001b[39m \u001b[38;5;66;03m# 모니터링 - 요약통계\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SAMSUNG\\miniconda3\\envs\\open\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3129\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3127\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3128\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3129\u001b[39m                 input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3130\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3131\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SAMSUNG\\miniconda3\\envs\\open\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:398\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    386\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     **kwargs: Any,\n\u001b[32m    392\u001b[39m ) -> AIMessage:\n\u001b[32m    393\u001b[39m     config = ensure_config(config)\n\u001b[32m    394\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    395\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    396\u001b[39m         cast(\n\u001b[32m    397\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    408\u001b[39m         ).message,\n\u001b[32m    409\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SAMSUNG\\miniconda3\\envs\\open\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1117\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1108\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1110\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1114\u001b[39m     **kwargs: Any,\n\u001b[32m   1115\u001b[39m ) -> LLMResult:\n\u001b[32m   1116\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SAMSUNG\\miniconda3\\envs\\open\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:927\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    924\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    925\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    926\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m927\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m         )\n\u001b[32m    934\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    935\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SAMSUNG\\miniconda3\\envs\\open\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1221\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1219\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1220\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1221\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1225\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SAMSUNG\\miniconda3\\envs\\open\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1356\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1354\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mhttp_response\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1355\u001b[39m         e.response = raw_response.http_response  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1356\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   1357\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1358\u001b[39m     \u001b[38;5;28mself\u001b[39m.include_response_headers\n\u001b[32m   1359\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1360\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1361\u001b[39m ):\n\u001b[32m   1362\u001b[39m     generation_info = {\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response.headers)}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SAMSUNG\\miniconda3\\envs\\open\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1351\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1344\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _construct_lc_result_from_responses_api(\n\u001b[32m   1345\u001b[39m             response,\n\u001b[32m   1346\u001b[39m             schema=original_schema_obj,\n\u001b[32m   1347\u001b[39m             metadata=generation_info,\n\u001b[32m   1348\u001b[39m             output_version=\u001b[38;5;28mself\u001b[39m.output_version,\n\u001b[32m   1349\u001b[39m         )\n\u001b[32m   1350\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1351\u001b[39m         raw_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_raw_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1352\u001b[39m         response = raw_response.parse()\n\u001b[32m   1353\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SAMSUNG\\miniconda3\\envs\\open\\Lib\\site-packages\\openai\\_legacy_response.py:364\u001b[39m, in \u001b[36mto_raw_response_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m extra_headers[RAW_RESPONSE_HEADER] = \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    362\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m] = extra_headers\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SAMSUNG\\miniconda3\\envs\\open\\Lib\\site-packages\\openai\\_utils\\_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: Completions.create() got an unexpected keyword argument 'callback'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import json\n",
    "import sqlite3\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Optional\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "# 필수 라이브러리 임포트\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.callbacks import BaseCallbackHandler\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Sqlite 기반 추적 시스템\n",
    "class LocalTraceDB:\n",
    "    '''SQLite 기반 로컬 추적시스템\n",
    "    LangSmith 대신 로컬에서 모든 LLM 호출을 추적하고 저장\n",
    "    '''\n",
    "    def __init__(self, db_path:str = 'local_traces.db'):\n",
    "        self.db_path = db_path\n",
    "        self._init_db()\n",
    "    def _init_db(self):\n",
    "        '''데이터베이스 초기화 및 데이블 생성'''\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        # 실행 추적 테이블\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS runs(\n",
    "                       id TEXT PRIMARY KEY,\n",
    "                       name TEXT,\n",
    "                       run_type TEXT,\n",
    "                       start_time TEXT,\n",
    "                       end_time TEXT,\n",
    "                       duration_seconds REAL,\n",
    "                       input_data TEXT,\n",
    "                       output_data TEXT,\n",
    "                       metadata TEXT,\n",
    "                       status TEXT,\n",
    "                       error TEXT\n",
    "                       )\n",
    "        ''')\n",
    "        # 메트릭 테이블\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS metircs(\n",
    "                       id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                       run_id TEXT,\n",
    "                       metric_name TEXT,\n",
    "                       metirc_value REAL,\n",
    "                       recorded_at TEXT,\n",
    "                       FOREIGN KEY(run_id) REFERENCES runs(id)\n",
    "                       )\n",
    "        ''')\n",
    "        # 토큰사용량 테이블\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS token_usage(\n",
    "                       id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                       run_id TEXT,\n",
    "                       prompt_tokens INTEGER,\n",
    "                       completion_tokens INTEGER,\n",
    "                       total_tokens INTEGER,\n",
    "                       estimated_cost REAL,\n",
    "                       model TEXT,\n",
    "                       recorded_at TEXT,\n",
    "                       FOREIGN KEY(run_id) REFERENCES runs(id)\n",
    "                       )\n",
    "        ''')     \n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "\n",
    "    def start_run(self, name:str, run_type:str, input_data:Any, metadata:Dict=None) -> str:\n",
    "        '''새 실행 시작'''\n",
    "        run_id = str(uuid.uuid4())\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute('''\n",
    "            INSERT INTO runs(id, name, run_type, start_time, input_data, metadata,status)\n",
    "                       values(?,?,?,?,?,?,?)'''\n",
    "                       ,(\n",
    "                        run_id,name,run_type,datetime.now().isoformat(),\n",
    "                        json.dumps(input_data, ensure_ascii=False) if input_data else None,\n",
    "                        json.dumps(metadata, ensure_ascii=False) if input_data else None,   \n",
    "                        'running'\n",
    "                       ))\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        return run_id\n",
    "    \n",
    "    def end_run(self, run_id:str, output_data:Any, status:str='success', error:str=None):\n",
    "        '''실행완료'''\n",
    "        conn=sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        #시작시간 가져오기\n",
    "        cursor.execute('SELECT start_time FROM runs WHERE ID = ?', (run_id))\n",
    "        result = cursor.fetchone()\n",
    "        if result:\n",
    "            start_time = datetime.fromisoformat(result[0])\n",
    "            end_time = datetime.now()\n",
    "            duration = (end_time - start_time).total_seconds()\n",
    "            cursor.execute('''\n",
    "                UPDATE runs\n",
    "                           SET end_time = ?, duration_seconds=?, output_data=?,status=?,error=?\n",
    "                           WHERE id = ?''',\n",
    "                           (\n",
    "                               end_time.isoformat(),\n",
    "                               duration,\n",
    "                               json.dumps(output_data,ensure_ascii=False) if output_data else None,\n",
    "                               status,\n",
    "                               error,\n",
    "                               run_id\n",
    "                           ))\n",
    "            conn.commit()\n",
    "            conn.close()\n",
    "\n",
    "    def record_token_usage(self, run_id:str, prompt_tokens:int ,completion_tokens:int, model:str='gpt-4o-mini'):\n",
    "        '''토큰사용량'''\n",
    "        total_tokens = prompt_tokens + completion_tokens\n",
    "        # 비용추정( gpt-4o-mini 기준)\n",
    "        cost_per_1k_input = 0.00015\n",
    "        cost_per_1k_output = 0.0006\n",
    "        estimated_cost = (prompt_tokens / 1000*cost_per_1k_input + \n",
    "                          completion_tokens / 1000 * cost_per_1k_output)\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        cursor.execute('''\n",
    "            INSERT INTO token_usage(run_id, prompt_tokens, completion_tokens, total_tokens, estimated_cost, model, recorded_at)\n",
    "                       values(?,?,?,?,?,?,?)'''\n",
    "                       ,(\n",
    "                           run_id,prompt_tokens,completion_tokens,total_tokens,estimated_cost,model,datetime.now().isoformat()\n",
    "                       ))\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "\n",
    "    def get_summary(self) -> Dict:\n",
    "        \"\"\"전체 요약 통계\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # 총 실행 수\n",
    "        cursor.execute(\"SELECT COUNT(*) FROM runs\")\n",
    "        total_runs = cursor.fetchone()[0]\n",
    "        \n",
    "        # 평균 응답 시간\n",
    "        cursor.execute(\"SELECT AVG(duration_seconds) FROM runs WHERE status = 'success'\")\n",
    "        avg_duration = cursor.fetchone()[0] or 0\n",
    "        \n",
    "        # 성공률\n",
    "        cursor.execute(\"SELECT COUNT(*) FROM runs WHERE status = 'success'\")\n",
    "        success_count = cursor.fetchone()[0]\n",
    "        success_rate = (success_count / total_runs * 100) if total_runs > 0 else 0\n",
    "        \n",
    "        # 총 토큰 사용량\n",
    "        cursor.execute(\"SELECT SUM(total_tokens), SUM(estimated_cost) FROM token_usage\")\n",
    "        token_result = cursor.fetchone()\n",
    "        total_tokens = token_result[0] or 0\n",
    "        total_cost = token_result[1] or 0\n",
    "        \n",
    "        conn.close()\n",
    "        \n",
    "        return {\n",
    "            \"total_runs\": total_runs,\n",
    "            \"avg_duration_seconds\": round(avg_duration, 2),\n",
    "            \"success_rate\": round(success_rate, 1),\n",
    "            \"total_tokens\": total_tokens,\n",
    "            \"total_estimated_cost\": round(total_cost, 4)\n",
    "        }\n",
    "    \n",
    "    def get_recent_runs(self, limit: int = 10) -> List[Dict]:\n",
    "        \"\"\"최근 실행 기록\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT id, name, run_type, start_time, duration_seconds, status\n",
    "            FROM runs\n",
    "            ORDER BY start_time DESC\n",
    "            LIMIT ?\n",
    "        \"\"\", (limit,))\n",
    "        \n",
    "        runs = []\n",
    "        for row in cursor.fetchall():\n",
    "            runs.append({\n",
    "                \"id\": row[0][:8] + \"...\",  # 짧게 표시\n",
    "                \"name\": row[1],\n",
    "                \"type\": row[2],\n",
    "                \"time\": row[3][:19] if row[3] else None,\n",
    "                \"duration\": f\"{row[4]:.2f}s\" if row[4] else None,\n",
    "                \"status\": row[5]\n",
    "            })\n",
    "        \n",
    "        conn.close()\n",
    "        return runs\n",
    "\n",
    "#콜벡핸들러\n",
    "class LocalMonitoringHandler(BaseCallbackHandler):\n",
    "    '''로컬모니터링을 위한 콜백 핸들러\n",
    "    모든 llm 호출을 로컬 SQLite3 DB에 기록'''\n",
    "    def __init__(self,trace_db:LocalTraceDB, log_to_console:bool = True):\n",
    "        self.trace_db = trace_db\n",
    "        self.log_to_console = log_to_console\n",
    "        self.current_run_id = None\n",
    "        self.start_time = None\n",
    "    def on_llm_start(self, serialized:Dict[str, Any], prompts:List[str], **kwargs):\n",
    "        '''LLM 호출 시작'''\n",
    "        self.start_time = datetime.now()\n",
    "        model_name = serialized.get('name','UnKown')\n",
    "        # db에 실행시간 기록\n",
    "        self.current_run_id = self.trace_db.start_run(\n",
    "            name = f'llm_call_{model_name}',\n",
    "            run_type='llm',\n",
    "            input_data={'prompts':prompts[:1]},\n",
    "            metadata={'model':model_name}\n",
    "        )\n",
    "        if self.log_to_console:\n",
    "            print(f\"    llm 호출시작        : {self.start_time.strftime('%H:%M:%S')}\")\n",
    "            print(f\"    모델               : {model_name}\")\n",
    "            print(f\"    프롬프트길이        : {len(prompts[0])}\")\n",
    "    def on_llm_end(self, response,  **kwargs):\n",
    "        '''llm 호출 완료'''\n",
    "        end_time = datetime.now()\n",
    "        duration = (end_time - self.start_time).total_seconds()\n",
    "        \n",
    "        # 토큰 사용량 추출\n",
    "        token_usage = {}\n",
    "        if hasattr(response, 'llm_output') and response.llm_output:\n",
    "            token_usage = response.llm_output.get('token_usage', {})\n",
    "        \n",
    "        # 출력 텍스트 추출\n",
    "        output_text = \"\"\n",
    "        if response.generations:\n",
    "            output_text = response.generations[0][0].text if response.generations[0] else \"\"\n",
    "        \n",
    "        # DB에 기록\n",
    "        if self.current_run_id:\n",
    "            self.trace_db.end_run(\n",
    "                self.current_run_id,\n",
    "                output_data={\"response\": output_text[:500]},  # 처음 500자만\n",
    "                status=\"success\"\n",
    "            )\n",
    "            \n",
    "            # 토큰 사용량 기록\n",
    "            if token_usage:\n",
    "                self.trace_db.record_token_usage(\n",
    "                    self.current_run_id,\n",
    "                    token_usage.get('prompt_tokens', 0),\n",
    "                    token_usage.get('completion_tokens', 0)\n",
    "                )\n",
    "            \n",
    "            # 응답 시간 메트릭\n",
    "            self.trace_db.record_metric(\n",
    "                self.current_run_id, \"latency_seconds\", duration\n",
    "            )\n",
    "        \n",
    "        if self.log_to_console:\n",
    "            print(f\"   LLM 호출 완료: {duration:.2f}초 소요\")\n",
    "            if token_usage:\n",
    "                print(f\"      입력 토큰: {token_usage.get('prompt_tokens', 'N/A')}\")\n",
    "                print(f\"      출력 토큰: {token_usage.get('completion_tokens', 'N/A')}\")\n",
    "    \n",
    "    def on_llm_error(self, error: Exception, **kwargs):\n",
    "        \"\"\"LLM 오류 발생\"\"\"\n",
    "        if self.current_run_id:\n",
    "            self.trace_db.end_run(\n",
    "                self.current_run_id,\n",
    "                output_data=None,\n",
    "                status=\"error\",\n",
    "                error=str(error)\n",
    "            )\n",
    "        \n",
    "        if self.log_to_console:\n",
    "            print(f\"    LLM 오류: {error}\")\n",
    "    \n",
    "    def on_chain_start(self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs):\n",
    "        \"\"\"체인 시작\"\"\"\n",
    "        if self.log_to_console:\n",
    "            chain_name = serialized.get('name', 'Unknown')\n",
    "            print(f\"\\n    체인 시작: {chain_name}\")\n",
    "    \n",
    "    def on_chain_end(self, outputs: Dict[str, Any], **kwargs):\n",
    "        \"\"\"체인 완료\"\"\"\n",
    "        if self.log_to_console:\n",
    "            print(f\"    체인 완료\")\n",
    "    \n",
    "    def on_retriever_start(self, serialized: Dict[str, Any], query: str, **kwargs):\n",
    "        \"\"\"리트리버 시작\"\"\"\n",
    "        if self.log_to_console:\n",
    "            print(f\"\\n    검색 시작: '{query[:50]}...'\")\n",
    "    \n",
    "    def on_retriever_end(self, documents: List[Document], **kwargs):\n",
    "        \"\"\"리트리버 완료\"\"\"\n",
    "        if self.log_to_console:\n",
    "            print(f\"    검색 완료: {len(documents)}개 문서 반환\")    \n",
    "\n",
    "class LocalMonitoringHandler(BaseCallbackHandler):\n",
    "    \"\"\"\n",
    "    로컬 모니터링을 위한 콜백 핸들러\n",
    "    \n",
    "    모든 LLM 호출을 로컬 SQLite DB에 기록합니다.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, trace_db: LocalTraceDB, log_to_console: bool = True):\n",
    "        self.trace_db = trace_db\n",
    "        self.log_to_console = log_to_console\n",
    "        self.current_run_id = None\n",
    "        self.start_time = None\n",
    "        \n",
    "    def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **kwargs):\n",
    "        \"\"\"LLM 호출 시작\"\"\"\n",
    "        self.start_time = datetime.now()\n",
    "        model_name = serialized.get(\"name\", \"Unknown\")\n",
    "        \n",
    "        # DB에 실행 기록 시작\n",
    "        self.current_run_id = self.trace_db.start_run(\n",
    "            name=f\"llm_call_{model_name}\",\n",
    "            run_type=\"llm\",\n",
    "            input_data={\"prompts\": prompts[:1]},  # 첫 프롬프트만 저장\n",
    "            metadata={\"model\": model_name}\n",
    "        )\n",
    "        \n",
    "        if self.log_to_console:\n",
    "            print(f\"\\n   LLM 호출 시작: {self.start_time.strftime('%H:%M:%S')}\")\n",
    "            print(f\"      모델: {model_name}\")\n",
    "            print(f\"      프롬프트 길이: {len(prompts[0])}자\")\n",
    "    \n",
    "    def on_llm_end(self, response, **kwargs):\n",
    "        \"\"\"LLM 호출 완료\"\"\"\n",
    "        end_time = datetime.now()\n",
    "        duration = (end_time - self.start_time).total_seconds()\n",
    "        \n",
    "        # 토큰 사용량 추출\n",
    "        token_usage = {}\n",
    "        if hasattr(response, 'llm_output') and response.llm_output:\n",
    "            token_usage = response.llm_output.get('token_usage', {})\n",
    "        \n",
    "        # 출력 텍스트 추출\n",
    "        output_text = \"\"\n",
    "        if response.generations:\n",
    "            output_text = response.generations[0][0].text if response.generations[0] else \"\"\n",
    "        \n",
    "        # DB에 기록\n",
    "        if self.current_run_id:\n",
    "            self.trace_db.end_run(\n",
    "                self.current_run_id,\n",
    "                output_data={\"response\": output_text[:500]},  # 처음 500자만\n",
    "                status=\"success\"\n",
    "            )\n",
    "            \n",
    "            # 토큰 사용량 기록\n",
    "            if token_usage:\n",
    "                self.trace_db.record_token_usage(\n",
    "                    self.current_run_id,\n",
    "                    token_usage.get('prompt_tokens', 0),\n",
    "                    token_usage.get('completion_tokens', 0)\n",
    "                )\n",
    "            \n",
    "            # 응답 시간 메트릭\n",
    "            self.trace_db.record_metric(\n",
    "                self.current_run_id, \"latency_seconds\", duration\n",
    "            )\n",
    "        \n",
    "        if self.log_to_console:\n",
    "            print(f\"   LLM 호출 완료: {duration:.2f}초 소요\")\n",
    "            if token_usage:\n",
    "                print(f\"      입력 토큰: {token_usage.get('prompt_tokens', 'N/A')}\")\n",
    "                print(f\"      출력 토큰: {token_usage.get('completion_tokens', 'N/A')}\")\n",
    "    \n",
    "    def on_llm_error(self, error: Exception, **kwargs):\n",
    "        \"\"\"LLM 오류 발생\"\"\"\n",
    "        if self.current_run_id:\n",
    "            self.trace_db.end_run(\n",
    "                self.current_run_id,\n",
    "                output_data=None,\n",
    "                status=\"error\",\n",
    "                error=str(error)\n",
    "            )\n",
    "        \n",
    "        if self.log_to_console:\n",
    "            print(f\"   LLM 오류: {error}\")\n",
    "    \n",
    "    def on_chain_start(self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs):\n",
    "        \"\"\"체인 시작\"\"\"\n",
    "        if self.log_to_console:\n",
    "            chain_name = serialized.get('name', 'Unknown')\n",
    "            print(f\"\\n    체인 시작: {chain_name}\")\n",
    "    \n",
    "    def on_chain_end(self, outputs: Dict[str, Any], **kwargs):\n",
    "        \"\"\"체인 완료\"\"\"\n",
    "        if self.log_to_console:\n",
    "            print(f\"    체인 완료\")\n",
    "    \n",
    "    def on_retriever_start(self, serialized: Dict[str, Any], query: str, **kwargs):\n",
    "        \"\"\"리트리버 시작\"\"\"\n",
    "        if self.log_to_console:\n",
    "            print(f\"\\n    검색 시작: '{query[:50]}...'\")\n",
    "    \n",
    "    def on_retriever_end(self, documents: List[Document], **kwargs):\n",
    "        \"\"\"리트리버 완료\"\"\"\n",
    "        if self.log_to_console:\n",
    "            print(f\"    검색 완료: {len(documents)}개 문서 반환\")\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    load_dotenv()\n",
    "    # 로컬 모니터링 적용 RAG 체인\n",
    "    print('sqllite 데이터베이스 초기중.......')\n",
    "    trace_db = LocalTraceDB()\n",
    "    print('sqllite 데이터베이스 초기화 완료')\n",
    "\n",
    "    # 콜백핸들러 인스턴스(객체) 생성    \n",
    "    monitoring_handler = LocalMonitoringHandler(trace_db=trace_db)\n",
    "    # llm 설정(콜백포함)\n",
    "    llm = ChatOpenAI(model = 'gpt-4o-mini',callbacks=[monitoring_handler])\n",
    "    # 프롬프트 템플릿\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "     ('system','''당신은 llm 모니터링 전문가입니다. 제공된 문맥을 바탕으로 질문에 답변하세요\n",
    "      \n",
    "      규칙:\n",
    "      1. 문맥에 있는 정보만 사용하세요\n",
    "      2. 한국어로 명확하게 답변하세요\n",
    "      3. 구조화된 형태로 답변하세요'''),\n",
    "     ('human','''문맥:\n",
    "      {context}\n",
    "      \n",
    "      질문:{question}\n",
    "      \n",
    "      답변:''')   \n",
    "    ])\n",
    "    def format_docs(docs:List[Document])->str:\n",
    "        return '\\n\\n'.join([  doc.page_content for doc in docs ])\n",
    "    \n",
    "    documents = [\n",
    "        Document(\n",
    "            page_content=\"로컬 모니터링은 외부 서비스 없이 LLM 애플리케이션을 추적하는 방법입니다. \"\n",
    "                        \"SQLite를 사용하여 모든 실행 기록을 로컬에 저장할 수 있습니다.\",\n",
    "            metadata={\"source\": \"local_monitoring_intro\", \"topic\": \"모니터링\"}\n",
    "        ),\n",
    "        Document(\n",
    "            page_content=\"커스텀 콜백 핸들러의 장점: 1) 완전한 제어 가능, 2) 무료, 3) 오프라인 작동, \"\n",
    "                        \"4) 데이터 프라이버시 보장, 5) 커스터마이징 용이\",\n",
    "            metadata={\"source\": \"callback_benefits\", \"topic\": \"콜백\"}\n",
    "        ),\n",
    "        Document(\n",
    "            page_content=\"SQLite 기반 추적의 장점: 파일 하나로 모든 데이터 관리, 설치 불필요, \"\n",
    "                        \"SQL로 복잡한 분석 가능, 백업 및 이동 용이\",\n",
    "            metadata={\"source\": \"sqlite_benefits\", \"topic\": \"저장소\"}\n",
    "        ),\n",
    "        Document(\n",
    "            page_content=\"LLM 모니터링 핵심 메트릭: 응답 시간(Latency), 토큰 사용량(Token Usage), \"\n",
    "                        \"성공률(Success Rate), 비용(Cost), 오류율(Error Rate)\",\n",
    "            metadata={\"source\": \"metrics\", \"topic\": \"메트릭\"}\n",
    "        ),\n",
    "        Document(\n",
    "            page_content=\"로컬 모니터링 vs 클라우드 모니터링: 로컬은 무료/프라이버시 보장, \"\n",
    "                        \"클라우드는 협업/고급분석 용이. 개발 단계에서는 로컬, 프로덕션에서는 클라우드 권장\",\n",
    "            metadata={\"source\": \"comparison\", \"topic\": \"비교\"}\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    embeddings = OpenAIEmbeddings(model = 'text-embedding-3-small')\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
    "    doc_chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=doc_chunks,\n",
    "        collection_name='local_monitorings',\n",
    "        embedding=embeddings\n",
    "    )\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={'k':3})\n",
    "    # RAG 체인 구성\n",
    "    rag_chain = (\n",
    "        {'context':retriever | RunnableLambda(format_docs),\n",
    "         'question' : RunnablePassthrough()\n",
    "         }\n",
    "         | prompt\n",
    "         | llm\n",
    "         | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    # 테스트 문장\n",
    "    test_questions = [\n",
    "        \"로컬 모니터링의 장점은 무엇인가요?\",\n",
    "        \"LLM 모니터링에서 중요한 메트릭은 무엇인가요?\",\n",
    "        \"SQLite 기반 추적의 이점은?\"\n",
    "    ]\n",
    "    # 체인 실행\n",
    "    for question in test_questions:\n",
    "        print(f'질문 : {question}')\n",
    "        answer = rag_chain.invoke(question)\n",
    "        print(f'답변 : {answer}\\n')\n",
    "\n",
    "    # 모니터링 - 요약통계\n",
    "    summary = trace_db.get_summary()\n",
    "    print(f' 요약통계 : \\n{summary}\\n\\n')\n",
    "\n",
    "    # 최근실행 기록\n",
    "    recuent_runs = trace_db.get_recent_runs(5)    \n",
    "    for id, run in enumerate(recuent_runs,1):\n",
    "        print(f'{id} : {run}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "581fb7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_openai\n",
      "  Using cached langchain_openai-1.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting langchain-core<2.0.0,>=1.1.0 (from langchain_openai)\n",
      "  Using cached langchain_core-1.1.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: openai<3.0.0,>=1.109.1 in c:\\users\\samsung\\miniconda3\\envs\\openai\\lib\\site-packages (from langchain_openai) (2.8.1)\n",
      "Collecting tiktoken<1.0.0,>=0.7.0 (from langchain_openai)\n",
      "  Using cached tiktoken-0.12.0-cp314-cp314-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting jsonpatch<2.0.0,>=1.33.0 (from langchain-core<2.0.0,>=1.1.0->langchain_openai)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langsmith<1.0.0,>=0.3.45 (from langchain-core<2.0.0,>=1.1.0->langchain_openai)\n",
      "  Downloading langsmith-0.4.56-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\samsung\\miniconda3\\envs\\openai\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain_openai) (25.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\samsung\\miniconda3\\envs\\openai\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain_openai) (2.12.4)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\users\\samsung\\miniconda3\\envs\\openai\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain_openai) (6.0.3)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<2.0.0,>=1.1.0->langchain_openai)\n",
      "  Using cached tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\samsung\\miniconda3\\envs\\openai\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain_openai) (4.15.0)\n",
      "Collecting uuid-utils<1.0,>=0.12.0 (from langchain-core<2.0.0,>=1.1.0->langchain_openai)\n",
      "  Using cached uuid_utils-0.12.0-cp39-abi3-win_amd64.whl.metadata (1.1 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.1.0->langchain_openai)\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\samsung\\miniconda3\\envs\\openai\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_openai) (0.28.1)\n",
      "Collecting orjson>=3.9.14 (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_openai)\n",
      "  Downloading orjson-3.11.5-cp314-cp314-win_amd64.whl.metadata (42 kB)\n",
      "Collecting requests-toolbelt>=1.0.0 (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_openai)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\samsung\\miniconda3\\envs\\openai\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_openai) (2.32.5)\n",
      "Collecting zstandard>=0.23.0 (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_openai)\n",
      "  Downloading zstandard-0.25.0-cp314-cp314-win_amd64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: anyio in c:\\users\\samsung\\miniconda3\\envs\\openai\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_openai) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\samsung\\miniconda3\\envs\\openai\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_openai) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\samsung\\miniconda3\\envs\\openai\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_openai) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\samsung\\miniconda3\\envs\\openai\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_openai) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\samsung\\miniconda3\\envs\\openai\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_openai) (0.16.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\samsung\\miniconda3\\envs\\openai\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\users\\samsung\\miniconda3\\envs\\openai\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (0.12.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\samsung\\miniconda3\\envs\\openai\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\samsung\\miniconda3\\envs\\openai\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\samsung\\miniconda3\\envs\\openai\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.1.0->langchain_openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\samsung\\miniconda3\\envs\\openai\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.1.0->langchain_openai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\samsung\\miniconda3\\envs\\openai\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.1.0->langchain_openai) (0.4.2)\n",
      "Collecting regex>=2022.1.18 (from tiktoken<1.0.0,>=0.7.0->langchain_openai)\n",
      "  Downloading regex-2025.11.3-cp314-cp314-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\samsung\\miniconda3\\envs\\openai\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_openai) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\samsung\\miniconda3\\envs\\openai\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_openai) (2.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\samsung\\miniconda3\\envs\\openai\\lib\\site-packages (from tqdm>4->openai<3.0.0,>=1.109.1->langchain_openai) (0.4.6)\n",
      "Using cached langchain_openai-1.1.0-py3-none-any.whl (84 kB)\n",
      "Using cached langchain_core-1.1.1-py3-none-any.whl (475 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langsmith-0.4.56-py3-none-any.whl (411 kB)\n",
      "Using cached tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading tiktoken-0.12.0-cp314-cp314-win_amd64.whl (921 kB)\n",
      "   ---------------------------------------- 0.0/921.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 921.1/921.1 kB 13.3 MB/s  0:00:00\n",
      "Using cached uuid_utils-0.12.0-cp39-abi3-win_amd64.whl (183 kB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading orjson-3.11.5-cp314-cp314-win_amd64.whl (133 kB)\n",
      "Downloading regex-2025.11.3-cp314-cp314-win_amd64.whl (280 kB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading zstandard-0.25.0-cp314-cp314-win_amd64.whl (516 kB)\n",
      "Installing collected packages: zstandard, uuid-utils, tenacity, regex, orjson, jsonpointer, tiktoken, requests-toolbelt, jsonpatch, langsmith, langchain-core, langchain_openai\n",
      "\n",
      "   ----------------------------------------  0/12 [zstandard]\n",
      "   ------ ---------------------------------  2/12 [tenacity]\n",
      "   ------ ---------------------------------  2/12 [tenacity]\n",
      "   ---------- -----------------------------  3/12 [regex]\n",
      "   ------------- --------------------------  4/12 [orjson]\n",
      "   -------------------- -------------------  6/12 [tiktoken]\n",
      "   -------------------- -------------------  6/12 [tiktoken]\n",
      "   ----------------------- ----------------  7/12 [requests-toolbelt]\n",
      "   ----------------------- ----------------  7/12 [requests-toolbelt]\n",
      "   ----------------------- ----------------  7/12 [requests-toolbelt]\n",
      "   ------------------------------ ---------  9/12 [langsmith]\n",
      "   ------------------------------ ---------  9/12 [langsmith]\n",
      "   ------------------------------ ---------  9/12 [langsmith]\n",
      "   ------------------------------ ---------  9/12 [langsmith]\n",
      "   ------------------------------ ---------  9/12 [langsmith]\n",
      "   ------------------------------ ---------  9/12 [langsmith]\n",
      "   ------------------------------ ---------  9/12 [langsmith]\n",
      "   ------------------------------ ---------  9/12 [langsmith]\n",
      "   ------------------------------ ---------  9/12 [langsmith]\n",
      "   --------------------------------- ------ 10/12 [langchain-core]\n",
      "   --------------------------------- ------ 10/12 [langchain-core]\n",
      "   --------------------------------- ------ 10/12 [langchain-core]\n",
      "   --------------------------------- ------ 10/12 [langchain-core]\n",
      "   --------------------------------- ------ 10/12 [langchain-core]\n",
      "   --------------------------------- ------ 10/12 [langchain-core]\n",
      "   --------------------------------- ------ 10/12 [langchain-core]\n",
      "   --------------------------------- ------ 10/12 [langchain-core]\n",
      "   --------------------------------- ------ 10/12 [langchain-core]\n",
      "   --------------------------------- ------ 10/12 [langchain-core]\n",
      "   --------------------------------- ------ 10/12 [langchain-core]\n",
      "   --------------------------------- ------ 10/12 [langchain-core]\n",
      "   --------------------------------- ------ 10/12 [langchain-core]\n",
      "   --------------------------------- ------ 10/12 [langchain-core]\n",
      "   --------------------------------- ------ 10/12 [langchain-core]\n",
      "   --------------------------------- ------ 10/12 [langchain-core]\n",
      "   --------------------------------- ------ 10/12 [langchain-core]\n",
      "   --------------------------------- ------ 10/12 [langchain-core]\n",
      "   --------------------------------- ------ 10/12 [langchain-core]\n",
      "   --------------------------------- ------ 10/12 [langchain-core]\n",
      "   --------------------------------- ------ 10/12 [langchain-core]\n",
      "   --------------------------------- ------ 10/12 [langchain-core]\n",
      "   --------------------------------- ------ 10/12 [langchain-core]\n",
      "   --------------------------------- ------ 10/12 [langchain-core]\n",
      "   --------------------------------- ------ 10/12 [langchain-core]\n",
      "   ------------------------------------ --- 11/12 [langchain_openai]\n",
      "   ------------------------------------ --- 11/12 [langchain_openai]\n",
      "   ---------------------------------------- 12/12 [langchain_openai]\n",
      "\n",
      "Successfully installed jsonpatch-1.33 jsonpointer-3.0.0 langchain-core-1.1.1 langchain_openai-1.1.0 langsmith-0.4.56 orjson-3.11.5 regex-2025.11.3 requests-toolbelt-1.0.0 tenacity-9.1.2 tiktoken-0.12.0 uuid-utils-0.12.0 zstandard-0.25.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1150d443",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
