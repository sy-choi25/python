{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f197bd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SAMSUNG\\miniconda3\\envs\\open\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "벡터 차원: 1024\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# BGE-M3 모델 로드\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-m3\",\n",
    "    model_kwargs={\n",
    "        'device': 'cpu',  # GPU 사용하려면 cuda \n",
    "        'trust_remote_code': True\n",
    "    },\n",
    "    encode_kwargs={\n",
    "        'normalize_embeddings': True,  # 정규화\n",
    "        'batch_size': 32\n",
    "    }\n",
    ")\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunked_documents,        # chunked_documents 청킹된 문서들 변수 이걸로!!!!!!!!!\n",
    "    embedding=embedding_model,\n",
    "    collection_name='basic_rag_collection',\n",
    "    persist_directory='./chroma_db'     # 프로세스 재시작하면 DB 다 날아갈 수 있어서 만들어두는 게 좋음\n",
    ")\n",
    "\n",
    "vectorstore.persist()  # ★ DB 저장\n",
    "\n",
    "# 임베딩 생성\n",
    "vector = embeddings.embed_query(\"한국어 텍스트입니다.\")\n",
    "print(f\"벡터 차원: {len(vector)}\")  # 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb921ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install FlagEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee872cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]c:\\Users\\SAMSUNG\\miniconda3\\envs\\open\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\SAMSUNG\\.cache\\huggingface\\hub\\models--BAAI--bge-m3. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Fetching 30 files: 100%|██████████| 30/30 [01:12<00:00,  2.43s/it]\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output : {'dense_vecs': array([[-0.03027173, -0.0018863 ,  0.01273594, ..., -0.01503609,\n",
      "        -0.03715397,  0.0316697 ]], shape=(1, 1024), dtype=float32), 'lexical_weights': [defaultdict(<class 'int'>, {'193751': np.float32(0.231622), '20945': np.float32(0.11059716), '42431': np.float32(0.19834816), '50260': np.float32(0.06794613), '153924': np.float32(0.25302356)})], 'colbert_vecs': [array([[ 0.03943136, -0.02208258, -0.03825085, ...,  0.04101678,\n",
      "         0.05276498,  0.00512858],\n",
      "       [ 0.03594309, -0.00261484, -0.01896146, ...,  0.04665302,\n",
      "         0.05101896, -0.01214622],\n",
      "       [ 0.05407967,  0.00022846, -0.03076653, ...,  0.01582858,\n",
      "         0.03391199,  0.00870219],\n",
      "       [ 0.01132207, -0.0074869 , -0.05498974, ...,  0.01342221,\n",
      "         0.04152261, -0.0064384 ],\n",
      "       [ 0.03558577, -0.01722921, -0.05326023, ...,  0.03895202,\n",
      "         0.01915203,  0.0194534 ],\n",
      "       [ 0.05479687, -0.01871605, -0.01291045, ...,  0.0283154 ,\n",
      "         0.07281571,  0.0136879 ]], shape=(6, 1024), dtype=float32)]}\n"
     ]
    }
   ],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "\n",
    "model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)\n",
    "\n",
    "# 문장 인코딩\n",
    "output = model.encode(\n",
    "    [\"한국어 임베딩 테스트\"],\n",
    "    return_dense=True,\n",
    "    return_sparse=True,\n",
    "    return_colbert_vecs=True\n",
    ")\n",
    "\n",
    "# 세 가지 출력\n",
    "dense_vecs = output['dense_vecs']      # (1, 1024)\n",
    "sparse_vecs = output['lexical_weights'] # {토큰: 가중치}\n",
    "colbert_vecs = output['colbert_vecs']  # (1, seq_len, 1024)\n",
    "\n",
    "print(f'output : {output}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
