{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b29e58e",
   "metadata": {},
   "source": [
    "<span style=\"color: Gold\"> CRAG\n",
    "\n",
    "<span style=\"font-size:12px;\">\n",
    "\n",
    "- 검색된 문서의 품질을 자체적으로 평가하고, 필요시 **자기 교정(Self-Correction)**을 수행하는 고급 RAG 패턴\n",
    "\n",
    "- 전체 과정\n",
    "\n",
    "```text\n",
    "START\n",
    "  │\n",
    "retrieve (내부 문서 검색)\n",
    "  │\n",
    "grade_documents (문서 관련성 평가 + web_search_needed 결정)\n",
    "  ├──> generate (web_search_needed = \"No\" → 답변 생성)\n",
    "  └──> web_search (web_search_needed = \"Yes\" → 웹 검색 수행)\n",
    "          │\n",
    "       generate (필터링 문서 + 웹 검색 결과 → 답변 생성)\n",
    "          │\n",
    "         END\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7bc8b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs path : c:\\python_src\\6.openAI\n",
      " 11개 청크로 VectorDB 구축 완료\n",
      "조건부 엣지 함수 정의 완료!\n",
      "\n",
      " CRAG StateGraph 구성 및 컴파일 중...\n",
      "\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      " 테스트 1: LangGraph의 핵심 개념을 설명해주세요.\n",
      "   예상 시나리오: 내부 문서에서 답변 가능 → 웹 검색 불필요\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "\n",
      " CRAG 워크플로우 실행 중...\n",
      "   노드 'retrieve' 실행 완료\n",
      "  3개 관련 문서 확보!\n",
      "\n",
      "   [DECISION] 다음 단계 결정 중...\n",
      "   결정: 답변 생성으로 이동\n",
      "   노드 'grade_documents' 실행 완료\n",
      "\n",
      "   [GENERATE 노드] 답변 생성 중...\n",
      "   답변 생성 완료!\n",
      "   노드 'generate' 실행 완료\n",
      "\n",
      " 최종 답변:\n",
      "LangGraph의 핵심 개념은 다음과 같습니다:\n",
      "\n",
      "1. **State(상태)**: 에이전트의 현재 상태를 나타내는 데이터 구조로, 에이전트가 수행하는 작업의 맥락을 제공합니다.\n",
      "\n",
      "2. **Node(노드)**: 실제 작업을 수행하는 함수로, 에이전트가 특정 작업을 실행할 때 사용됩니다.\n",
      "\n",
      "3. **Edge(엣지)**: 노드 간의 제어 흐름을 정의하여, 어떤 노드가 다음에 실행될지를 결정합니다.\n",
      "\n",
      "4. **Conditional Edge**: 특정 조건에 따라 다른 노드로 분기할 수 있는 엣지로, 에이전트의 의사결정 과정을 유연하게 만듭니다.\n",
      "\n",
      "LangGraph는 이러한 개념들을 통해 순환(Cycle)을 지원하며, 복잡한 에이전트 워크플로우를 효과적으로 구현할 수 있습니다.\n",
      "\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      " 테스트 2: CRAG 패턴의 장점은 무엇인가요?\n",
      "   예상 시나리오: 내부 문서에서 답변 가능 → 웹 검색 불필요\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "\n",
      " CRAG 워크플로우 실행 중...\n",
      "   노드 'retrieve' 실행 완료\n",
      "  3개 관련 문서 확보!\n",
      "\n",
      "   [DECISION] 다음 단계 결정 중...\n",
      "   결정: 답변 생성으로 이동\n",
      "   노드 'grade_documents' 실행 완료\n",
      "\n",
      "   [GENERATE 노드] 답변 생성 중...\n",
      "   답변 생성 완료!\n",
      "   노드 'generate' 실행 완료\n",
      "\n",
      " 최종 답변:\n",
      "CRAG 패턴에 대한 구체적인 정보는 제공된 문맥에 포함되어 있지 않습니다. 그러나 CRAG가 RAG와 유사한 검색 증강 생성 기술일 가능성이 있습니다. 일반적으로 이러한 기술의 장점은 다음과 같을 수 있습니다:\n",
      "\n",
      "1. **최신 정보 반영**: CRAG 패턴이 최신 데이터를 활용할 수 있다면, 사용자에게 더 정확하고 신뢰할 수 있는 정보를 제공할 수 있습니다.\n",
      "2. **환각(Hallucination) 감소**: 생성 모델이 실제 데이터를 기반으로 답변을 생성하므로, 잘못된 정보나 비현실적인 답변을 줄일 수 있습니다.\n",
      "3. **출처 명시**: 사용자가 제공받은 정보의 출처를 확인할 수 있어, 정보의 신뢰성을 높일 수 있습니다.\n",
      "\n",
      "CRAG 패턴의 구체적인 장점에 대한 정보가 필요하다면, 추가적인 자료나 문맥이 필요합니다.\n",
      "\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      " 테스트 3: 최신 GPT-5 모델의 특징은 무엇인가요?\n",
      "   예상 시나리오: 내부 문서에 없음 → 웹 검색 필요\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "\n",
      " CRAG 워크플로우 실행 중...\n",
      "   노드 'retrieve' 실행 완료\n",
      "   관련 문서 없음 → 웹 검색 필요!\n",
      "\n",
      "   [DECISION] 다음 단계 결정 중...\n",
      "   결정: 웹 검색으로 이동\n",
      "   노드 'grade_documents' 실행 완료\n",
      "\n",
      "   [WEB SEARCH 노드] 외부 웹 검색 수행 중...\n",
      "   웹 검색 완료! 결과가 문서에 추가됨\n",
      "   노드 'web_search' 실행 완료\n",
      "\n",
      "   [GENERATE 노드] 답변 생성 중...\n",
      "   답변 생성 완료!\n",
      "   노드 'generate' 실행 완료\n",
      "\n",
      " 최종 답변:\n",
      "최신 GPT-5 모델의 특징은 다음과 같습니다:\n",
      "\n",
      "1. **즉석 소프트웨어 생성**: GPT-5는 사용자의 요청에 따라 즉시 소프트웨어를 생성할 수 있는 능력을 갖추고 있습니다.\n",
      "\n",
      "2. **복잡한 과학적 질문 응답**: 이전 세대 모델보다 더 복잡한 과학적 질문에 대한 응답이 가능해졌습니다.\n",
      "\n",
      "3. **모델 선택기 기능**: 챗GPT Plus, Pro 및 Team 사용자는 GPT-5와 GPT-5 싱킹(Thinking) 간에 전환할 수 있는 모델 선택기를 사용할 수 있습니다. 이를 통해 사용자는 필요에 따라 적합한 모델을 선택할 수 있습니다.\n",
      "\n",
      "4. **개성 있는 응답**: 텍스트 기반 응답에 대해 네 가지 새로운 개성 있는 응답 스타일 중에서 선택할 수 있는 기능이 추가되었습니다.\n",
      "\n",
      "5. **사용자 인터페이스 개선**: 대화 말풍선, 음성 버튼, 강조된 텍스트의 모양을 변경할 수 있는 기능이 제공되어 사용자 경험이 향상되었습니다.\n",
      "\n",
      "6. **고급 음성 모드**: 유료 사용자에게는 고급 음성 모드를 거의 무제한으로 사용할 수 있는 옵션이 제공됩니다.\n",
      "\n",
      "7. **구글 서비스 통합**: Gmail 계정과 구글 캘린더를 챗GPT에 쉽게 연결할 수 있는 기능이 추가되어 편리함이 증대되었습니다.\n",
      "\n",
      "이러한 특징들은 GPT-5가 사용자에게 더 나은 경험을 제공하고, 다양한 요구에 맞춰 활용될 수 있도록 돕고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from typing import List, Literal\n",
    "from typing_extensions import TypedDict\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain 관련 임포트\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "# LangGraph 관련 임포트\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# 환경설정\n",
    "load_dotenv()\n",
    "\n",
    "if not os.environ.get('OPENAI_API_KEY'):\n",
    "    raise ValueError('key check....')\n",
    "\n",
    "class CGRAState(TypedDict):\n",
    "    question : str\n",
    "    documents : List[Document]\n",
    "    filtered_documents: List[Document] # 관련성 평가를 통과한 문서\n",
    "    web_search_needed : str   # 웹검색 여부(yes / no)\n",
    "    context : str\n",
    "    answer : str\n",
    "    grade_results : List[str]   #각 문서의 평가 결과\n",
    "\n",
    "# 문서 로드\n",
    "script_dir = os.getcwd()\n",
    "docs_path = os.path.join(script_dir)\n",
    "print(f'docs path : {docs_path}')\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    docs_path,\n",
    "    glob = '**/*.txt',\n",
    "    loader_cls = TextLoader,\n",
    "    loader_kwargs={'encoding':'utf-8'},  # 한국어면 꼭 써줄 것\n",
    ")\n",
    "docs= loader.load()\n",
    "\n",
    "# Step 2 텍스트 분할 (청크)\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300, chunk_overlap = 50\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Step 3 임베딩 및 VectorDB\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name='crag_collection',\n",
    "    embedding=OpenAIEmbeddings(model='text-embedding-3-small')\n",
    ")\n",
    "\n",
    "# Step 4 리트리버 설정 \n",
    "retriever = vectorstore.as_retriever(search_kwargs={'k':3})\n",
    "\n",
    "print(f' {len(doc_splits)}개 청크로 VectorDB 구축 완료')\n",
    "\n",
    "# 문서 관련성 평가를 위한 Grader 정의\n",
    "from pydantic import BaseModel, Field\n",
    "class GradeDocuments(BaseModel):\n",
    "    '''문서 관련성 평가 결과를 위한 pydantic 모델'''\n",
    "    binary_score: str  = Field(description=\"문서가 질문과 관련이 있으면 'yes, 없으면 no\")\n",
    "\n",
    "# llm\n",
    "grader_llm = ChatOpenAI(model = 'gpt-4o-mini',temperature=0)\n",
    "structured_grader =  grader_llm.with_structured_output(GradeDocuments)\n",
    "grade_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system','''당신은 검색된 문서가 사용자의 질문에 답변하는데 관련이 있는지 평가하는 전문가 입니다.\n",
    "     \n",
    "     평가기준:\n",
    "     - 문서가 질문의 키워드나 의미와 연관되어 있다면 '관련있음'으로 평가\n",
    "     - 답변에 도움이 될 가능성이 조금이라도 있다면 '관련있음'\n",
    "     - 와전히 무관한 내용이면 '관련없음'\n",
    "\n",
    "     엄격하게 평가하지 말고, 약간의 연관성이라도 있으면 'yes'를 반환하세요     \n",
    "'''),\n",
    "('human','''질문:{question}\n",
    " \n",
    " 문서내용:\n",
    " {document}\n",
    "\n",
    " 이 문서가 질문과 관련이 있습니까? 'yes' 또는 'no'로만 답하세요\n",
    " ''')\n",
    "])\n",
    "\n",
    "document_grader = grade_prompt | structured_grader\n",
    "\n",
    "def retrieve_node(state:CGRAState) -> dict:\n",
    "    '''내부 문서 검색 노드'''\n",
    "    question = state['question']\n",
    "    documents =  retriever.invoke(question)\n",
    "    return {\n",
    "        'documents':documents,\n",
    "        'question' : question\n",
    "    }\n",
    "\n",
    "def grade_documents_node(state:CGRAState) -> dict:\n",
    "    '''문서관련성 평가 노드\n",
    "    검색된 문서의 관련성 여부를 llm 평가 \n",
    "    관련없으면 웹 검색 플래그를 활성\n",
    "    '''\n",
    "    question = state['question']\n",
    "    documents = state['documents']\n",
    "    filtered_docs, grade_results = [],[]\n",
    "    for i, doc in enumerate(documents,1):\n",
    "        # 각 문서의 관련성 평가\n",
    "        score = document_grader.invoke({\n",
    "            'question' : question,\n",
    "            'document' : doc.page_content\n",
    "        })\n",
    "        grade = score.binary_score.lower()\n",
    "        if grade == 'yes':\n",
    "            filtered_docs.append(doc)\n",
    "            grade_results.append(\"relevant\")\n",
    "        else:\n",
    "            grade_results.append(\"not_relevant\")\n",
    "     # 관련 문서가 없으면 웹 검색 필요\n",
    "    if len(filtered_docs) == 0:\n",
    "        web_search_needed = \"Yes\"\n",
    "        print(\"   관련 문서 없음 → 웹 검색 필요!\")\n",
    "    else:\n",
    "        web_search_needed = \"No\"\n",
    "        print(f\"  {len(filtered_docs)}개 관련 문서 확보!\")\n",
    "    \n",
    "    return {\n",
    "        \"filtered_documents\": filtered_docs,\n",
    "        \"web_search_needed\": web_search_needed,\n",
    "        \"grade_results\": grade_results\n",
    "    }\n",
    "\n",
    "def web_search_node(state: CGRAState) -> dict:\n",
    "    \"\"\"\n",
    "    웹 검색 노드 (시뮬레이션)\n",
    "    \n",
    "    실제 환경에서는 Tavily API나 다른 검색 API를 사용합니다.\n",
    "    여기서는 학습 목적으로 시뮬레이션합니다.\n",
    "    \"\"\"\n",
    "    print(\"\\n   [WEB SEARCH 노드] 외부 웹 검색 수행 중...\")\n",
    "    \n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    # 웹 검색 시뮬레이션 (실제로는 Tavily API 등 사용)\n",
    "    # 실제 구현 예시:\n",
    "    from langchain_community.retrievers import TavilySearchAPIRetriever\n",
    "    web_search = TavilySearchAPIRetriever(k=3)\n",
    "    web_results = web_search.invoke(question)\n",
    "    \n",
    "    # 시뮬레이션된 웹 검색 결과\n",
    "    simulated_web_results = f\"\"\"\n",
    "    [웹 검색 결과 - 시뮬레이션]\n",
    "    \n",
    "    질문 '{question}'에 대한 웹 검색 결과:\n",
    "    \n",
    "    1. LLM(Large Language Model) 관련 최신 정보:\n",
    "       - LLM은 자연어 처리에서 혁신적인 발전을 이루고 있습니다.\n",
    "       - OpenAI, Anthropic, Google 등이 주요 제공자입니다.\n",
    "       - RAG, Fine-tuning, Prompt Engineering이 주요 활용 기법입니다.\n",
    "    \n",
    "    2. AI 에이전트 트렌드:\n",
    "       - 자율적인 AI 에이전트가 주목받고 있습니다.\n",
    "       - LangGraph, AutoGPT 등이 대표적인 프레임워크입니다.\n",
    "       - 멀티 에이전트 시스템이 복잡한 작업을 수행합니다.\n",
    "    \n",
    "    출처: 시뮬레이션된 웹 검색 (실제 환경에서는 Tavily API 사용)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 웹 검색 결과를 Document 형태로 변환\n",
    "    web_doc = Document(\n",
    "        page_content=simulated_web_results,\n",
    "        metadata={\"source\": \"web_search\", \"type\": \"external\"}\n",
    "    )\n",
    "    \n",
    "    # 기존 필터링된 문서에 웹 검색 결과 추가\n",
    "    filtered_docs = state.get(\"filtered_documents\", [])\n",
    "    # filtered_docs.append(web_doc)  # 시뮬레이션은 web_doc 사용\n",
    "    # api 사용\n",
    "    for doc in web_results:        \n",
    "        filtered_docs.append(doc)\n",
    "\n",
    "    \n",
    "    print(\"   웹 검색 완료! 결과가 문서에 추가됨\")\n",
    "    \n",
    "    return {\n",
    "        \"filtered_documents\": filtered_docs\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_node(state: CGRAState) -> dict:\n",
    "    \"\"\"\n",
    "    답변 생성 노드\n",
    "    필터링된 문서(내부 문서 + 웹 검색 결과)를 바탕으로 답변을 생성합니다.\n",
    "    \"\"\"\n",
    "    print(\"\\n   [GENERATE 노드] 답변 생성 중...\")\n",
    "    \n",
    "    question = state[\"question\"]\n",
    "    filtered_documents = state['filtered_documents']\n",
    "    \n",
    "    # 컨텍스트 구성\n",
    "    context = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in filtered_documents])\n",
    "    \n",
    "    # 답변 생성 LLM\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"당신은 제공된 문맥을 바탕으로 질문에 답변하는 AI 어시스턴트입니다.\n",
    "\n",
    "규칙:\n",
    "1. 제공된 문맥 내의 정보를 우선적으로 사용하세요.\n",
    "2. 답변은 한국어로 명확하고 구조화되게 작성하세요.\n",
    "3. 웹 검색 결과가 포함된 경우, 해당 정보도 적절히 활용하세요.\n",
    "4. 확실하지 않은 정보는 추측하지 마세요.\"\"\"),\n",
    "        (\"human\", \"\"\"문맥(Context):\n",
    "{context}\n",
    "\n",
    "질문: {question}\n",
    "\n",
    "답변:\"\"\")\n",
    "    ])\n",
    "    \n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    answer = chain.invoke({\"context\": context, \"question\": question})\n",
    "    \n",
    "    print(\"   답변 생성 완료!\")\n",
    "    \n",
    "    return {\n",
    "        \"context\": context,\n",
    "        \"answer\": answer\n",
    "    }\n",
    "\n",
    "\n",
    "# 조건부 엣지 함수 정의\n",
    "\n",
    "def decide_to_generate(state: CGRAState) -> Literal[\"generate\", \"web_search\"]:\n",
    "    \"\"\"\n",
    "    문서 평가 결과에 따라 다음 단계를 결정합니다.\n",
    "    \n",
    "    - 관련 문서가 있으면 → generate (답변 생성)\n",
    "    - 관련 문서가 없으면 → web_search (웹 검색)\n",
    "    \n",
    "    Returns:\n",
    "        \"generate\" 또는 \"web_search\"\n",
    "    \"\"\"\n",
    "    print(\"\\n   [DECISION] 다음 단계 결정 중...\")\n",
    "    \n",
    "    web_search_needed = state[\"web_search_needed\"]\n",
    "    \n",
    "    if web_search_needed == \"Yes\":\n",
    "        print(\"   결정: 웹 검색으로 이동\")\n",
    "        return \"web_search\"\n",
    "    else:\n",
    "        print(\"   결정: 답변 생성으로 이동\")\n",
    "        return \"generate\"\n",
    "\n",
    "\n",
    "print(\"조건부 엣지 함수 정의 완료!\")\n",
    "\n",
    "print(\"\\n CRAG StateGraph 구성 및 컴파일 중...\")\n",
    "\n",
    "# StateGraph 생성\n",
    "workflow = StateGraph(CGRAState)\n",
    "\n",
    "# 노드 추가\n",
    "workflow.add_node(\"retrieve\", retrieve_node)\n",
    "workflow.add_node(\"grade_documents\", grade_documents_node)\n",
    "workflow.add_node(\"web_search\", web_search_node)\n",
    "workflow.add_node(\"generate\", generate_node)\n",
    "\n",
    "# 엣지 추가\n",
    "# START -> retrieve -> grade_documents\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "\n",
    "# 조건부 엣지: grade_documents 이후 분기\n",
    "# - 관련 문서 있음 → generate\n",
    "# - 관련 문서 없음 → web_search\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",      # 시작 노드\n",
    "    decide_to_generate,     # 조건 함수\n",
    "    {\n",
    "        \"generate\": \"generate\",      # \"generate\" 반환 시\n",
    "        \"web_search\": \"web_search\"   # \"web_search\" 반환 시\n",
    "    }\n",
    ")\n",
    "\n",
    "# web_search 이후 generate로 이동\n",
    "workflow.add_edge(\"web_search\", \"generate\")\n",
    "\n",
    "# generate 이후 종료\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "# 그래프 컴파일\n",
    "app = workflow.compile()\n",
    "\n",
    "# 테스트 시나리오\n",
    "test_cases = [\n",
    "    {\n",
    "        \"question\": \"LangGraph의 핵심 개념을 설명해주세요.\",\n",
    "        \"expected\": \"내부 문서에서 답변 가능 → 웹 검색 불필요\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"CRAG 패턴의 장점은 무엇인가요?\",\n",
    "        \"expected\": \"내부 문서에서 답변 가능 → 웹 검색 불필요\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"최신 GPT-5 모델의 특징은 무엇인가요?\",\n",
    "        \"expected\": \"내부 문서에 없음 → 웹 검색 필요\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, test in enumerate(test_cases, 1):\n",
    "    print(f\"\\n{'━' * 70}\")\n",
    "    print(f\" 테스트 {i}: {test['question']}\")\n",
    "    print(f\"   예상 시나리오: {test['expected']}\")\n",
    "    print(f\"{'━' * 70}\")\n",
    "    \n",
    "    # 초기 상태\n",
    "    initial_state = {\n",
    "        \"question\": test[\"question\"],\n",
    "        \"documents\": [],\n",
    "        \"filtered_documents\": [],\n",
    "        \"web_search_needed\": \"No\",\n",
    "        \"context\": \"\",\n",
    "        \"answer\": \"\",\n",
    "        \"grade_results\": []\n",
    "    }\n",
    "    \n",
    "    # 그래프 실행\n",
    "    print(\"\\n CRAG 워크플로우 실행 중...\")\n",
    "    \n",
    "    final_state = None\n",
    "    for output in app.stream(initial_state):\n",
    "        for node_name, node_output in output.items():\n",
    "            print(f\"   노드 '{node_name}' 실행 완료\")\n",
    "        final_state = output\n",
    "    \n",
    "    # 결과 출력\n",
    "    if \"generate\" in final_state:\n",
    "        answer = final_state[\"generate\"][\"answer\"]\n",
    "    else:\n",
    "        answer = \"답변을 생성할 수 없습니다.\"\n",
    "    \n",
    "    print(f\"\\n 최종 답변:\\n{answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
