{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a23066e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from docling.document_converter import DocumentConverter\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from typing import List,Literal\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b66850e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SAMSUNG\\miniconda3\\envs\\open\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-12-02 14:20:25,593 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-02 14:20:26,076 - INFO - Going to convert document batch...\n",
      "2025-12-02 14:20:26,078 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 44ae89a68fc272bc7889292e9b5a1bad\n",
      "2025-12-02 14:20:26,337 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-02 14:20:26,347 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2025-12-02 14:20:26,585 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-02 14:20:26,611 - INFO - Registered ocr engines: ['auto', 'easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "2025-12-02 14:20:27,454 - INFO - Accelerator device: 'cpu'\n",
      "\u001b[32m[INFO] 2025-12-02 14:20:27,502 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-02 14:20:27,567 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\SAMSUNG\\miniconda3\\envs\\open\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-02 14:20:27,571 [RapidOCR] main.py:53: Using C:\\Users\\SAMSUNG\\miniconda3\\envs\\open\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-02 14:20:27,748 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-02 14:20:27,755 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\SAMSUNG\\miniconda3\\envs\\open\\Lib\\site-packages\\rapidocr\\models\\ch_ppocr_mobile_v2.0_cls_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-02 14:20:27,757 [RapidOCR] main.py:53: Using C:\\Users\\SAMSUNG\\miniconda3\\envs\\open\\Lib\\site-packages\\rapidocr\\models\\ch_ppocr_mobile_v2.0_cls_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-02 14:20:27,845 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-02 14:20:27,882 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\SAMSUNG\\miniconda3\\envs\\open\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-02 14:20:27,883 [RapidOCR] main.py:53: Using C:\\Users\\SAMSUNG\\miniconda3\\envs\\open\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.onnx\u001b[0m\n",
      "2025-12-02 14:20:28,093 - INFO - Auto OCR model selected rapidocr with onnxruntime.\n",
      "2025-12-02 14:20:28,137 - INFO - Accelerator device: 'cpu'\n",
      "c:\\Users\\SAMSUNG\\miniconda3\\envs\\open\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\SAMSUNG\\.cache\\huggingface\\hub\\models--docling-project--docling-models. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "2025-12-02 14:20:29,745 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-02 14:20:30,624 - INFO - Processing document 2408.09869v5.pdf\n",
      "\u001b[32m[INFO] 2025-12-02 14:21:54,894 [RapidOCR] download_file.py:68: Initiating download: https://www.modelscope.cn/models/RapidAI/RapidOCR/resolve/v3.4.0/resources/fonts/FZYTK.TTF\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-02 14:21:56,771 [RapidOCR] download_file.py:82: Download size: 3.09MB\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-02 14:21:56,922 [RapidOCR] download_file.py:95: Successfully saved to: C:\\Users\\SAMSUNG\\miniconda3\\envs\\open\\Lib\\site-packages\\rapidocr\\models\\FZYTK.TTF\u001b[0m\n",
      "2025-12-02 14:22:54,100 - INFO - Finished converting document 2408.09869v5.pdf in 148.88 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!-- image -->\n",
      "\n",
      "## Docling Technical Report\n",
      "\n",
      "## Version 1.0\n",
      "\n",
      "Christoph Auer Maksym Lysak Ahmed Nassar Michele Dolfi Nikolaos Livathinos Panos Vagenas Cesar Berrospi Ramis Matteo Omenetti Fabian Lindlbauer Kasper Dinkla Lokesh Mishra Yusik Kim Shubham Gupta Rafael Teixeira de Lima Valery Weber Lucas Morin Ingmar Meijer Viktor Kuropiatnyk Peter W. J. Staar\n",
      "\n",
      "AI4K Group, IBM Research R¨ uschlikon, Switzerland\n",
      "\n",
      "## Abstract\n",
      "\n",
      "This technical report introduces Docling , an easy to use, self-contained, MITlicensed open-source package for PDF document conversion. It is powered by state-of-the-art specialized AI models for layout analysis (DocLayNet) and table structure recognition (TableFormer), and runs efficiently on commodity hardware in a small resource budget. The code interface allows for easy extensibility and addition of new features and models.\n",
      "\n",
      "## 1 Introduction\n",
      "\n",
      "Converting PDF documents back into a machine-processable format has been a major challenge for decades due to their huge variability in formats, weak standardization and printing-optimized characteristic, which discards most structural features and metadata. With the advent of LLMs and popular application patterns such as retrieval-augmented generation (RAG), leveraging the rich content embedded in PDFs has become ever more relevant. In the past decade, several powerful document understanding solutions have emerged on the market, most of which are commercial software, cloud offerings [3] and most recently, multi-modal vision-language models. As of today, only a handful of open-source tools cover PDF conversion, leaving a significant feature and quality gap to proprietary solutions.\n",
      "\n",
      "With Docling , we open-source a very capable and efficient document conversion tool which builds on the powerful, specialized AI models and datasets for layout analysis and table structure recognition we developed and presented in the recent past [12, 13, 9]. Docling is designed as a simple, self-contained python library with permissive license, running entirely locally on commodity hardware. Its code architecture allows for easy extensibility and addition of new features and models.\n",
      "\n",
      "Here is what Docling delivers today:\n",
      "\n",
      "- Converts PDF documents to JSON or Markdown format, stable and lightning fast\n",
      "- Understands detailed page layout, reading order, locates figures and recovers table structures\n",
      "- Extracts metadata from the document, such as title, authors, references and language\n",
      "- Optionally applies OCR, e.g. for scanned PDFs\n",
      "- Can be configured to be optimal for batch-mode (i.e high throughput, low time-to-solution) or interactive mode (compromise on efficiency, low time-to-solution)\n",
      "- Can leverage different accelerators (GPU, MPS, etc).\n",
      "\n",
      "## 2 Getting Started\n",
      "\n",
      "To use Docling, you can simply install the docling package from PyPI. Documentation and examples are available in our GitHub repository at github.com/DS4SD/docling. All required model assets 1 are downloaded to a local huggingface datasets cache on first use, unless you choose to pre-install the model assets in advance.\n",
      "\n",
      "Docling provides an easy code interface to convert PDF documents from file system, URLs or binary streams, and retrieve the output in either JSON or Markdown format. For convenience, separate methods are offered to convert single documents or batches of documents. A basic usage example is illustrated below. Further examples are available in the Doclign code repository.\n",
      "\n",
      "from docling.document\\_converter import DocumentConverter\n",
      "\n",
      "```\n",
      "source = \"https://arxiv.org/pdf/2206.01062\" # PDF path or URL converter = DocumentConverter() result = converter.convert_single(source) print(result.render_as_markdown()) # output: \"## DocLayNet: A Large Human -Annotated Dataset for Document -Layout Analysis [...]\"\n",
      "```\n",
      "\n",
      "Optionally, you can configure custom pipeline features and runtime options, such as turning on or off features (e.g. OCR, table structure recognition), enforcing limits on the input document size, and defining the budget of CPU threads. Advanced usage examples and options are documented in the README file. Docling also provides a Dockerfile to demonstrate how to install and run it inside a container.\n",
      "\n",
      "## 3 Processing pipeline\n",
      "\n",
      "Docling implements a linear pipeline of operations, which execute sequentially on each given document (see Fig. 1). Each document is first parsed by a PDF backend, which retrieves the programmatic text tokens, consisting of string content and its coordinates on the page, and also renders a bitmap image of each page to support downstream operations. Then, the standard model pipeline applies a sequence of AI models independently on every page in the document to extract features and content, such as layout and table structures. Finally, the results from all pages are aggregated and passed through a post-processing stage, which augments metadata, detects the document language, infers reading-order and eventually assembles a typed document object which can be serialized to JSON or Markdown.\n",
      "\n",
      "## 3.1 PDF backends\n",
      "\n",
      "Two basic requirements to process PDF documents in our pipeline are a) to retrieve all text content and their geometric coordinates on each page and b) to render the visual representation of each page as it would appear in a PDF viewer. Both these requirements are encapsulated in Docling's PDF backend interface. While there are several open-source PDF parsing libraries available for python, we faced major obstacles with all of them for different reasons, among which were restrictive\n",
      "\n",
      "1 see huggingface.co/ds4sd/docling-models/\n",
      "\n",
      "Figure 1: Sketch of Docling's default processing pipeline. The inner part of the model pipeline is easily customizable and extensible.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "licensing (e.g. pymupdf [7]), poor speed or unrecoverable quality issues, such as merged text cells across far-apart text tokens or table columns (pypdfium, PyPDF) [15, 14].\n",
      "\n",
      "We therefore decided to provide multiple backend choices, and additionally open-source a custombuilt PDF parser, which is based on the low-level qpdf [4] library. It is made available in a separate package named docling-parse and powers the default PDF backend in Docling. As an alternative, we provide a PDF backend relying on pypdfium , which may be a safe backup choice in certain cases, e.g. if issues are seen with particular font encodings.\n",
      "\n",
      "## 3.2 AI models\n",
      "\n",
      "As part of Docling, we initially release two highly capable AI models to the open-source community, which have been developed and published recently by our team. The first model is a layout analysis model, an accurate object-detector for page elements [13]. The second model is TableFormer [12, 9], a state-of-the-art table structure recognition model. We provide the pre-trained weights (hosted on huggingface) and a separate package for the inference code as docling-ibm-models . Both models are also powering the open-access deepsearch-experience, our cloud-native service for knowledge exploration tasks.\n",
      "\n",
      "## Layout Analysis Model\n",
      "\n",
      "Our layout analysis model is an object-detector which predicts the bounding-boxes and classes of various elements on the image of a given page. Its architecture is derived from RT-DETR [16] and re-trained on DocLayNet [13], our popular human-annotated dataset for document-layout analysis, among other proprietary datasets. For inference, our implementation relies on the onnxruntime [5].\n",
      "\n",
      "The Docling pipeline feeds page images at 72 dpi resolution, which can be processed on a single CPU with sub-second latency. All predicted bounding-box proposals for document elements are post-processed to remove overlapping proposals based on confidence and size, and then intersected with the text tokens in the PDF to group them into meaningful and complete units such as paragraphs, section titles, list items, captions, figures or tables.\n",
      "\n",
      "## Table Structure Recognition\n",
      "\n",
      "The TableFormer model [12], first published in 2022 and since refined with a custom structure token language [9], is a vision-transformer model for table structure recovery. It can predict the logical row and column structure of a given table based on an input image, and determine which table cells belong to column headers, row headers or the table body. Compared to earlier approaches, TableFormer handles many characteristics of tables, such as partial or no borderlines, empty cells, rows or columns, cell spans and hierarchy both on column-heading or row-heading level, tables with inconsistent indentation or alignment and other complexities. For inference, our implementation relies on PyTorch [2].\n",
      "\n",
      "The Docling pipeline feeds all table objects detected in the layout analysis to the TableFormer model, by providing an image-crop of the table and the included text cells. TableFormer structure predictions are matched back to the PDF cells in post-processing to avoid expensive re-transcription text in the table image. Typical tables require between 2 and 6 seconds to be processed on a standard CPU, strongly depending on the amount of included table cells.\n",
      "\n",
      "## OCR\n",
      "\n",
      "Docling provides optional support for OCR, for example to cover scanned PDFs or content in bitmaps images embedded on a page. In our initial release, we rely on EasyOCR [1], a popular thirdparty OCR library with support for many languages. Docling, by default, feeds a high-resolution page image (216 dpi) to the OCR engine, to allow capturing small print detail in decent quality. While EasyOCR delivers reasonable transcription quality, we observe that it runs fairly slow on CPU (upwards of 30 seconds per page).\n",
      "\n",
      "We are actively seeking collaboration from the open-source community to extend Docling with additional OCR backends and speed improvements.\n",
      "\n",
      "## 3.3 Assembly\n",
      "\n",
      "In the final pipeline stage, Docling assembles all prediction results produced on each page into a well-defined datatype that encapsulates a converted document, as defined in the auxiliary package docling-core . The generated document object is passed through a post-processing model which leverages several algorithms to augment features, such as detection of the document language, correcting the reading order, matching figures with captions and labelling metadata such as title, authors and references. The final output can then be serialized to JSON or transformed into a Markdown representation at the users request.\n",
      "\n",
      "## 3.4 Extensibility\n",
      "\n",
      "Docling provides a straight-forward interface to extend its capabilities, namely the model pipeline. A model pipeline constitutes the central part in the processing, following initial document parsing and preceding output assembly, and can be fully customized by sub-classing from an abstract baseclass ( BaseModelPipeline ) or cloning the default model pipeline. This effectively allows to fully customize the chain of models, add or replace models, and introduce additional pipeline configuration parameters. To use a custom model pipeline, the custom pipeline class to instantiate can be provided as an argument to the main document conversion methods. We invite everyone in the community to propose additional or alternative models and improvements.\n",
      "\n",
      "Implementations of model classes must satisfy the python Callable interface. The \\_\\_call\\_\\_ method must accept an iterator over page objects, and produce another iterator over the page objects which were augmented with the additional features predicted by the model, by extending the provided PagePredictions data model accordingly.\n",
      "\n",
      "## 4 Performance\n",
      "\n",
      "In this section, we establish some reference numbers for the processing speed of Docling and the resource budget it requires. All tests in this section are run with default options on our standard test set distributed with Docling, which consists of three papers from arXiv and two IBM Redbooks, with a total of 225 pages. Measurements were taken using both available PDF backends on two different hardware systems: one MacBook Pro M3 Max, and one bare-metal server running Ubuntu 20.04 LTS on an Intel Xeon E5-2690 CPU. For reproducibility, we fixed the thread budget (through setting OMP NUM THREADS environment variable ) once to 4 (Docling default) and once to 16 (equal to full core count on the test hardware). All results are shown in Table 1.\n",
      "\n",
      "If you need to run Docling in very low-resource environments, please consider configuring the pypdfium backend. While it is faster and more memory efficient than the default docling-parse backend, it will come at the expense of worse quality results, especially in table structure recovery.\n",
      "\n",
      "Establishing GPU acceleration support for the AI models is currently work-in-progress and largely untested, but may work implicitly when CUDA is available and discovered by the onnxruntime and\n",
      "\n",
      "torch runtimes backing the Docling pipeline. We will deliver updates on this topic at in a future version of this report.\n",
      "\n",
      "Table 1: Runtime characteristics of Docling with the standard model pipeline and settings, on our test dataset of 225 pages, on two different systems. OCR is disabled. We show the time-to-solution (TTS), computed throughput in pages per second, and the peak memory used (resident set size) for both the Docling-native PDF backend and for the pypdfium backend, using 4 and 16 threads.\n",
      "\n",
      "| CPU                     | Thread budget   | native backend   | native backend   | native backend   | pypdfium backend   | pypdfium backend   | pypdfium backend   |\n",
      "|-------------------------|-----------------|------------------|------------------|------------------|--------------------|--------------------|--------------------|\n",
      "|                         |                 | TTS              | Pages/s          | Mem              | TTS                | Pages/s            | Mem                |\n",
      "| Apple M3 Max (16 cores) | 4 16            | 177 s 167 s      | 1.27 1.34        | 6.20 GB          | 103 s 92 s         | 2.18 2.45          | 2.56 GB            |\n",
      "| Intel(R) Xeon E5-2690   | 4 16            | 375 s 244 s      | 0.60 0.92        | 6.16 GB          | 239 s 143 s        | 0.94 1.57          | 2.42 GB            |\n",
      "\n",
      "## 5 Applications\n",
      "\n",
      "Thanks to the high-quality, richly structured document conversion achieved by Docling, its output qualifies for numerous downstream applications. For example, Docling can provide a base for detailed enterprise document search, passage retrieval or classification use-cases, or support knowledge extraction pipelines, allowing specific treatment of different structures in the document, such as tables, figures, section structure or references. For popular generative AI application patterns, such as retrieval-augmented generation (RAG), we provide quackling , an open-source package which capitalizes on Docling's feature-rich document output to enable document-native optimized vector embedding and chunking. It plugs in seamlessly with LLM frameworks such as LlamaIndex [8]. Since Docling is fast, stable and cheap to run, it also makes for an excellent choice to build document-derived datasets. With its powerful table structure recognition, it provides significant benefit to automated knowledge-base construction [11, 10]. Docling is also integrated within the open IBM data prep kit [6], which implements scalable data transforms to build large-scale multi-modal training datasets.\n",
      "\n",
      "## 6 Future work and contributions\n",
      "\n",
      "Docling is designed to allow easy extension of the model library and pipelines. In the future, we plan to extend Docling with several more models, such as a figure-classifier model, an equationrecognition model, a code-recognition model and more. This will help improve the quality of conversion for specific types of content, as well as augment extracted document metadata with additional information. Further investment into testing and optimizing GPU acceleration as well as improving the Docling-native PDF backend are on our roadmap, too.\n",
      "\n",
      "We encourage everyone to propose or implement additional features and models, and will gladly take your inputs and contributions under review . The codebase of Docling is open for use and contribution, under the MIT license agreement and in alignment with our contributing guidelines included in the Docling repository. If you use Docling in your projects, please consider citing this technical report.\n",
      "\n",
      "## References\n",
      "\n",
      "- [1] J. AI. Easyocr: Ready-to-use ocr with 80+ supported languages. https://github.com/ JaidedAI/EasyOCR , 2024. Version: 1.7.0.\n",
      "- [2] J. Ansel, E. Yang, H. He, N. Gimelshein, A. Jain, M. Voznesensky, B. Bao, P. Bell, D. Berard, E. Burovski, G. Chauhan, A. Chourdia, W. Constable, A. Desmaison, Z. DeVito, E. Ellison, W. Feng, J. Gong, M. Gschwind, B. Hirsh, S. Huang, K. Kalambarkar, L. Kirsch, M. Lazos, M. Lezcano, Y. Liang, J. Liang, Y. Lu, C. Luk, B. Maher, Y. Pan, C. Puhrsch, M. Reso, M. Saroufim, M. Y. Siraichi, H. Suk, M. Suo, P. Tillet, E. Wang, X. Wang, W. Wen, S. Zhang, X. Zhao, K. Zhou, R. Zou, A. Mathews, G. Chanan, P. Wu, and S. Chintala. Pytorch 2: Faster\n",
      "\n",
      "machine learning through dynamic python bytecode transformation and graph compilation. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS '24) . ACM, 4 2024. doi: 10.1145/3620665.3640366. URL https://pytorch.org/assets/pytorch2-2.pdf .\n",
      "\n",
      "- [3] C. Auer, M. Dolfi, A. Carvalho, C. B. Ramis, and P. W. Staar. Delivering document conversion as a cloud service with high throughput and responsiveness. In 2022 IEEE 15th International Conference on Cloud Computing (CLOUD) , pages 363-373. IEEE, 2022.\n",
      "- [4] J. Berkenbilt. Qpdf: A content-preserving pdf document transformer, 2024. URL https: //github.com/qpdf/qpdf .\n",
      "- [5] O. R. developers. Onnx runtime. https://onnxruntime.ai/ , 2024. Version: 1.18.1.\n",
      "- [6] IBM. Data Prep Kit: a community project to democratize and accelerate unstructured data preparation for LLM app developers, 2024. URL https://github.com/IBM/ data-prep-kit .\n",
      "- [7] A. S. Inc. PyMuPDF, 2024. URL https://github.com/pymupdf/PyMuPDF .\n",
      "- [8] J. Liu. LlamaIndex, 11 2022. URL https://github.com/jerryjliu/llama\\_index .\n",
      "- [9] M. Lysak, A. Nassar, N. Livathinos, C. Auer, and P. Staar. Optimized Table Tokenization for Table Structure Recognition. In Document Analysis and Recognition - ICDAR 2023: 17th International Conference, San Jos´ e, CA, USA, August 21-26, 2023, Proceedings, Part II , pages 37-50, Berlin, Heidelberg, Aug. 2023. Springer-Verlag. ISBN 978-3-031-41678-1. doi: 10. 1007/978-3-031-41679-8 3. URL https://doi.org/10.1007/978-3-031-41679-8\\_3 .\n",
      "- [10] L. Mishra, S. Dhibi, Y. Kim, C. Berrospi Ramis, S. Gupta, M. Dolfi, and P. Staar. Statements: Universal information extraction from tables with large language models for ESG KPIs. In D. Stammbach, J. Ni, T. Schimanski, K. Dutia, A. Singh, J. Bingler, C. Christiaen, N. Kushwaha, V. Muccione, S. A. Vaghefi, and M. Leippold, editors, Proceedings of the 1st Workshop on Natural Language Processing Meets Climate Change (ClimateNLP 2024) , pages 193-214, Bangkok, Thailand, Aug. 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.climatenlp-1.15 .\n",
      "- [11] L. Morin, V. Weber, G. I. Meijer, F. Yu, and P. W. J. Staar. Patcid: an open-access dataset of chemical structures in patent documents. Nature Communications , 15(1):6532, August 2024. ISSN 2041-1723. doi: 10.1038/s41467-024-50779-y. URL https://doi.org/10.1038/ s41467-024-50779-y .\n",
      "- [12] A. Nassar, N. Livathinos, M. Lysak, and P. Staar. Tableformer: Table structure understanding with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4614-4623, 2022.\n",
      "- [13] B. Pfitzmann, C. Auer, M. Dolfi, A. S. Nassar, and P. Staar. Doclaynet: a large humanannotated dataset for document-layout segmentation. pages 3743-3751, 2022.\n",
      "- [14] pypdf Maintainers. pypdf: A Pure-Python PDF Library, 2024. URL https://github.com/ py-pdf/pypdf .\n",
      "- [15] P. Team. PyPDFium2: Python bindings for PDFium, 2024. URL https://github.com/ pypdfium2-team/pypdfium2 .\n",
      "- [16] Y. Zhao, W. Lv, S. Xu, J. Wei, G. Wang, Q. Dang, Y. Liu, and J. Chen. Detrs beat yolos on real-time object detection, 2023.\n",
      "\n",
      "## Appendix\n",
      "\n",
      "In this section, we illustrate a few examples of Docling's output in Markdown and JSON.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "1 INTRODUCTION\n",
      "\n",
      "Despite the substantial improvements achieved with machine-learning (ML) approaches and deep neural networks in recent years, document conversion remains a challenging problem, as demonstrated by the numerous public competitions held on this topic [1-4]. The challenge originates from the huge variability in PDF documents regarding layout, language and formats (scanned, programmatic or a combination of both). Engineering a single ML model that can be applied on all types of documents and provides high-quality layout segmentation remains to this day extremely challenging [5]. To highlight the variability in document layouts, we show a few example documents from the DocLayNet dataset in Figure 1. Figure 2: Title page of the DocLayNet paper (arxiv.org/pdf/2206.01062) - left PDF, right rendered Markdown. If recognized, metadata such as authors are appearing first under the title. Text content inside figures is currently dropped, the caption is retained and linked to the figure in the JSON representation (not shown).\n",
      "\n",
      "KDD '22, August 14-18, 2022, Washington, DC, USA Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar\n",
      "\n",
      "Table 2: Prediction performance (mAP@0.5-0.95) of object detection networks on DocLayNet test set. The MRCNN (Mask R-CNN) and FRCNN (Faster R-CNN) models with ResNet-50 or ResNet-101 backbone were trained based on the network architectures from the detectron2 model zoo (Mask R-CNN R50, R101-FPN 3x, Faster R-CNN R101-FPN 3x), with default configurations. The YOLO implementation utilized was YOLOv5x6 [13]. All models were initialised using pre-trained weights from the COCO 2017 dataset.\n",
      "\n",
      "|                                                                                                        | human                                                                   | MRCNN R50 R101                                                                                                          | FRCNN R101                                                  | YOLO v5x6                                                   |\n",
      "|--------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------|-------------------------------------------------------------|\n",
      "| Caption Footnote Formula List-item Page-footer Page-header Picture Section-header Table Text Title All | 84-89 83-91 83-85 87-88 93-94 85-89 69-71 83-84 77-81 84-86 60-72 82-83 | 68.4 71.5 70.9 71.8 60.1 63.4 81.2 80.8 61.6 59.3 71.9 70.0 71.7 72.7 67.6 69.3 82.2 82.9 84.6 85.8 76.7 80.4 72.4 73.5 | 70.1 73.7 63.5 81.0 58.9 72.0 72.0 68.4 82.2 85.4 79.9 73.4 | 77.7 77.2 66.2 86.2 61.1 67.9 77.1 74.6 86.3 88.1 82.7 76.8 |\n",
      "\n",
      "to avoid this at any cost in order to have clear, unbiased baseline numbers for human document-layout annotation. Third, we introduced the feature of snapping boxes around text segments to obtain a pixel-accurate annotation and again reduce time and effort. The CCS annotation tool automatically shrinks every user-drawn box to the minimum bounding-box around the enclosed text-cells for all purely text-based segments, which excludes only Table and Picture . For the latter, we instructed annotation staff to minimise inclusion of surrounding whitespace while including all graphical lines. A downside of snapping boxes to enclosed text cells is that some wrongly parsed PDF pages cannot be annotated correctly and need to be skipped. Fourth, we established a way to flag pages as rejected for cases where no valid annotation according to the label guidelines could be achieved. Example cases for this would be PDF pages that render incorrectly or contain layouts that are impossible to capture with non-overlapping rectangles. Such rejected pages are not contained in the final dataset. With all these measures in place, experienced annotation staff managed to annotate a single page in a typical timeframe of 20s to 60s, depending on its complexity.\n",
      "\n",
      "## 5 EXPERIMENTS\n",
      "\n",
      "The primary goal of DocLayNet is to obtain high-quality ML models capable of accurate document-layout analysis on a wide variety of challenging layouts. As discussed in Section 2, object detection models are currently the easiest to use, due to the standardisation of ground-truth data in COCO format [16] and the availability of general frameworks such as detectron2 [17]. Furthermore, baseline numbers in PubLayNet and DocBank were obtained using standard object detection models such as Mask R-CNN and Faster R-CNN. As such, we will relate to these object detection methods in this\n",
      "\n",
      "Figure 5: Prediction performance (mAP@0.5-0.95) of a Mask R-CNNnetworkwithResNet50backbonetrainedonincreasing fractions of the DocLayNet dataset. The learning curve flattens around the 80% mark, indicating that increasing the size of the DocLayNet dataset with similar data will not yield significantly better predictions.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "paper and leave the detailed evaluation of more recent methods mentioned in Section 2 for future work.\n",
      "\n",
      "In this section, we will present several aspects related to the performance of object detection models on DocLayNet. Similarly as in PubLayNet, we will evaluate the quality of their predictions using mean average precision (mAP) with 10 overlaps that range from 0.5 to 0.95 in steps of 0.05 (mAP@0.5-0.95). These scores are computed by leveraging the evaluation code provided by the COCO API [16].\n",
      "\n",
      "## Baselines for Object Detection\n",
      "\n",
      "In Table 2, we present baseline experiments (given in mAP) on Mask R-CNN [12], Faster R-CNN [11], and YOLOv5 [13]. Both training and evaluation were performed on RGB images with dimensions of 1025 × 1025 pixels. For training, we only used one annotation in case of redundantly annotated pages. As one can observe, the variation in mAP between the models is rather low, but overall between 6 and 10% lower than the mAP computed from the pairwise human annotations on triple-annotated pages. This gives a good indication that the DocLayNet dataset poses a worthwhile challenge for the research community to close the gap between human recognition and ML approaches. It is interesting to see that Mask R-CNN and Faster R-CNN produce very comparable mAP scores, indicating that pixel-based image segmentation derived from bounding-boxes does not help to obtain better predictions. On the other hand, the more recent Yolov5x model does very well and even out-performs humans on selected labels such as Text , Table and Picture . This is not entirely surprising, as Text , Table and Picture are abundant and the most visually distinctive in a document.\n",
      "\n",
      "Table 2: Prediction performance (mAP@0.5-0.95) of object detection networks on DocLayNet test set. The MRCNN (Mask R-CNN) and FRCNN (Faster R-CNN) models with ResNet-50 or ResNet-101 backbone were trained based on the network architectures from the detectron2 model zoo (Mask R-CNN R50, R101-FPN 3x, Faster R-CNN R101-FPN 3x), with default configurations. The YOLO implementation utiized was YOLOv5x6 [13]. All models were initialised using pre-trained weights from the COCO 2017 dataset.\n",
      "\n",
      "|                | human   |   MRCNN |      |   FRCNN |   YOLO |\n",
      "|----------------|---------|---------|------|---------|--------|\n",
      "| Caption        | 84-89   |    68.4 | 71.5 |    70.1 |   77.7 |\n",
      "| Footnote       | 83-91   |    70.9 | 71.8 |    73.7 |   77.2 |\n",
      "| Formula        | 83-85   |    60.1 | 63.4 |    63.5 |   66.2 |\n",
      "| List-item      | 87-88   |    81.2 | 80.8 |    81   |   86.2 |\n",
      "| Page-footer    | 93-94   |    61.6 | 59.3 |    58.9 |   61.1 |\n",
      "| Page-header    | 85-89   |    71.9 | 70   |    72   |   67.9 |\n",
      "| Picture        | 69-71   |    71.7 | 72.7 |    72   |   77.1 |\n",
      "| Section-header | 83-84   |    67.6 | 69.3 |    68.4 |   74.6 |\n",
      "| Table          | 77-81   |    82.2 | 82.9 |    82.2 |   86.3 |\n",
      "| Text           | 84-86   |    84.6 | 85.8 |    85.4 |   88.1 |\n",
      "|                |         |    76.7 | 80.4 |    79.9 |   82.7 |\n",
      "| AIl            | 82-83   |    72.4 | 73.5 |    73.4 |   76.8 |\n",
      "\n",
      "to avoid this at any cost in order to have clear, unbiased baseline numbers for human document-layout annotation. Third, we introduced the feature of snapping boxes around text segments to obtain a pixel-accurate annotation and again reduce time and effort. The CCS annotation tool automatically shrinks every user-drawn box to the minimum bounding-box around the enclosed text-cells for all purely textbased segments, which excludes only Table and Picture . For the latter, we instructed annotation staff to minimise inclusion of surrounding whitespace while including allgraphical lines. A downside of snapping boxes to enclosed text cels is that some wrongly parsed PDF pages cannot be annotated correctly and need to be skipped. Fourth, we established a way to flag pages as rejected for cases where no Aooul epu je sebed ad en pinom s o sese eldexg penee e pino seuepnb jqel eu o bupoe uoeouue piia or contain layouts that are impossible to capture with non-overlapping rectangles. Such rejected pages are not contained in the final dataset. With allthese measures in place, experienced annotation staff managed to annotate a single page in a typical timeframe of 20s to 60s, depending on its complexity.\n",
      "\n",
      "## 5EXPERIMENTS\n",
      "\n",
      "The primary goal of DocLayNet is to obtain high-quality ML models capable of accurate document-layout analysis on a wide variety of challenging layouts. As discussed in Section 2, object detection models are currently the easiest to use, due to the standardisation of ground-truth data in COCO format [16] and the availability of general frameworks such as detectron2 [17]. Furthermore, baseline numbers in PubLayNet and DocBank were obtained using standard object detection models such as Mask R-CNN and Faster R-CNN. As such, we will relate to these object detection methods in this\n",
      "\n",
      "Figure 5: Prediction performance (mAP @0.5-0.95) of a Mask R-CNN network with ResNet50 backbone trained on increasing fractions of the DocLayNet dataset. The learning curve fliattens around the 80% mark, indicating that increasing the size of the DocLayNet dataset with similar data will not yield significantly better predictions.\n",
      "\n",
      "paper and leave the detailed evaluation of more recent methods mentioned in Section 2 for future work.\n",
      "\n",
      "In this section, we will present several aspects related to the performance of object detection models on DocLayNet. Similarly as in 01 S'o wo oueu je sdeμano o1 gn (dvw) uoslud obee ueou Susn suogopeud son jo Agenb au ogenjene m om joenqnd 0.95 in steps of 0.05 (mAP@0.5-0.95). These scores are computed by leveraging the evaluation code provided by the COCO API [16].\n",
      "\n",
      "## BaselinesforObjectDetection\n",
      "\n",
      "Buuje qog [e1] AOTO pue [11] NN- sseg \"[z] NN- xsew uo (dvw u uenjf) squewuedxe eugeseq jueseud am °2 e(qe1 u] and evaluation were performed on RGB images with dimensions of 1025 × 1025 poxels. For training, we only used one annotation in case of redundantly annotated pages. As one can observe, the variation in mAP between the models is rather low, but overall between 6 and aug jeug uogeo(pul poo6 e sanj6 sju1 sobed papeqouue-ajdj4 uo suogepouue uewnq esjwujed oug wog pepndwoo dyw aug ueug samol %01 DocLayNet dataset poses a worthwhile challenge for the research community to close the gap between human recognition and ML approaches. It is interesting to see that Mask R-CNN and Faster R-CNN produce very comparable mAP scores, indicating that pixelbased image segmentation derived from bounding-boxes does not help to obtain better predictions. On the other hand, the more recent Yolov5x model does very welland even out-performs humans on selected labels such as Text , Table and Picture This is not entirely surprising, as Text , Table and Picture are abundant and the most visually distinctive in a document.\n",
      "\n",
      "Figure 3: Page 6 of the DocLayNet paper. If recognized, metadata such as authors are appearing first under the title. Elements recognized as page headers or footers are suppressed in Markdown to deliver uninterrupted content in reading order. Tables are inserted in reading order. The paragraph in '5. Experiments' wrapping over the column end is broken up in two and interrupted by the table.\n",
      "\n",
      "KDD '22, August 14-18, 2022, Washington, DC, USA\n",
      "\n",
      "Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar\n",
      "\n",
      "Table 1: DocLayNet dataset overview. Along with the frequency of each class label, we present the relative occurrence (as %\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "of row 'Total') in the train, test and validation sets. The inter-annotator agreement is computed as the mAP@0.5-0.95 metric we distributed the annotation workload and performed continuous\n",
      "\n",
      "only. For phases three and four, a group of 40 dedicated annotators quality controls. Phase one and two required a small team of experts\n",
      "\n",
      "were assembled and supervised.\n",
      "\n",
      "while coverage ensures that all meaningful items on a page can to a document category, such as\n",
      "\n",
      "be annotated. We refrained from class labels that are very specific\n",
      "\n",
      "Abstract in the\n",
      "\n",
      "Scientific Articles semantics of the text. Labels such as\n",
      "\n",
      "category. We also avoided class labels that are tightly linked to the\n",
      "\n",
      "Author\n",
      "\n",
      "Affiliation\n",
      "\n",
      "teria for documents were described in Section 3. A large effort went into ensuring that all documents are free to use. The data sources in DocBank, are often only distinguishable by discriminating on 3 https://arxiv.org/ Figure 4: Table 1 from the DocLayNet paper in the original PDF (A), as rendered Markdown (B) and in JSON representation (C). Spanning table cells, such as the multi-column header 'triple interannotator mAP@0.5-0.95 (%)', is repeated for each column in the Markdown representation (B), which guarantees that every data point can be traced back to row and column headings only by its grid coordinates in the table. In the JSON representation, the span information is reflected in the fields of each table cell (C).\n",
      "\n",
      "and\n",
      "\n",
      ", as seen\n",
      "\n",
      "Phase 1: Data selection and preparation.\n",
      "\n",
      "Our inclusion cri-\n"
     ]
    }
   ],
   "source": [
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "source = \"https://arxiv.org/pdf/2408.09869\"  # document per local path or URL\n",
    "converter = DocumentConverter()\n",
    "result = converter.convert(source)\n",
    "print(result.document.export_to_markdown())  # output: \"## Docling Technical Report[...]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb71d1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain-docling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e3795f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install docling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9f7a12",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'DoclingPDFLoader' from 'langchain_docling' (c:\\Users\\SAMSUNG\\miniconda3\\envs\\open\\Lib\\site-packages\\langchain_docling\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_chroma\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Chroma\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_text_splitters\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_docling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DoclingPDFLoader\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'DoclingPDFLoader' from 'langchain_docling' (c:\\Users\\SAMSUNG\\miniconda3\\envs\\open\\Lib\\site-packages\\langchain_docling\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6914f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 15:42:35,303 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 15:42:35,892 - INFO - Going to convert document batch...\n",
      "2025-12-02 15:42:35,896 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 44ae89a68fc272bc7889292e9b5a1bad\n",
      "2025-12-02 15:42:36,156 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-02 15:42:36,169 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2025-12-02 15:42:36,396 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-02 15:42:36,438 - INFO - Registered ocr engines: ['auto', 'easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "2025-12-02 15:42:37,778 - INFO - Accelerator device: 'cpu'\n",
      "\u001b[32m[INFO] 2025-12-02 15:42:37,830 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-02 15:42:37,874 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\SAMSUNG\\miniconda3\\envs\\open\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-02 15:42:37,877 [RapidOCR] main.py:53: Using C:\\Users\\SAMSUNG\\miniconda3\\envs\\open\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-02 15:42:38,060 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-02 15:42:38,078 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\SAMSUNG\\miniconda3\\envs\\open\\Lib\\site-packages\\rapidocr\\models\\ch_ppocr_mobile_v2.0_cls_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-02 15:42:38,078 [RapidOCR] main.py:53: Using C:\\Users\\SAMSUNG\\miniconda3\\envs\\open\\Lib\\site-packages\\rapidocr\\models\\ch_ppocr_mobile_v2.0_cls_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-02 15:42:38,159 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-02 15:42:38,197 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\SAMSUNG\\miniconda3\\envs\\open\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-02 15:42:38,199 [RapidOCR] main.py:53: Using C:\\Users\\SAMSUNG\\miniconda3\\envs\\open\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.onnx\u001b[0m\n",
      "2025-12-02 15:42:38,416 - INFO - Auto OCR model selected rapidocr with onnxruntime.\n",
      "2025-12-02 15:42:38,467 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-02 15:42:39,712 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-02 15:42:40,649 - INFO - Processing document test.pdf\n",
      "2025-12-02 15:44:11,466 - INFO - Finished converting document test.pdf in 96.16 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "표 1 종합 성능 비교 및 통계적 유의성 (SNR 0dB).\n",
      "\n",
      "| 지표   | Li et al.        | 제안 방법        | 개선도   | t- 검정 p- 값   |\n",
      "|--------|------------------|------------------|----------|-----------------|\n",
      "| CS     | 0.9975 ± 0.0024  | 0.9982 ± 0.0018  | +0.07%   | 0.0041          |\n",
      "| EVM    | 2.41% ± 0.52%    | 1.99% ± 0.40%    | -0.42%p  | 0.0014          |\n",
      "| PSLR   | -44.80 ± 1.23 dB | -46.35 ± 0.89 dB | +1.55 dB | <0.0001         |\n",
      "| ISLR   | -30.00 ± 1.05 dB | -31.70 ± 0.76 dB | +1.70 dB | 0.0002          |\n",
      "\n",
      "모든 지표에서 p-value 가 0.05 미만으로 95% 신뢰수준에서 통계적으 로 유의미한 개선을 확인했으며 , 특히 스펙트럼 품질 지표인 PSLR 과 ISLR 은 p  &lt;  0.001 로 99.9% 신뢰수준에서 매우 강력한 우수성을 보였다 .  95% 신뢰 구간 분석 결과 , 모든 지표의 개선 구간이 0 을 포함하지 않아 (e.g., CS 개선 :  [+0.0003,  +0.0011]), 최악의 경우에도 성능 향상이 보장됨을 입증하였다 . 성능 향상은 [ 그림 9] 과 [ 그림 10] 에서 보듯이 ,  SNR  -20  dB 의 열악한 환경 (CS +0.48%) 과 5 개의 다중 간섭 환경 (CS +3.30%) 에서 가 장 극대화되어 , 제안 방법이 극한 조건과 복잡한 실제 환경 대응에 특화되 어 있음을 확인하였다 .\n",
      "\n",
      "## Comprehensive Performance Comparison (All Metrics at SNR OdB)\n",
      "\n",
      "그림 1 Comprehensive Performance\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## CS Performance vs Signal-to-Noise Ratio (SNR Variation)\n",
      "\n",
      "그림 2 CS Performance vs Signal-to-Noise Ratio\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## 3. 한계점 및 향후 연구 방향\n",
      "\n",
      "본 연구는 513 개의 테스트 프레임 중 97.46% 의 성공률을 달성하였으 나 ,  2.54%(13 개 ) 의 실패 사례가 존재하였다 . 실패 사례 분석 결과 , 대부분 은 SNR  -30  dB 이하의 신호가 배경 노이즈보다 낮아 물리적으로 탐지가\n",
      "\n",
      "불가능하거나 (1.56%), 학습 범위를 초과하는 5 개 이상의 극단적인 다중 간 섭 (0.59%), 혹은 센서의 도플러 측정 한계를 벗어난 경우 (0.39%) 로 나타났 다 . 이는 제안된 방법이 신호처리 영역 내에서 해결 가능한 대부분의 문제 를 이미 극복하였음을 시사한다 .\n",
      "\n",
      "향후 연구는 이러한 물리적·학습적 한계를 넘어 , 보다 복잡한 실제 주 행 환경에서의 간섭 상황을 다루는 데 초점을 맞출 예정이다 . 특히 실제 교통 정체 (Traffic  Jam) 상황에서 발생하는 FMCW -FMCW 상호간섭은 새 로운 도전 과제이다 . 이 환경에서는 차량 간 레이더의 슬로프와 프레임 구 조가 상이하거나 비동기적으로 작동하면서 다음과 같은 문제가 빈번히 발 생한다 :\n",
      "\n",
      "비동기 슬로프 간섭으로 인한 비트 신호 대역 내 스퓨리어스 및 고스트 타 깃 , 치프 (Chirp) 충돌과 시간 오프셋으로 인한 Range -Doppler 고스트 라 인 증가 , 그리고 프론트엔드 포화 및 위상 잡음 상승에 따른 PSLR/ISLR 악화 가능성이다 .\n",
      "\n",
      "이러한 실교통 간섭 문제를 해결하기 위해 , 리얼 Traffic  Jam 환경 기 반의 다차선 도로 실험 설계가 필요하다 . 도시 및 고속도로 혼잡 구간에서 주변 차량의 FMCW 파라미터 ( 슬로프 , 대역폭 ,  PRI/ 프레임 구조 ) 를 실측·로 그화하고 , 간섭 구간의 타임스탬프를 라벨링하여 Range -Doppler(RD) 맵 기반의 정량적 GT(ground truth) 생성 절차를 정립할 예정이다 .\n",
      "\n",
      "또한 상호간섭 전용 검출·완화 모듈의 추가가 요구된다 . 구체적으로는 크로스 앰비규어티 함수 (CAF) 기반의 비동기 FMCW 간섭 서명 탐지 , 스 펙트로그램 리지 / 허프 (2D) 탐색을 통한 RD고스트 특징 분리 , 비트 도메인 적응형 노치 필터와 RD 도메인 정합필터를 결합한 2 단계 제거 구조를 도 입함으로써 간섭의 정밀 억제를 실현할 수 있다 .\n",
      "\n",
      "이를 지원하기 위해 , 시뮬레이터를 확장하여 다양한 슬로프 및 프레임 충돌 모델 ( 동일 / 상이 주파수대 , 시간 지터 , 슬로프 랜덤화 ) 을 포함하고 , 실 제 및 시뮬레이션 데이터를 혼합한 도메인 믹싱 학습 데이터셋을 구축함으 로써 현실 도메인 간 갭을 줄일 계획이다 . 향후 검증 목표는 CW/ 스윕 간 섭을 넘어 , 실제 도심 정체 구간에서의 FMCW -FMCW 상호간섭까지 포괄 적으로 평가하는 것이다 . 이를 위해 다차선 혼잡·정체 구간의 실주행 로그 를 다양한 시간대와 기상 조건에서 수집하고 , 주변 차량의 FMCW 메타데 이터를 병행 확보한다 . 이후 RD 맵 기반의 고스트 / 스퓨리어스 자동 라벨링 시스템을 구축하여 제안 알고리즘과 기준선 기법을 비교 평가할 예정이다 .\n",
      "\n",
      "주요 평가지표는 CS, EVM, PSLR, ISLR 외에 고스트 검출률 (GDR), 오 경보율 (FAR),  RD고스트 밀도 ( 개 / 프레임 ) 등을 포함하며 , 목표 성능 기준 은 다음과 같다 :\n",
      "\n",
      "GDR 90% 이상 , FAR 5% 이하 ( 혼잡 구간 평균 ), PSLR/ISLR 기존 대비 각각 +1.0 dB 이상 개선 유지 , 그리고 30 fps( 약 8.5 ms/ 프레임 ) 수준의 실시간 처리 성능 확보이다 .\n",
      "\n",
      "종합하면 , 본 연구는 FMCW 레이더의 CW 간섭 제거 문제에 대해 기 존 SOTA 기법의 실무적 한계를 근본적으로 극복한 통합 솔루션을 제시하 였다 . 제안된 방법은 실제 데이터 기반 검증 ,  GPU 최적화 , 적응형 파라미 터 설계 , 그리고 딥러닝 기반 하이브리드 구조라는 네 가지 핵심 개선을 통해 성능 , 속도 , 강건성을 동시에 달성하였다 .\n",
      "\n",
      "그 결과 , 모든 평가 지표에서 기존 SOTA 대비 통계적으로 유의미한 (p &lt; 0.05) 성능 향상 (CS 0.9982, PSLR -46.35 dB) 을 달성하였으며 , 처리 시 간을 198.2  ms 에서 8.5  ms 로 단축하여 약 23 배의 속도 개선을 이루었다 . 또한 SNR -20 dB 의 극한 환경과 5 개의 다중 간섭 상황에서도 안정적인 성\n",
      "\n",
      "능을 유지함으로써 높은 강건성을 입증하였다 .\n",
      "\n",
      "따라서 본 연구는 해석 가능한 신호처리와 딥러닝 기반 패턴 인식의 융합이 극한의 저신호 및 복잡한 간섭 환경에서도 효과적으로 작동함을 입 증하였으며 , 향후 실제 도심 정체 구간에서의 FMCW -FMCW 상호간섭까지 포괄 검증함으로써 자동차 레이더 기반 ADAS 시스템의 안전성과 신뢰성 을 한층 더 향상시킬 수 있는 실질적 기반을 마련하였다 .\n",
      "\n",
      "## 참고문헌 (References)\n",
      "\n",
      "- [1] C. Ballard, \"Advanced automotive radar applications,\" in Proceedings of the 2020 IEEE Radio Frequency Integrated Circuits Symposium, pp. 18, 2020.\n",
      "- [2] I. Goodfellow, Y. Bengio, and A. Courville, Deep learning, MIT Press, 2016.\n",
      "- [3] M. I. Skolnik, Radar handbook, 3rd ed., McGraw-Hill, 2008.\n",
      "- [4] Yanbing Li; Weichuan Zhang; Lianying Ji, \"Automotive Radar Mutual Interference Mitigation Based on Hough Transform in Time-Frequency Domain\", 2024.\n",
      "- [5] G. F. DeLong and E. M. Hofstetter, \"On the design of optimum radar detectors for clutter rejection,\" IEEE Transactions on Information Theory, vol. 13, no. 3, pp. 434-444, 1967.\n",
      "- [6] Y. LeCun, Y. Bengio, and G. E. Hinton, \"Deep learning,\" Nature, vol. 521, no. 7553, pp. 436-444, 2015.\n",
      "- [7]  Y.  LeCun,  L.  Bottou,  Y.  Bengio,  and  P.  Haffner,  \"Gradient-based learning applied to document recognition,\" Proceedings of the IEEE, vol. 86, no. 11, pp. 2278-2324, 1998.\n",
      "- [8] T. W. Parks and C. S. Burrus, Digital filter design, Wiley-Interscience, 1987.\n",
      "- [9] A. Farina, \"Electronic counter-countermeasures to radar,\" in Radar Handbook, 3rd ed., McGraw-Hill, 2008.\n",
      "- [10] M. A. Richards, Fundamentals of radar signal processing, 2nd ed., McGraw-Hill, 2014.\n",
      "- [11] S. Haykin and B. Van Veen, Signals and systems, 2nd ed., Wiley, 2003.\n",
      "- [12]  D. P. Kingma  and  J.  Ba, \"Adam:  A  method  for  stochastic optimization,\" in Proceedings of the International Conference on Learning Representations, 2015.\n",
      "\n",
      "- [13] S. Ioffe and C. Szegedy, \"Batch normalization: Accelerating deep network training by reducing internal covariate shift,\" in Proceedings of the International Conference on Machine Learning, pp. 448-456, 2015.\n",
      "- [14] N. Wiener, Extrapolation, interpolation, and smoothing of stationary time series: With engineering applications, Wiley, 1949.\n",
      "- [15] S. Haykin and X. B. Li, \"Detection of signals in chaos,\" Proceedings of the IEEE, vol. 83, no. 1, pp. 95-122, 1995.\n",
      "- [16]  L.  Cohen  and  C.  Barnes,  \"The  Wigner  distribution  for  radar  and sonar  processing,\"  IEEE  Transactions  on  Aerospace  and  Electronic Systems, vol. 24, no. 4, pp. 330-340, 1988.\n",
      "- [17] D. Garmatyuk and K. E. EDC, \"Multicarrier radar signal processing for target recognition,\" IEEE Transactions on Aerospace and Electronic Systems, vol. 47, no. 2, pp. 1094-1108, 2011.\n",
      "- [18] S. M. Patole, M. Torlak, D. Wang, and M. Ali, \"Automotive radars and challenges ahead,\" IEEE Microwave Magazine, vol. 18, no. 7, pp. 4252, 2017.\n",
      "- [19] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, \"Image quality assessment: From error visibility to structural similarity,\" IEEE Transactions on Image Processing, vol. 13, no. 4, pp. 600-612, 2004.\n",
      "- [20] J. Johnson, A. Alahi, and L. Fei-Fei, \"Perceptual losses for realtime  style  transfer  and  super-resolution,\"  in  European  Conference  on Computer Vision, pp. 694-711, 2016.\n",
      "- [21]  O.  Ronneberger,  P.  Fischer,  and  T.  Brox,  \"U-Net:  Convolutional networks  for  biomedical  image  segmentation,\"  in  Proceedings  of  the Medical Image Computing and Computer-Assisted Intervention, pp. 234241, 2015.\n",
      "- [22] A. Krizhevsky, I. Sutstever, and G. E. Hinton, \"Imagenet classification with deep convolutional neural networks,\" in Advances in Neural Information Processing Systems 25, pp. 1097-1105, 2012.\n",
      "- [23] K. He, X. Zhang, S. Ren, and J. Sun, \"Deep residual learning for\n",
      "\n",
      "- image recognition,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778, 2016.\n",
      "- [24]  I.  J.  Goodfellow,  J.  Pouget-Abadie,  M.  Mirza,  et  al.,  \"Generative adversarial nets,\" in Advances in Neural Information Processing Systems, pp. 2672-2680, 2014.\n",
      "- [25] D. P. Kingma and M. Welling, \"Auto-encoding variational Bayes,\" in Proceedings of the International Conference on Learning Representations, 2014.\n",
      "- [26] A. Vaswani, N. Shazeer, N. Parmar, et al., \"Attention is all you need,\" in Advances in Neural Information Processing Systems, pp. 5998-6008, 2017.\n",
      "\n",
      "## 부록 (Appendix): CARRADA 데이터셋 파일 구조\n",
      "\n",
      "## 1. 개요\n",
      "\n",
      "CARRADA( 카메라 및 레이더 융합 도로 인식 ) 데이터셋은 자동차 환경에서 의 멀티모달 센서 데이터를 수용하도록 설계된 계층적 구조로 구성되어 있 다 . 데이터셋은 2019 년과 2020 년 두 시점에서 수집된 총 30 개의 시퀀스로 이루어져 있으며 , 각 시퀀스는 타임스탬프 형식의 디렉토리 이름으로 관리 된다 .\n",
      "\n",
      "## 2. 루트 디렉토리 구조\n",
      "\n",
      "## Carrada/\n",
      "\n",
      "```\n",
      "├── 시퀀스 디렉토리 (30 개 시퀀스 ) │ ├── 2019-09-16-12-52-12/ │ ├── 2019-09-16-12-55-51/ │ ├── ... (2019-09-16 의 12 개 시퀀스 ) │ ├── 2020-02-28-12-12-16/ │ ├── 2020-02-28-12-13-54/ │ └── ... (2020-02-28 의 18 개 시퀀스 ) ├── cam_params/ │ ├── intrinsics.xml │ ├── extrinsics_2019.xml │ └── extrinsics_2020.xml ├── 전역 주석 파일\n",
      "```\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "CARRADA 데이터셋의 최상위 계층은 위의 구조와 같이 여러 구성 요소로 이루어진다 . 2019 년 9 월 16 일에 12:52 부터 13:25 사이에 수집된 12 개의 시퀀스 디렉토리와 2020 년 2 월 28 일에 12:12 부터 13:15 사이에 수집된 18 개의 시퀀스 디렉토리가 각각의 녹화 날짜별로 분류되어 있다 . 이들 시 퀀스 디렉토리들 외에도 카메라 캘리브레이션 파라미터를 저장하는 cam\\_params 폴더 , 전체 데이터셋에 대한 주석 정보를 담은 JSON 파일들 , 메타데이터 및 검증 파일들 , 그리고 통계 및 가중치 파일들이 최상위 디렉 토리에 위치한다 .\n",
      "\n",
      "## 3. 시퀀스 디렉토리 구조\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "[ 타임스탬프 ] ( 예 : 2019-09-16-12-52-12)/ ├── annotations/ │ ├── box/ │ ├── dense/ │ └── sparse/ └── rd\\_points.json\n",
      "\n",
      "각 시퀀스는 YYYY-MM-DD-HH-MM-SS 형식의 타임스탬프로 명명되는 디렉토리로 저장된다 . 예를 들어 , 2019-09-16-12-52-12 는 2019 년 9 월 16 일 오후 12 시 52 분 12 초에 수집된 시퀀스를 의미한다 . 각 시퀀스 디렉 토리 내부는 annotations 폴더와 rd\\_points.json 파일로 구성된다 .\n",
      "\n",
      "## 4. 주석 디렉토리 (annotations/)\n",
      "\n",
      "annotations/\n",
      "\n",
      "├── box/                            (\n",
      "\n",
      "바운딩 박스 주석 저장 )\n",
      "\n",
      "├── dense/                        (\n",
      "\n",
      "픽셀 레벨 밀집 주석 저장 )\n",
      "\n",
      "└──\n",
      "\n",
      "sparse/                      (\n",
      "\n",
      "희소 주석 저장 )\n",
      "\n",
      "시퀀스 내의 annotations 폴더는 세 가지 하위 디렉토리를 포함한다 . 첫째 , box 디렉토리는 객체 탐지 작업을 위한 바운딩 박스 주석을 저장하며 , 시 퀀스에서 탐지된 차량 및 기타 객체에 대한 형식화된 바운딩 박스 좌표를 포함한다 . 둘째 , dense 디렉토리는 분할 작업에 주로 사용되는 밀집된 픽셀 레벨 주석을 포함하며 , 시퀀스의 각 프레임에 대한 포괄적인 픽셀당 라벨 을 제공한다 . 셋째 , sparse 디렉토리는 감소된 주석 밀도를 가진 희소 주석 을 포함하며 , 특정 객체 인스턴스 또는 관심 영역에 사용된다 .\n",
      "\n",
      "## 5. 레이더 포인트 데이터 (rd\\_points.json)\n",
      "\n",
      "각 시퀀스 디렉토리에 포함된 rd\\_points.json 파일은 해당 시퀀스의 레이더 탐지 포인트 클라우드를 JSON 형식으로 저장한다 . 이 파일은 3D 포인트의 공간 좌표와 각 포인트의 레이더 반사율 (radar reflectivity) 정보를 포함하\n",
      "\n",
      "며 , 레이더 기반 객체 탐지 및 분석에 필수적인 데이터이다 .\n",
      "\n",
      "카메라 캘리브레이션 파라미터 (cam\\_params/)\n",
      "\n",
      "cam\\_params/\n",
      "\n",
      "├── intrinsics.xml                      (\n",
      "\n",
      "공통 카메라 내부 파라미터 )\n",
      "\n",
      "├── extrinsics\\_2019.xml            (2019\n",
      "\n",
      "년 외부 파라미터 )\n",
      "\n",
      "└── extrinsics\\_2020.xml            (2020\n",
      "\n",
      "년 외부 파라미터 )\n",
      "\n",
      "cam\\_params 폴더는 모든 시퀀스에서 사용되는 공통 캘리브레이션 정보를 포함한다 . 이 폴더 내에는 세 개의 XML 파일이 저장되어 있다 .\n",
      "\n",
      "intrinsics.xml 파일은 카메라의 내부 파라미터 (intrinsic parameters) 를 저 장한다 . 이는 초점거리 (focal length), 주점 좌표 (principal point), 그리고 렌즈 왜곡 계수 (distortion coefficients) 와 같은 카메라의 광학적 특성을 나 타내며 , 이 값들은 모든 시퀀스에서 동일하게 적용된다 .\n",
      "\n",
      "extrinsics\\_2019.xml 파일은 2019 년 9 월 16 일 녹화 세션에서 사용된 카 메라와 다른 센서 ( 레이더 , 라이더 등 ) 간의 외부 캘리브레이션 (extrinsic calibration) 정보를 포함한다 . 이는 카메라의 좌표계와 다른 센서의 좌표계 를 연결하는 회전 행렬과 변환 벡터로 표현된다 .\n",
      "\n",
      "extrinsics\\_2020.xml 파일은 2020 년 2 월 28 일 녹화 세션에서 사용된 카 메라의 외부 캘리브레이션 파라미터를 정의한다 . 두 시점의 녹화 세션 사 이에 센서의 장착 위치나 각도가 변경되었을 가능성을 고려하여 별도의 파 일로 관리된다 .\n",
      "\n",
      "## 6. 전역 주석 파일\n",
      "\n",
      "annotations\\_frame\\_oriented.json                   (\n",
      "\n",
      "프레임 기반 주석 )\n",
      "\n",
      "annotations\\_instance\\_oriented.json              (\n",
      "\n",
      "인스턴스 기반 주석 )\n",
      "\n",
      "light\\_dataset\\_frame\\_oriented.json                (\n",
      "\n",
      "경량 프레임 기반 주석 )\n",
      "\n",
      "selected\\_light\\_dataset\\_frame\\_oriented.json ( 선택된 경량 주석 )\n",
      "\n",
      "데이터셋의 최상위 디렉토리에는 전체 데이터셋에 대한 주석 정보를 담은\n",
      "\n",
      "여러 JSON 파일이 저장된다 .\n",
      "\n",
      "annotations\\_frame\\_oriented.json 파일은 프레임 인덱스를 기본 키로 하여 구성된 프레임 레벨 주석을 제공한다 . 이는 각 프레임마다 어떤 객체가 어 디에 위치하는지에 대한 정보를 담고 있으며 , 프레임별 처리 및 분석에 적 합한 형식이다 .\n",
      "\n",
      "annotations\\_instance\\_oriented.json 파일은 프레임 간 객체 인스턴스별로 구성된 인스턴스 레벨 주석을 제공한다 . 동일한 객체가 여러 프레임에서 추적되는 경우 , 이를 하나의 인스턴스로 그룹화하여 저장하기 때문에 객체 추적 및 다중 프레임 객체 분석에 유용하다 .\n",
      "\n",
      "light\\_dataset\\_frame\\_oriented.json 파일은 축소된 파일 크기로 필수 주석 정보만 포함하는 프레임 지향 주석의 경량 버전이다 . 이는 빠른 처리가 필 요한 경우나 저장 공간 제약이 있는 경우 활용된다 .\n",
      "\n",
      "selected\\_light\\_dataset\\_frame\\_oriented.json 파일은 고품질로 검증된 주석 의 부분집합을 나타내며 , 연구 목적에 따라 신뢰할 수 있는 데이터만 선별 하여 사용할 수 있다 .\n",
      "\n",
      "## 7. 메타데이터 및 검증 파일\n",
      "\n",
      "data\\_seq\\_ref.json                      (\n",
      "\n",
      "시퀀스 참조 정보 및 메타데이터 )\n",
      "\n",
      "validated\\_seqs.txt                    (\n",
      "\n",
      "검증 통과 시퀀스 목록 )\n",
      "\n",
      "instance\\_exceptions.json       (\n",
      "\n",
      "예외 인스턴스 정보 )\n",
      "\n",
      "데이터셋의 최상위 디렉토리에는 데이터 관리 및 검증을 위한 파일들이 저 장된다 .\n",
      "\n",
      "data\\_seq\\_ref.json 파일은 모든 시퀀스에 대한 참조 정보와 메타데이터를 포함한다 . 이는 각 시퀀스의 녹화 시간 , 위치 , 환경 정보 , 그리고 시퀀스의 유효성 플래그 등을 담고 있어 데이터셋 전체를 이해하는 데 필수적이다 .\n",
      "\n",
      "validated\\_seqs.txt 파일은 품질 검증을 통과한 시퀀스 목록을 기록한다 . 이는 훈련 및 평가에 적합한 신뢰할 수 있는 시퀀스를 식별하는 데 사용되 며 , 모델 개발 시 데이터 선택의 기준이 된다 .\n",
      "\n",
      "instance\\_exceptions.json 파일은 주석 불규칙성이나 수동 검토가 필요한 특수한 경우의 인스턴스 정보를 문서화한다 . 이를 통해 연구자들은 잠재적\n",
      "\n",
      "인 문제 상황을 미리 파악할 수 있다 .\n",
      "\n",
      "## 8. 통계 및 가중치 파일\n",
      "\n",
      "| rad_stats.json / rad_stats_all.json    | ( 레이더 데이터 통계 )      |\n",
      "|----------------------------------------|-----------------------------|\n",
      "| ra_stats.json / ra_stats_all.json      | ( 거리 - 방위각 표현 통계 ) |\n",
      "| rd_stats.json / rd_stats_all.json      | ( 거리 - 도플러 표현 통계 ) |\n",
      "| ad_stats.json / ad_stats_all.json      | ( 방위각 - 도플러 표현 통계 |\n",
      "| ra_weights.json / ra_weights_kaul.json | ( 거리 - 방위각 가중치 )    |\n",
      "| rd_weights.json / rd_weights_kaul.json | ( 거리 - 도플러 가중치 )    |\n",
      "\n",
      "데이터셋은 서로 다른 레이더 표현에 대한 포괄적인 통계 정보를 제공한 다 . 각 파일은 두 가지 버전으로 제공되는데 , 일반 버전은 정규화된 통계 를 , \\_all 버전은 전체 데이터의 통계를 포함한다 .\n",
      "\n",
      "rad\\_stats 파일은 레이더 데이터 전체의 통계 ( 평균 , 표준편차 , 최솟값 , 최댓 값 등 ) 를 저장한다 . ra\\_stats 파일은 거리 (Range) 와 방위각 (Azimuth) 차원 의 레이더 표현에 대한 통계를 포함하며 , rd\\_stats 파일은 거리 (Range) 와 도플러 (Doppler) 차원의 표현 통계를 , ad\\_stats 파일은 방위각과 도플러 차 원의 표현 통계를 각각 제공한다 .\n",
      "\n",
      "가중치 파일들 (ra\\_weights, rd\\_weights 등 ) 은 각 표현에 대한 데이터 균형 가중치를 저장한다 . 이러한 통계 파일들은 데이터 정규화 , 훈련 중 클래스 불균형 해결 , 그리고 데이터 분포 분석에 활용된다 .\n",
      "\n",
      "문서 및 라이선싱\n",
      "\n",
      "README.md       (\n",
      "\n",
      "데이터셋 설명 및 사용 가이드 )\n",
      "\n",
      "LICENSE         ( 라이선싱 정보 )\n",
      "\n",
      "README.md 파일은 데이터셋의 전반적인 개요 , 주석 형식 설명 , 데이터 구조 사양 , 그리고 사용 가이드라인을 포함하는 주요 문서이다 . 데이터셋을 처음 다루는 사용자가 참고해야 할 기본 정보를 담고 있다 .\n",
      "\n",
      "LICENSE 파일은 CARRADA 데이터셋의 라이선싱 정보 및 사용 약관을 명시한다 . 연구 목적으로 데이터를 사용하기 전에 확인해야 할 중요한 파\n",
      "\n",
      "일이다 .\n",
      "\n",
      "데이터 활용 방법\n",
      "\n",
      "CARRADA 데이터셋을 활용하기 위한 일반적인 절차는 다음과 같다 . 먼저 cam\\_params 폴더에서 캘리브레이션 파라미터를 로드하고 , data\\_seq\\_ref.json 에서 시퀀스 메타데이터를 읽는다 . 그 다음 각 시퀀스 디 렉토리에 대해 , rd\\_points.json 에서 레이더 포인트 클라우드를 로드하고 , 전 역 주석 파일에서 해당 시퀀스의 프레임 또는 인스턴스 레벨 주석을 검색 한다 . 마지막으로 통계 파일을 이용하여 데이터를 정규화하고 , 클래스 균형 을 위해 가중치 파일을 적용한다 .\n",
      "\n",
      "## 9. 요약\n",
      "\n",
      "CARRADA 데이터셋은 총 30 개의 시퀀스 , 2 개의 녹화 세션 , 카메라와 레이 더 두 가지 센서 모달리티 , 바운딩 박스 , 밀집 분할 , 희소 주석 등의 다양 한 주석 유형을 제공한다 . 모든 데이터는 JSON 형식의 구조화된 주석과 XML 형식의 캘리브레이션 파라미터로 관리되며 , 포괄적인 통계 정보와 함 께 제공되어 연구자들이 카메라 -레이더 융합 기술 연구에 효과적으로 활용 할 수 있도록 설계되었다 .\n",
      "\n",
      "가 . 전역 주석 파일\n",
      "\n",
      "annotations\\_frame\\_oriented.json                   (\n",
      "\n",
      "프레임 기반 주석 )\n",
      "\n",
      "annotations\\_instance\\_oriented.json              (\n",
      "\n",
      "인스턴스 기반 주석 )\n",
      "\n",
      "light\\_dataset\\_frame\\_oriented.json                (\n",
      "\n",
      "경량 프레임 기반 주석 )\n",
      "\n",
      "selected\\_light\\_dataset\\_frame\\_oriented.json ( 선택된 경량 주석 )\n",
      "\n",
      "데이터셋의 최상위 디렉토리에는 전체 데이터셋에 대한 주석 정보를 담은 여러 JSON 파일이 저장된다 .\n",
      "\n",
      "annotations\\_frame\\_oriented.json 파일은 프레임 인덱스를 기본 키로 하여 구성된 프레임 레벨 주석을 제공한다 . 이는 각 프레임마다 어떤 객체가 어 디에 위치하는지에 대한 정보를 담고 있으며 , 프레임별 처리 및 분석에 적 합한 형식이다 .\n",
      "\n",
      "annotations\\_instance\\_oriented.json 파일은 프레임 간 객체 인스턴스별로 구성된 인스턴스 레벨 주석을 제공한다 . 동일한 객체가 여러 프레임에서 추적되는 경우 , 이를 하나의 인스턴스로 그룹화하여 저장하기 때문에 객체 추적 및 다중 프레임 객체 분석에 유용하다 .\n",
      "\n",
      "light\\_dataset\\_frame\\_oriented.json 파일은 축소된 파일 크기로 필수 주석 정보만 포함하는 프레임 지향 주석의 경량 버전이다 . 이는 빠른 처리가 필 요한 경우나 저장 공간 제약이 있는 경우 활용된다 .\n",
      "\n",
      "selected\\_light\\_dataset\\_frame\\_oriented.json 파일은 고품질로 검증된 주석 의 부분집합을 나타내며 , 연구 목적에 따라 신뢰할 수 있는 데이터만 선별 하여 사용할 수 있다 .\n",
      "\n",
      "## 나 . 메타데이터 및 검증 파일\n",
      "\n",
      "data\\_seq\\_ref.json                      (\n",
      "\n",
      "시퀀스 참조 정보 및 메타데이터 )\n",
      "\n",
      "validated\\_seqs.txt                    (\n",
      "\n",
      "검증 통과 시퀀스 목록 )\n",
      "\n",
      "instance\\_exceptions.json       ( 예외 인스턴스 정보 )\n",
      "\n",
      "데이터셋의 최상위 디렉토리에는 데이터 관리 및 검증을 위한 파일들이 저 장된다 .\n",
      "\n",
      "data\\_seq\\_ref.json 파일은 모든 시퀀스에 대한 참조 정보와 메타데이터를\n",
      "\n",
      "포함한다 . 이는 각 시퀀스의 녹화 시간 , 위치 , 환경 정보 , 그리고 시퀀스의 유효성 플래그 등을 담고 있어 데이터셋 전체를 이해하는 데 필수적이다 .\n",
      "\n",
      "validated\\_seqs.txt 파일은 품질 검증을 통과한 시퀀스 목록을 기록한다 . 이는 훈련 및 평가에 적합한 신뢰할 수 있는 시퀀스를 식별하는 데 사용되 며 , 모델 개발 시 데이터 선택의 기준이 된다 .\n",
      "\n",
      "instance\\_exceptions.json 파일은 주석 불규칙성이나 수동 검토가 필요한 특수한 경우의 인스턴스 정보를 문서화한다 . 이를 통해 연구자들은 잠재적 인 문제 상황을 미리 파악할 수 있다 .\n",
      "\n",
      "다 . 통계 및 가중치 파일\n",
      "\n",
      "| rad_stats.json / rad_stats_all.json    | ( 레이더 데이터 통계 )      |\n",
      "|----------------------------------------|-----------------------------|\n",
      "| ra_stats.json / ra_stats_all.json      | ( 거리 - 방위각 표현 통계 ) |\n",
      "| rd_stats.json / rd_stats_all.json      | ( 거리 - 도플러 표현 통계 ) |\n",
      "| ad_stats.json / ad_stats_all.json      | ( 방위각 - 도플러 표현 통계 |\n",
      "| ra_weights.json / ra_weights_kaul.json | ( 거리 - 방위각 가중치 )    |\n",
      "| rd_weights.json / rd_weights_kaul.json | ( 거리 - 도플러 가중치 )    |\n",
      "\n",
      "데이터셋은 서로 다른 레이더 표현에 대한 포괄적인 통계 정보를 제공한 다 . 각 파일은 두 가지 버전으로 제공되는데 , 일반 버전은 정규화된 통계 를 , \\_all 버전은 전체 데이터의 통계를 포함한다 . rad\\_stats 파일은 레이더 데이터 전체의 통계 ( 평균 , 표준편차 , 최솟값 , 최댓값 등 ) 를 저장한다 . ra\\_stats 파일은 거리 (Range) 와 방위각 (Azimuth) 차원의 레이더 표현에 대 한 통계를 포함하며 , rd\\_stats 파일은 거리 (Range) 와 도플러 (Doppler) 차원 의 표현 통계를 , ad\\_stats 파일은 방위각과 도플러 차원의 표현 통계를 각 각 제공한다 . 가중치 파일들 (ra\\_weights, rd\\_weights 등 ) 은 각 표현에 대한 데이터 균형 가중치를 저장한다 . 이러한 통계 파일들은 데이터 정규화 , 훈 련 중 클래스 불균형 해결 , 그리고 데이터 분포 분석에 활용된다 .\n",
      "\n",
      "라 . 문서 및 라이선싱\n",
      "\n",
      "README.md       (\n",
      "\n",
      "데이터셋 설명 및 사용 가이드 )\n",
      "\n",
      "LICENSE         ( 라이선싱 정보 )\n",
      "\n",
      "README.md 파일은 데이터셋의 전반적인 개요 , 주석 형식 설명 , 데이터 구조 사양 , 그리고 사용 가이드라인을 포함하는 주요 문서이다 . 데이터셋을 처음 다루는 사용자가 참고해야 할 기본 정보를 담고 있다 . LICENSE 파일 은 CARRADA 데이터셋의 라이선싱 정보 및 사용 약관을 명시한다 . 연구 목적으로 데이터를 사용하기 전에 확인해야 할 중요한 파일이다 .\n",
      "\n",
      "## 마 . 데이터 활용 방법\n",
      "\n",
      "CARRADA 데이터셋을 활용하기 위한 일반적인 절차는 다음과 같다 . 먼저 cam\\_params 폴더에서 캘리브레이션 파라미터를 로드하고 , data\\_seq\\_ref.json 에서 시퀀스 메타데이터를 읽는다 . 그 다음 각 시퀀스 디 렉토리에 대해 , rd\\_points.json 에서 레이더 포인트 클라우드를 로드하고 , 전 역 주석 파일에서 해당 시퀀스의 프레임 또는 인스턴스 레벨 주석을 검색 한다 . 마지막으로 통계 파일을 이용하여 데이터를 정규화하고 , 클래스 균형 을 위해 가중치 파일을 적용한다 .\n",
      "\n",
      "바 . 요약\n",
      "\n",
      "CARRADA 데이터셋은 총 30 개의 시퀀스 , 2 개의 녹화 세션 , 카메라와 레이 더 두 가지 센서 모달리티 , 바운딩 박스 , 밀집 분할 , 희소 주석 등의 다양 한 주석 유형을 제공한다 . 모든 데이터는 JSON 형식의 구조화된 주석과 XML 형식의 캘리브레이션 파라미터로 관리되며 , 포괄적인 통계 정보와 함 께 제공되어 연구자들이 카메라 -레이더 융합 기술 연구에 효과적으로 활용 할 수 있도록 설계되었다 .\n"
     ]
    }
   ],
   "source": [
    "convert = DocumentConverter()\n",
    "file_path = r'C:/python_src/6.openAI/8/test.pdf'\n",
    "result = convert.convert(file_path)\n",
    "# markdown 추출(표 구조 보존)\n",
    "markdown_content = result.document.export_to_markdown()\n",
    "print(markdown_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c94ce2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoclingPDFLoder:\n",
    "    '''Doclings을 사용한 pdf 로더'''\n",
    "    def __init__(self,file_path:str):\n",
    "        self.file_path = file_path\n",
    "    def load(self) -> List[Document]:\n",
    "        '''PDF를 로드하고 Document 리스트로 반환'''\n",
    "        convert = DocumentConverter()\n",
    "        result = convert.convert(self.file_path)\n",
    "        markdown_content = result.document.export_to_markdown()\n",
    "        # langchain의 Document 형식으로 변환\n",
    "        documents = [\n",
    "            Document(\n",
    "                page_content=markdown_content,\n",
    "                metadate = {\n",
    "                    'source' : self.file_path,\n",
    "                    'loader' : 'docling',\n",
    "                    'format' : 'markdown'\n",
    "                }\n",
    "            )\n",
    "        ]\n",
    "        return documents\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "class SimplePDFLoader: \n",
    "    '''기본 pdf 로더\n",
    "    간단한 텍스트 추출에 적합\n",
    "    표 구조는 보존되지 않음\n",
    "    이미지의 텍스트는 잘 안됨\n",
    "    '''\n",
    "    def __init__(self,file_path:str):\n",
    "        self.file_path = file_path\n",
    "    def load(self) -> List[Document]:\n",
    "        '''pdf로더 (텍스트만 추출)'''\n",
    "        loader = PyPDFLoader(self.file_path)\n",
    "        documents = loader.load()\n",
    "        # 메타데이터에 로더 정보 추가\n",
    "        for doc in documents:\n",
    "            doc.metadata['loader'] = 'pypdf'\n",
    "        return documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cea2740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스플리터\n",
    "korean_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap = 100,\n",
    "    separators= {\n",
    "        '\\n##'  # 마크다운 2단계 헤더\n",
    "        '\\n###' # 마크다운 3단계 헤더,\n",
    "        '\\n\\n',\n",
    "        '\\n',\n",
    "        '다.',\n",
    "        '요.',\n",
    "        '니다.',\n",
    "        ' ',\n",
    "        '',\n",
    "    }, length_function = len,\n",
    "    is_separator_regex= False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "72466c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 16:14:24,341 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 16:14:24,416 - INFO - Going to convert document batch...\n",
      "2025-12-02 16:14:24,424 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 44ae89a68fc272bc7889292e9b5a1bad\n",
      "2025-12-02 16:14:24,435 - INFO - Accelerator device: 'cpu'\n",
      "\u001b[32m[INFO] 2025-12-02 16:14:24,531 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-02 16:14:24,554 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\SAMSUNG\\miniconda3\\envs\\open\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-02 16:14:24,561 [RapidOCR] main.py:53: Using C:\\Users\\SAMSUNG\\miniconda3\\envs\\open\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-02 16:14:24,881 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-02 16:14:24,887 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\SAMSUNG\\miniconda3\\envs\\open\\Lib\\site-packages\\rapidocr\\models\\ch_ppocr_mobile_v2.0_cls_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-02 16:14:24,888 [RapidOCR] main.py:53: Using C:\\Users\\SAMSUNG\\miniconda3\\envs\\open\\Lib\\site-packages\\rapidocr\\models\\ch_ppocr_mobile_v2.0_cls_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-02 16:14:25,086 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-02 16:14:25,142 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\SAMSUNG\\miniconda3\\envs\\open\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-02 16:14:25,144 [RapidOCR] main.py:53: Using C:\\Users\\SAMSUNG\\miniconda3\\envs\\open\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.onnx\u001b[0m\n",
      "2025-12-02 16:14:25,410 - INFO - Auto OCR model selected rapidocr with onnxruntime.\n",
      "2025-12-02 16:14:25,412 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-02 16:14:30,706 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-02 16:14:32,127 - INFO - Processing document test.pdf\n",
      "2025-12-02 16:15:49,287 - INFO - Finished converting document test.pdf in 84.98 sec.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'set' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m docs = loader.load()\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# 청킹\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m doc_splits = \u001b[43mkorean_splitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m청킹수 : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(doc_splits)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# 벡터DB\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# 리트리버\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SAMSUNG\\miniconda3\\envs\\open\\Lib\\site-packages\\langchain_text_splitters\\base.py:117\u001b[39m, in \u001b[36mTextSplitter.split_documents\u001b[39m\u001b[34m(self, documents)\u001b[39m\n\u001b[32m    115\u001b[39m     texts.append(doc.page_content)\n\u001b[32m    116\u001b[39m     metadatas.append(doc.metadata)\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcreate_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SAMSUNG\\miniconda3\\envs\\open\\Lib\\site-packages\\langchain_text_splitters\\base.py:100\u001b[39m, in \u001b[36mTextSplitter.create_documents\u001b[39m\u001b[34m(self, texts, metadatas)\u001b[39m\n\u001b[32m     98\u001b[39m index = \u001b[32m0\u001b[39m\n\u001b[32m     99\u001b[39m previous_chunk_len = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msplit_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    101\u001b[39m     metadata = copy.deepcopy(metadatas_[i])\n\u001b[32m    102\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._add_start_index:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SAMSUNG\\miniconda3\\envs\\open\\Lib\\site-packages\\langchain_text_splitters\\character.py:151\u001b[39m, in \u001b[36mRecursiveCharacterTextSplitter.split_text\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msplit_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    143\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Split the input text into smaller chunks based on predefined separators.\u001b[39;00m\n\u001b[32m    144\u001b[39m \n\u001b[32m    145\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    149\u001b[39m \u001b[33;03m        A list of text chunks obtained after splitting.\u001b[39;00m\n\u001b[32m    150\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_split_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_separators\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SAMSUNG\\miniconda3\\envs\\open\\Lib\\site-packages\\langchain_text_splitters\\character.py:104\u001b[39m, in \u001b[36mRecursiveCharacterTextSplitter._split_text\u001b[39m\u001b[34m(self, text, separators)\u001b[39m\n\u001b[32m    102\u001b[39m final_chunks = []\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m# Get appropriate separator to use\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m separator = \u001b[43mseparators\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    105\u001b[39m new_separators = []\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, _s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(separators):\n",
      "\u001b[31mTypeError\u001b[39m: 'set' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# Step 1 일반적인 chaing을 이용한 RAG\n",
    "# 문서로딩\n",
    "file_path = r'C:/python_src/6.openAI/8/test.pdf'\n",
    "\n",
    "loader = DoclingPDFLoder(file_path)\n",
    "docs = loader.load()\n",
    "# 청킹\n",
    "doc_splits = korean_splitter.split_documents(docs)\n",
    "print(f'청킹수 : {len(doc_splits)}')\n",
    "\n",
    "# 벡터DB\n",
    "# 리트리버\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name='crag_collection',\n",
    "    embedding=OpenAIEmbeddings(model='text-embedding-3-small')\n",
    ")\n",
    "retriever = vectorstore.as_retriever(search_kwargs={'k':3})\n",
    "question = '실제 교통 정체상황에서 상호 간섭에 대해서 알려줘'\n",
    "# 사용자 질문에 대한 리트리버를 수행 context\n",
    "documents = retriever.invoke('question')\n",
    "print(f'리트리버가 찾은 context 수 : {len(documents)}')\n",
    "context = '\\n\\n---\\n\\n'.join( doc.page_content for doc in documents)\n",
    "# context로 LLM을 위한 프폼프트 작성\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template('''\n",
    "사용자의 질문에 대한 답을 주어진 context 에서만 찾고 해당 사항이 없으면 관련 없음이라고 출력할것\n",
    "context : \n",
    "{context}\n",
    "\n",
    "\n",
    "사용자 질문 : \n",
    "{question}\n",
    "                                          \n",
    "출력:\n",
    "''')\n",
    "# LLM정의\n",
    "llm = ChatOpenAI(model='gpt-4o-mini',temperature=0)\n",
    "# 체인\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "# 실행\n",
    "result = chain.invoke({\"context\":context, \"question\":question})\n",
    "print(f'LLM이 찾은 정답 : {result}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d330e47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2 -> 랭그래프를 이용한 RAG"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
