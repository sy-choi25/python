{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66f0748f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "warnings.filterwarnings('ignore')\n",
    "load_dotenv()\n",
    "api_key = os.environ.get('OPENAI_API_KEY')\n",
    "if not api_key :\n",
    "    raise ValueError('OPENAI_API_KEY')\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fdb88cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs path : c:\\python_src\\6.openAI\\6\n",
      "읽은 문서의 수 5\n"
     ]
    }
   ],
   "source": [
    "#script_dir = os.path.dirname(os.path.abspath(__file__)) # abspath 절대경로\n",
    "# print(script_dir)\n",
    "# docs_path = os.path.join(script_dir,'sample_docs', 'langraph_rag')\n",
    "script_dir = os.getcwd()\n",
    "docs_path = os.path.join(script_dir)\n",
    "print(f'docs path : {docs_path}')\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    docs_path,\n",
    "    glob = '**/*.txt',\n",
    "    loader_cls = TextLoader,\n",
    "    loader_kwargs={'encoding':'utf-8'},  # 한국어면 꼭 써줄 것\n",
    ")\n",
    "document = loader.load()\n",
    "print(f'읽은 문서의 수 {len(document)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa513830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "청킹개수 : 20\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "     chunk_size=100,\n",
    "     chunk_overlap=50,\n",
    "     separators = ['\\n\\n','\\n','.',' ',''],\n",
    "     length_function=len\n",
    ")\n",
    "\n",
    "# 스플릿 = 청킹\n",
    "doc_splits = text_splitter.split_documents(document)\n",
    "print(f'청킹개수 : {len(doc_splits)}')\n",
    "\n",
    "# 임베딩 벡터\n",
    "embedding_model = OpenAIEmbeddings(model = 'text-embedding-3-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2077a5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstroe = Chroma.from_documents(\n",
    "    documents= doc_splits,\n",
    "    collection_name='basic_rag_collection',\n",
    "    embedding=embedding_model\n",
    ")\n",
    "\n",
    "retriever = vectorstroe.as_retriever(\n",
    "    search_type = 'similarity',\n",
    "    search_kwargs = {'k':3}\n",
    ")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    ('system','Answer question based on the given context in Korean '),\n",
    "    ('human', 'Context: \\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:' )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0451b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return '\\n\\n---\\n\\n'.join([doc.page_content for doc in docs])\n",
    "\n",
    "# LCEL 방식 Runnale 객체 실행 invoke - 파이프라인\n",
    "llm = ChatOpenAI(model = 'gpt-4o-mini',temperature=0)\n",
    "rag_chain = (                                                                   # 체인은 고정이지만 invoke는 명령어라 부를때마다 변경할 수 있음\n",
    "    {'context': retriever | format_docs, 'question': RunnablePassthrough()}     # 체인을 여러번쓰려면(다른 질문에도 사용하려면) RunnablePassthrough 가 있어야 함\n",
    "    |prompt_template\n",
    "    |llm\n",
    "    |StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ae8e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_1 : RAG란 무엇인가요\n",
      "answer : RAG(검색 증강 생성)는 검색 증강 생성 기술로, 사용자 질문을 임베딩 벡터로 변환한 후, 벡터 데이터베이스에서 유사한 문서를 검색하여, 검색된 문서를 컨텍스트로 사용해 LLM이 답변을 생성하는 방식입니다. RAG의 장점으로는 최신 정보를 반영할 수 있고, 환각(Hallucination)을 감소시키며, 출처를 명시할 수 있는 점이 있습니다.\n",
      "sources : ['rag_concept.txt', 'rag_concept.txt', 'rag_concept.txt']\n",
      "question_2 : LangGrraph의 핵심 개념을 설명해주세요\n",
      "answer : LangGraph의 핵심 개념은 다음과 같습니다:\n",
      "\n",
      "1. **State(상태)**: 에이전트의 현재 상태를 나타내는 데이터 구조로, 에이전트가 작업을 수행하는 데 필요한 정보를 담고 있습니다.\n",
      "\n",
      "2. **Node(노드)**: 실제 작업을 수행하는 함수로, 에이전트가 특정 작업을 실행할 때 호출되는 단위입니다.\n",
      "\n",
      "이러한 개념을 바탕으로 LangGraph는 순환(Cycle)을 지원하여 복잡한 에이전트 워크플로우를 구현할 수 있습니다. LangGraph는 또한 LangChain 위에 구축된 상태 기반 에이전트 프레임워크입니다.\n",
      "sources : ['langgraph_intro.txt', 'langgraph_intro.txt', 'langgraph_intro.txt']\n",
      "question_3 : 프롬프트 엔지니어링 기법에는 어떤 것들이 있나요?\n",
      "answer : 프롬프트 엔지니어링 기법에는 다음과 같은 것들이 있습니다:\n",
      "\n",
      "1. 프롬프트 템플릿 관리 및 최적화: 효과적인 프롬프트를 만들기 위해 템플릿을 관리하고 최적화하는 기술입니다.\n",
      "2. 체인(Chains): 여러 구성 요소를 연결하여 파이프라인을 형성하는 기법입니다.\n",
      "3. 메모리(Memory): 대화의 맥락을 유지하기 위한 메모리 시스템을 활용하는 방법입니다.\n",
      "\n",
      "이러한 기법들은 LLM에게 명확하고 구체적인 지시를 내리는 데 도움을 줍니다.\n",
      "sources : ['prompt_engineering.txt', 'langchain_intro.txt', 'prompt_engineering.txt']\n"
     ]
    }
   ],
   "source": [
    "test_question = [\n",
    "    'RAG란 무엇인가요',\n",
    "    'LangGrraph의 핵심 개념을 설명해주세요',\n",
    "    '프롬프트 엔지니어링 기법에는 어떤 것들이 있나요?'\n",
    "]\n",
    "def ask_question(question):\n",
    "    '''질문에 대한 답변 생성'''\n",
    "    answer = rag_chain.invoke(question)\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    sources = [os.path.basename(doc.metadata.get('source', 'unknown')) for doc in retrieved_docs]\n",
    "    return answer, sources\n",
    "\n",
    "# 각 질문에 대해 답변 생성\n",
    "for i,question in  enumerate(test_question,1):\n",
    "    print(f'question_{i} : {question}')\n",
    "    answer,sources = ask_question(question)\n",
    "    print(f'answer : {answer}')\n",
    "    print(f'sources : {sources}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
