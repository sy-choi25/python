{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a26ee1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Query Transformation  (질문 변화) - 검색 최적화\n",
    "# 2. Multi-Query            (다중 질의) - 검색 범위 확대\n",
    "# 3. Self-RAG               (자기 보정) - 문서 관련성 평가\n",
    "# 4. Contextual Compressioin (문맥 압축) - 관련 부분만 추출\n",
    "# 5. Fusion Retrieval       (융합 검색) - 키워드 + 벡터 검색 결합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29c38b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "warnings.filterwarnings('ignore')\n",
    "load_dotenv()\n",
    "api_key = os.environ.get('OPENAI_API_KEY')\n",
    "if not api_key :\n",
    "    raise ValueError('OPENAI_API_KEY')\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cf29a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서로드\n",
    "# 텍스트 분할 - 청킹\n",
    "# 임베딩 및 VectorDB\n",
    "# 리트리버\n",
    "# LLM 설정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "057071a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs path : c:\\python_src\\6.openAI\\6\n",
      "읽은 문서의 수 5\n"
     ]
    }
   ],
   "source": [
    "# 문서로드\n",
    "# paths = 'C:/python_src/6.openAI/6'\n",
    "\n",
    "script_dir = os.getcwd()\n",
    "docs_path = os.path.join(script_dir)\n",
    "print(f'docs path : {docs_path}')\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    docs_path,\n",
    "    glob = '**/*.txt',\n",
    "    loader_cls = TextLoader,\n",
    "    loader_kwargs={'encoding':'utf-8'},  # 한국어면 꼭 써줄 것\n",
    ")\n",
    "document = loader.load()\n",
    "print(f'읽은 문서의 수 {len(document)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8ec44e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "청킹개수 : 20\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 분할 - 청킹\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "     chunk_size=100,\n",
    "     chunk_overlap=50,\n",
    "     separators = ['\\n\\n','\\n','.',' ',''],\n",
    "     length_function=len\n",
    ")\n",
    "\n",
    "# 스플릿 = 청킹\n",
    "doc_splits = text_splitter.split_documents(document)\n",
    "print(f'청킹개수 : {len(doc_splits)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d823c6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 및 VectorDB\n",
    "embedding_model = OpenAIEmbeddings(model = 'text-embedding-3-small')\n",
    "\n",
    "vectorstroe = Chroma.from_documents(\n",
    "    documents= doc_splits,\n",
    "    collection_name='basic_rag_collection',\n",
    "    embedding=embedding_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7694f345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리트리버\n",
    "\n",
    "base_retriever = vectorstroe.as_retriever(\n",
    "    search_type = 'similarity',\n",
    "    search_kwargs = {'k':3}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6fc0b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setup complete\n"
     ]
    }
   ],
   "source": [
    "# LLM 설정\n",
    "\n",
    "llm = ChatOpenAI(model = 'gpt-4o-mini',temperature=0)\n",
    "print(f'setup complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a792b534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유틸리티 함수 # 프로그램에서 반복적으로 쓰이는 작은 기능을 재사용 가능하게 모아둔 함수\n",
    "def format_docs(docs):\n",
    "    '''문서를 문자열로 포멧팅'''\n",
    "    return '\\n\\n---\\n\\n'.join([doc.page_content for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264e8ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question : RAG란 무엇인가요\n",
      " 1. Query Transformation  (질문 변화) - 검색 최적화\n",
      "사용자 질문을 검색에 최적화된 형태로 변환합니다.\n",
      "\n",
      "원본 질문 : RAG란 무엇인가요\n",
      "transformed 질문 : RAG 정의 의미\n",
      "answer : RAG(검색 증강 생성, Retrieval-Augmented Generation)는 자연어 처리 기술로, 사용자의 질문에 대한 답변을 생성하기 위해 외부 데이터베이스에서 정보를 검색하고 이를 활용하는 방법입니다. RAG는 다음과 같은 장점을 가지고 있습니다:\n",
      "\n",
      "1. 최신 정보를 반영할 수 있어, 사용자가 원하는 최신 데이터를 제공할 수 있습니다.\n",
      "2. 환각(Hallucination) 현상을 줄여, 보다 정확하고 신뢰할 수 있는 답변을 생성합니다.\n",
      "3. 출처를 명시할 수 있어, 제공된 정보의 신뢰성을 높입니다.\n",
      "\n",
      "RAG의 작동 원리는 다음과 같습니다:\n",
      "\n",
      "1. 사용자의 질문을 임베딩 벡터로 변환합니다.\n",
      "2. 벡터 데이터베이스에서 유사한 문서를 검색합니다.\n",
      "3. 검색된 문서를 컨텍스트로 사용하여 대형 언어 모델(LLM)이 답변을 생성합니다.\n",
      "\n",
      "이러한 과정을 통해 RAG는 보다 정확하고 유용한 정보를 제공할 수 있습니다.  sources : ['rag_concept.txt', 'rag_concept.txt', 'rag_concept.txt']\n",
      "Question : LangGrraph의 핵심 개념을 설명해주세요\n",
      " 1. Query Transformation  (질문 변화) - 검색 최적화\n",
      "사용자 질문을 검색에 최적화된 형태로 변환합니다.\n",
      "\n",
      "원본 질문 : LangGrraph의 핵심 개념을 설명해주세요\n",
      "transformed 질문 : LangGraph 핵심 개념 설명\n",
      "answer : LangGraph의 핵심 개념은 다음과 같습니다:\n",
      "\n",
      "1. **State(상태)**: 에이전트의 현재 상태를 나타내는 데이터 구조로, 에이전트가 수행하는 작업이나 프로세스의 진행 상황을 추적하는 데 사용됩니다.\n",
      "\n",
      "2. **Node(노드)**: 실제 작업을 수행하는 함수로, 에이전트가 특정 작업을 실행할 때 호출되는 구성 요소입니다. 각 노드는 특정 기능이나 작업을 담당합니다.\n",
      "\n",
      "이러한 개념들은 LangChain 위에 구축된 LangGraph의 상태 기반 에이전트 프레임워크에서 복잡한 에이전트 워크플로우를 구현하는 데 중요한 역할을 합니다. LangGraph는 순환(Cycle)을 지원하여 다양한 작업을 효율적으로 처리할 수 있도록 돕습니다.  sources : ['langgraph_intro.txt', 'langgraph_intro.txt', 'langgraph_intro.txt']\n"
     ]
    }
   ],
   "source": [
    "# 질문 재작성 프롬프트\n",
    "rewrite_prompt=  ChatPromptTemplate.from_template('''\n",
    "다음 질문을 검색에 더 적합한 형태로 변환해 주세요.\n",
    "키워드 중심으로 명확하게 바꿔주세요\n",
    "변환된 검색어만 출력하세요\n",
    "\n",
    "원본 질문: {question}\n",
    "변환된 검색어:\n",
    "''')\n",
    "\n",
    "rewrite_chain =  rewrite_prompt | llm | StrOutputParser()\n",
    "\n",
    "# RAG프롬프트\n",
    "rag_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system','제공된 문맥을 바탕으로 한국어로 답변하세요'),\n",
    "    ('human', '문맥:\\n{context}\\n\\n질문:{question}\\n\\n답변:')\n",
    "])\n",
    "\n",
    "def query_transformation(question):\n",
    "    '''Query Transformation  (질문 변화) - 검색 최적화'''\n",
    "    print(' 1. Query Transformation  (질문 변화) - 검색 최적화')\n",
    "    print('사용자 질문을 검색에 최적화된 형태로 변환합니다.\\n')\n",
    "\n",
    "    # 1. 질문 변환\n",
    "    transformed = rewrite_chain.invoke({'qeustion' : question})\n",
    "    print(f'원본 질문 : {question}')\n",
    "    print(f'transformed 질문 : {transformed}')\n",
    "    \n",
    "    # 2. 변환된 질문으로 검색\n",
    "    docs = base_retriever.invoke(transformed)\n",
    "    context = format_docs(docs) # format_docs 여러 Document 객체를 LLM이 이해할 수 있는 문자열로 합치는 역할\n",
    "\n",
    "    answer_chain = rag_prompt | llm | StrOutputParser()\n",
    "    answer = answer_chain.invoke({'context':context, 'question':question})\n",
    "    return answer, [ os.path.basename(d.metadata.get('source','unknown')) for d in docs ]\n",
    "\n",
    "test_question = [\n",
    "    'RAG란 무엇인가요',\n",
    "    'LangGrraph의 핵심 개념을 설명해주세요']\n",
    "\n",
    "for q in test_question:\n",
    "    print(f'Question : {q}')\n",
    "    answer, sources = query_transformation(q)\n",
    "    print(f'answer : {answer}  sources : {sources}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de44582a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question : LangChain 시작하는 방법\n",
      "LangChain 사용법 튜토리얼  \n",
      "\n",
      "LangChain 시작하기 위한 기본 가이드  \n",
      "\n",
      "LangChain 설치 및 설정 방법  \n",
      "검색된 문서의 개수 : 3\n",
      "answer : LangChain을 시작하는 방법은 다음과 같습니다:\n",
      "\n",
      "1. **환경 설정**: Python이 설치된 환경을 준비합니다. 필요한 패키지를 설치하기 위해 `pip`를 사용합니다. 예를 들어, LangChain을 설치하려면 다음 명령어를 실행합니다:\n",
      "   ```bash\n",
      "   pip install langchain\n",
      "   ```\n",
      "\n",
      "2. **모델 선택**: LangChain은 다양한 LLM 제공자와 통합되어 있습니다. OpenAI, Anthropic 등에서 제공하는 모델 중 하나를 선택하고 API 키를 준비합니다.\n",
      "\n",
      "3. **프롬프트 설정**: 프롬프트 템플릿을 관리하고 최적화하기 위해 필요한 프롬프트를 정의합니다. 이를 통해 모델이 원하는 방식으로 응답하도록 유도할 수 있습니다.\n",
      "\n",
      "4. **애플리케이션 개발**: LangChain의 구성 요소를 활용하여 애플리케이션을 개발합니다. 모델과 프롬프트를 결합하여 원하는 기능을 구현합니다.\n",
      "\n",
      "5. **테스트 및 배포**: 개발한 애플리케이션을 테스트하고, 필요에 따라 수정한 후 배포합니다.\n",
      "\n",
      "이 과정을 통해 LangChain을 활용한 애플리케이션 개발을 시작할 수 있습니다. 추가적인 문서나 튜토리얼을 참고하면 더 많은 정보를 얻을 수 있습니다.\n",
      "answer : ['langchain_intro.txt', 'langchain_intro.txt', 'langgraph_intro.txt']\n"
     ]
    }
   ],
   "source": [
    "# 2. Multi-Query            (다중 질의) - 검색 범위 확대    # 쿼리는 검색 엔진이나 벡터 DB에 넣는 검색어를 말함\n",
    "# 다중 쿼리 생성 프롬프트\n",
    "multi_query_prompt = ChatPromptTemplate.from_template('''\n",
    "다음질문에 대해 3가지 다른 관점의 검색 쿼리를 생성하세요.\n",
    "각쿼리는 세 줄로 구분하여 출력하세요        \n",
    "번호나 설명 없이 쿼리만 출력하세요\n",
    "원본질문 : {question}\n",
    "다른 관점의 쿼리들 ''')\n",
    "                                # 세 줄로 구분하여, 3가지 다른 관점의~~ => 다중 쿼리를 만드는 프롬프트\n",
    "# lag chain 구성 LCEL\n",
    "multi_query_chain = multi_query_prompt | llm | StrOutputParser()\n",
    "\n",
    "def multi_query_rag(question):\n",
    "    '''다중 쿼리로 검색해서 결과 통일'''\n",
    "    # multi_query_chain 실행해서 결과출력\n",
    "    queries_text = multi_query_chain.invoke( {'question': question} )\n",
    "    print(queries_text)\n",
    "    queries = [q.strip() for q in queries_text.strip().split('\\n')  if q.strip()]   # 공백제거, 줄띄움을 기준으로 나눠준다\n",
    "    # 각 쿼리(질문)으로 검색하고 결과를 통합(중복제거)\n",
    "    all_docs = [] # 각각의 답을 리스트에 담는다\n",
    "    seen_contents = set()\n",
    "    for query in queries:   # 각 3개의 쿼리로 검색 실행\n",
    "        docs = base_retriever.invoke(query)  # 각 3개의 쿼리들을 리트리버에서 찾기. 각각 따로 작동한다\n",
    "        for doc in docs: \n",
    "            if doc.page_content not in seen_contents:   # document에는 page_content(속성에 본문 내용),metadata(속성출처) 의 두가지 정보를 가진다/ page_content는 여기에서 문서 실제 내용을 불러오는 것\n",
    "                seen_contents.add(doc.page_content) # 중복된 내용 체크\n",
    "                all_docs.append(doc)                # 중복을 제외하고 저장\n",
    "\n",
    "    print(f'검색된 문서의 개수 : {len(all_docs)}')\n",
    "    # 리트리버 답변 생성 상위 3개만 사용\n",
    "    context = format_docs(all_docs[:3])\n",
    "    answer_chain = rag_prompt | llm | StrOutputParser()\n",
    "    answer = answer_chain.invoke({'context' : context, 'question':question})    # {context} = 위에서 만든 3개 문서 합친 내용\n",
    "    return answer, [ os.path.basename(d.metadata.get('source','unknown')) for d in all_docs ]\n",
    "     \n",
    "test = [\n",
    "    'LangChain 시작하는 방법'\n",
    "]\n",
    "for q in test:\n",
    "    print(f'question : {q}')\n",
    "    answer, sources = multi_query_rag(q)\n",
    "    print(f'answer : {answer}')\n",
    "    print(f'answer : {sources}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe5420c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.Self-RAG\n",
      "검색된 문서의 관련성을 평가하여 필터링합니다.\n",
      "\n",
      "리트리버가 찾은 문서 수 : 3개\n",
      "    -RAG의 장점:\n",
      "- 최신 정보를 반영할 수 있음\n",
      "- 환각(Hallucination) 감소\n",
      ".... : Relevent\n",
      "    -RAG의 작동 원리:\n",
      "1. 사용자 질문을 임베딩 벡터로 변환\n",
      "2. 벡터 데이터베이스에서 유.... : Not Relevent\n",
      "    -RAG (Retrieval-Augmented Generation)는 검색 증강 생성 기술입.... : Relevent\n",
      "관련 문서 수 : 2개\n",
      " 답변 : messages=[SystemMessage(content='제공된 문맥을 바탕으로 한국어로 답변하세요', additional_kwargs={}, response_metadata={}), HumanMessage(content='문맥:\\nRAG의 장점:\\n- 최신 정보를 반영할 수 있음\\n- 환각(Hallucination) 감소\\n- 출처를 명시할 수 있음\\n\\n---\\n\\nRAG (Retrieval-Augmented Generation)는 검색 증강 생성 기술입니다.\\n\\n질문:RAG의 장점은 무엇인가요?\\n\\n답변:', additional_kwargs={}, response_metadata={})]\n",
      " 근거 : ['rag_concept.txt', 'rag_concept.txt']\n"
     ]
    }
   ],
   "source": [
    "# 3. Self-RAG               (자기 보정) - 문서 관련성 평가\n",
    "\n",
    "print(f'3.Self-RAG')\n",
    "print(f'검색된 문서의 관련성을 평가하여 필터링합니다.\\n')\n",
    "\n",
    "# 프롬프트 \n",
    "check_prompt = ChatPromptTemplate.from_template(''' \n",
    "다음 문서가 다음 질문에 관련이 있는지 평가하세요\n",
    "'YES'또는 'NO'로만 대답하세요\n",
    "\n",
    "문서: {document}\n",
    "질문: {question}\n",
    "관련성: \n",
    " ''')\n",
    "# LCEL 체인 구성\n",
    "check_prompt_chain = check_prompt | llm | StrOutputParser()\n",
    "\n",
    "def filter_relevant_docs(docs, question):   # docs → 리트리버가 검색해서 가져온 문서 리스트/question 사용자가 입력한 질문 문자열\n",
    "    '''관련 있는 문서만 필터링'''\n",
    "    relevant = []       # 필터링된 문서를 담을 리스트\n",
    "    for doc in docs :   # docs 리스트를 하나씩 가져옴/ # doc → Document 객체\n",
    "        result = check_prompt_chain.invoke({    # llm은 \n",
    "        'document': doc.page_content,\n",
    "        'question' : question})\n",
    "        is_relevant = 'yes' in result.lower()   # 문자열에 yes라는 글자가 있으면” 관련 있다고 판단\n",
    "        print(f\"    -{doc.page_content[:50]}.... : {\"Relevent\" if is_relevant else \"Not Relevent\"}\")\n",
    "        if is_relevant:\n",
    "            relevant.append(doc)\n",
    "    return relevant\n",
    "\n",
    "\n",
    "# 1) 문서를 검색(리트리버를 이용해서)\n",
    "question = \"RAG의 장점은 무엇인가요?\"\n",
    "docs = base_retriever.invoke(question)\n",
    "print(f\"리트리버가 찾은 문서 수 : {len(docs)}개\")\n",
    "\n",
    "# 2) 관련성 평가\n",
    "relevant_docs = filter_relevant_docs(docs, question)\n",
    "print(f\"관련 문서 수 : {len(relevant_docs)}개\")\n",
    "\n",
    "if not relevant_docs:\n",
    "    raise ValueError('관련있는 문서가 없어서 답변을 종료합니다.다른 질문을 입력하세요')\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    '''문서를 문자열로 포멧팅'''\n",
    "    return '\\n\\n---\\n\\n'.join([ doc.page_content for doc in docs ])\n",
    "\n",
    "context = format_docs(relevant_docs)\n",
    "\n",
    "#답변 생성\n",
    "#RAG프롬프트\n",
    "rag_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system','제공된 문맥을 바탕으로 한국어로 답변하세요'),\n",
    "    ('human', '문맥:\\n{context}\\n\\n질문:{question}\\n\\n답변:')\n",
    "])\n",
    "answer_chain = rag_prompt | llm | StrOutputParser()\n",
    "answer = rag_prompt.invoke({'context' : context, 'question' : question})\n",
    "print(f' 답변 : {answer}')\n",
    "sources = [ os.path.basename(doc.metadata.get('source',\"\")) for doc in relevant_docs]\n",
    "print(f' 근거 : {sources}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "bdc191a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VectorDB의 종류에는 여러 가지가 있습니다. 대표적인 예로는 다음과 같은 것들이 있습니다:\n",
      "\n",
      "1. **ChromaDB**: 로컬 개발에 적합한 오픈소스 솔루션으로, 개발자들이 쉽게 사용할 수 있도록 설계되었습니다.\n",
      "2. **Pinecone**: 완전 관리형 클라우드 서비스로, 대규모 데이터 처리와 빠른 검색 성능을 제공합니다.\n",
      "3. **Faiss**: Facebook에서 개발한 라이브러리로, 대규모 벡터 검색을 위한 효율적인 방법을 제공합니다.\n",
      "4. **Annoy**: Spotify에서 개발한 라이브러리로, 근사 최근접 이웃 검색을 위한 효율적인 구조를 가지고 있습니다.\n",
      "5. **Milvus**: 오픈소스 벡터 데이터베이스로, 대규모 데이터셋을 처리하고 실시간 검색을 지원합니다.\n",
      "\n",
      "이 외에도 다양한 VectorDB 솔루션이 있으며, 각기 다른 용도와 기능을 가지고 있습니다. ['vectordb_intro.txt', 'vectordb_intro.txt', 'langchain_intro.txt']\n"
     ]
    }
   ],
   "source": [
    "# 4. Contextual Compressioin (문맥 압축) - 관련 부분만 추출\n",
    "\n",
    "question = 'VectorDB의 종류를 알려주세요'\n",
    "\n",
    "# 1. 문맥압축 프롬프트를 실행\n",
    "compress_prompt = ChatPromptTemplate.from_template(\n",
    "'''\n",
    "다음 문서가 질문에 관련이 있는지 평가하세요.\n",
    "관련 없는 부분은 제외하고, 관련 있는 내용만 그대로 출력하세요.\n",
    "관련 내용이 없으면 '관련 없음'이라고 출력하세요\n",
    "\n",
    "문서: {document}\n",
    "질문: {question}\n",
    "관련성:                                                                                                                                    \n",
    "'''\n",
    "\n",
    ")\n",
    "\n",
    "docs = base_retriever.invoke(question)\n",
    "compressed = []\n",
    "sources = []\n",
    "for doc in docs: \n",
    "    document = doc.page_content\n",
    "    compress_chain = compress_prompt | llm | StrOutputParser()\n",
    "    compress_result = compress_chain.invoke({\"question\":question, \"document\":document})\n",
    "\n",
    "    if '관련없음' not in compress_result:\n",
    "        compressed.append(compress_result)\n",
    "        sources.append(os.path.basename(doc.metadata.get('source',\"\")))\n",
    "context = '\\n\\n---\\n\\n'.join(compressed)\n",
    "\n",
    "# 최종답변\n",
    "# RAG프롬프트\n",
    "rag_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system','제공된 문맥을 바탕으로 한국어로 답변하세요'),\n",
    "    ('human', '문맥:\\n{context}\\n\\n질문:{question}\\n\\n답변:')\n",
    "])\n",
    "rag_prompt_chain = rag_prompt | llm | StrOutputParser()\n",
    "result = rag_prompt_chain.invoke({'context': context, 'question' : question})\n",
    "print(result,sources)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b8c0f118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fusion docs 결과 상위 3개 : {sorted_docs[:3]}\n",
      "VectorDB의 주요 종류는 다음과 같습니다:\n",
      "\n",
      "1. **ChromaDB**: 로컬 개발에 적합한 오픈소스 솔루션.\n",
      "2. **Pinecone**: 클라우드 기반의 벡터 데이터베이스로, 확장성과 성능이 뛰어남.\n",
      "3. **Weaviate**: 그래프 기반의 벡터 데이터베이스로, 스키마리스 구조를 지원.\n",
      "4. **Milvus**: 대규모 벡터 데이터를 처리할 수 있는 오픈소스 솔루션.\n",
      "5. **Faiss**: Facebook에서 개발한 벡터 검색 라이브러리로, 대량의 벡터를 효율적으로 검색할 수 있음.\n",
      "\n",
      "이 외에도 다양한 VectorDB 솔루션이 있으며, 각 솔루션은 특정 용도와 요구 사항에 맞춰 선택할 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# 5. Fusion Retrieval       (융합 검색) - 키워드 + 벡터 검색 결합\n",
    "\n",
    "# 1. 개별 검색 결과 비교\n",
    "\n",
    "# 2. 융합 결과로 답변 생성\n",
    "\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "# BM25 리트리버   : 키워드 기반\n",
    "# Vector 리트리버 : 의미기반\n",
    "bm25_retriever = BM25Retriever.from_documents(doc_splits)\n",
    "bm25_retriever.k = 3\n",
    "\n",
    "question = 'VectorDB의 종류를 알려주세요'\n",
    "# 벡터 검색\n",
    "vector_docs = base_retriever.invoke(question)\n",
    "#bm25 검색\n",
    "bm25_docs = bm25_retriever.invoke(question)\n",
    "fusion_scores = {}\n",
    "# 벡터 검색 결과 점수화\n",
    "for rank, doc in enumerate(vector_docs):\n",
    "    doc_key = doc.page_content[:50]\n",
    "    score =1/(60+rank)\n",
    "    fusion_scores[doc_key] = fusion_scores.get(doc_key,0) + score\n",
    "# bm25 검색 결과 점수화\n",
    "for rank, doc in enumerate(bm25_docs):\n",
    "    doc_key = doc.page_content[:50]\n",
    "    1/(60+rank)\n",
    "    fusion_scores[doc_key] = fusion_scores.get(doc_key,0) + score\n",
    "# 점수로 정렬\n",
    "sorted_docs = sorted(\n",
    "    fusion_scores.items(),key=lambda x : x[1], reverse = True\n",
    ")\n",
    "print('fusion docs 결과 상위 3개 : {sorted_docs[:3]}')\n",
    "docs = []\n",
    "for doc,score in sorted_docs[:3]:\n",
    "    docs.append(doc)\n",
    "\n",
    "inputs = '\\n\\n---\\n\\n'.join(docs)\n",
    "\n",
    "rag_prompt_chain = rag_prompt | llm | StrOutputParser()\n",
    "result = rag_prompt_chain.invoke({'context' : inputs, 'question' : question})\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
