{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61b4ac04",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e46fa1b",
   "metadata": {},
   "source": [
    "<span style=\"font-size:13px;\">\n",
    "\n",
    "▶ LSTM\n",
    "- RNN의 단기기억 문제를 하기 위한 모델, RNN보다 휠씬 기억력이 좋음\n",
    "- 오래 기억할 건 오래 기억하고, 짧게 기억하는 건 짧게 기억\n",
    "\n",
    "▶ LSTM의 핵심 원리 '게이트'\n",
    "- <span style=\"color: yellow;\">Forget Gate</span> (망각 게이트): \"어떤 정보를 버릴까?\"\n",
    "- <span style=\"color: yellow;\">Input Gate</span> (입력 게이트): \"어떤 새로운 정보를 저장할까?\"\n",
    "    - 새로운 정보가 들어올 때 저장할 가치가 있는지 판단\n",
    "- <span style=\"color: yellow;\">Output Gate</span> (출력 게이트): \"무엇을 출력(예측)할까?\n",
    "    - 지금 당장 필요한 정보가 뭔지 판단하여 결과를 출력\n",
    "\n",
    "▶ LSTM의 과정\n",
    "1. 데이터 준비\n",
    "    1) 데이터 수집\n",
    "    2) 데이터 전처리 : 특수문자, 오타, 공백 등을 제거\n",
    "    3) 토큰화 및 단어사전 구축 : \n",
    "        - 문장을 단어, 형태소 등 의미 있는 최소단위(토큰)으로 자른다\n",
    "        - 데이터에 등장하는 모든 단어들을 '단어사전'으로 만들고, 각 단어들은 고유번호(인덱스)가 부여된다\n",
    "    4) 데이터셋 구성 : 입력(문제) 와 정답 쌍으로 데이터를 만들어 준다\n",
    "    \n",
    "2. 모델학습\n",
    "    1) 모델 설계 : 어떤 종류의 신경망(예: LSTM)을 사용할지 결정하고, 계층 수, 뉴런 수 등 모델의 구조를 코드로 정의\n",
    "    2) 학습 환경 설정 : \n",
    "        - 모델의 예측이 얼마나 틀렸는지 계산할 **손실 함수(Loss Function)**와, 틀린 값을 바탕으로 모델을 어떻게 개선할지 정하는 **옵티마이저(Optimizer)**를 선택\n",
    "    3) 학습 진행 : [모델 예측 → 정답과 비교하여 오차 계산 → 오차를 줄이는 방향으로 모델 업데이트] 과정을 수없이 반복\n",
    "    \n",
    "3. 결과 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d028bf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f689e082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SAMSUNG\\.cache\\kagglehub\\datasets\\aashita\\nyt-comments\\versions\\13\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download('aashita/nyt-comments')\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad725a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "article_lists = glob(path+'/*.*',recursive = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52230fb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>articleID</th>\n",
       "      <th>articleWordCount</th>\n",
       "      <th>byline</th>\n",
       "      <th>documentType</th>\n",
       "      <th>headline</th>\n",
       "      <th>keywords</th>\n",
       "      <th>multimedia</th>\n",
       "      <th>newDesk</th>\n",
       "      <th>printPage</th>\n",
       "      <th>pubDate</th>\n",
       "      <th>sectionName</th>\n",
       "      <th>snippet</th>\n",
       "      <th>source</th>\n",
       "      <th>typeOfMaterial</th>\n",
       "      <th>webURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>58def1347c459f24986d7c80</td>\n",
       "      <td>716</td>\n",
       "      <td>By STEPHEN HILTNER and SUSAN LEHMAN</td>\n",
       "      <td>article</td>\n",
       "      <td>Finding an Expansive View  of a Forgotten Peop...</td>\n",
       "      <td>['Photography', 'New York Times', 'Niger', 'Fe...</td>\n",
       "      <td>3</td>\n",
       "      <td>Insider</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-04-01 00:15:41</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>One of the largest photo displays in Times his...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2017/03/31/insider/nig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>58def3237c459f24986d7c84</td>\n",
       "      <td>823</td>\n",
       "      <td>By GAIL COLLINS</td>\n",
       "      <td>article</td>\n",
       "      <td>And Now,  the Dreaded Trump Curse</td>\n",
       "      <td>['United States Politics and Government', 'Tru...</td>\n",
       "      <td>3</td>\n",
       "      <td>OpEd</td>\n",
       "      <td>23</td>\n",
       "      <td>2017-04-01 00:23:58</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Meet the gang from under the bus.</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>Op-Ed</td>\n",
       "      <td>https://www.nytimes.com/2017/03/31/opinion/and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>58def9f57c459f24986d7c90</td>\n",
       "      <td>575</td>\n",
       "      <td>By THE EDITORIAL BOARD</td>\n",
       "      <td>article</td>\n",
       "      <td>Venezuela’s Descent Into Dictatorship</td>\n",
       "      <td>['Venezuela', 'Politics and Government', 'Madu...</td>\n",
       "      <td>3</td>\n",
       "      <td>Editorial</td>\n",
       "      <td>22</td>\n",
       "      <td>2017-04-01 00:53:06</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>A court ruling annulling the legislature’s aut...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>Editorial</td>\n",
       "      <td>https://www.nytimes.com/2017/03/31/opinion/ven...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>58defd317c459f24986d7c95</td>\n",
       "      <td>1374</td>\n",
       "      <td>By MICHAEL POWELL</td>\n",
       "      <td>article</td>\n",
       "      <td>Stain Permeates Basketball Blue Blood</td>\n",
       "      <td>['Basketball (College)', 'University of North ...</td>\n",
       "      <td>3</td>\n",
       "      <td>Sports</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-04-01 01:06:52</td>\n",
       "      <td>College Basketball</td>\n",
       "      <td>For two decades, until 2013, North Carolina en...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2017/03/31/sports/ncaa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>58df09b77c459f24986d7ca7</td>\n",
       "      <td>708</td>\n",
       "      <td>By DEB AMLEN</td>\n",
       "      <td>article</td>\n",
       "      <td>Taking Things for Granted</td>\n",
       "      <td>['Crossword Puzzles']</td>\n",
       "      <td>3</td>\n",
       "      <td>Games</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-04-01 02:00:14</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>In which Howard Barkin and Will Shortz teach u...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2017/03/31/crosswords/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  abstract                 articleID  articleWordCount  \\\n",
       "0      NaN  58def1347c459f24986d7c80               716   \n",
       "1      NaN  58def3237c459f24986d7c84               823   \n",
       "2      NaN  58def9f57c459f24986d7c90               575   \n",
       "3      NaN  58defd317c459f24986d7c95              1374   \n",
       "4      NaN  58df09b77c459f24986d7ca7               708   \n",
       "\n",
       "                                byline documentType  \\\n",
       "0  By STEPHEN HILTNER and SUSAN LEHMAN      article   \n",
       "1                      By GAIL COLLINS      article   \n",
       "2               By THE EDITORIAL BOARD      article   \n",
       "3                    By MICHAEL POWELL      article   \n",
       "4                         By DEB AMLEN      article   \n",
       "\n",
       "                                            headline  \\\n",
       "0  Finding an Expansive View  of a Forgotten Peop...   \n",
       "1                  And Now,  the Dreaded Trump Curse   \n",
       "2              Venezuela’s Descent Into Dictatorship   \n",
       "3              Stain Permeates Basketball Blue Blood   \n",
       "4                          Taking Things for Granted   \n",
       "\n",
       "                                            keywords  multimedia    newDesk  \\\n",
       "0  ['Photography', 'New York Times', 'Niger', 'Fe...           3    Insider   \n",
       "1  ['United States Politics and Government', 'Tru...           3       OpEd   \n",
       "2  ['Venezuela', 'Politics and Government', 'Madu...           3  Editorial   \n",
       "3  ['Basketball (College)', 'University of North ...           3     Sports   \n",
       "4                              ['Crossword Puzzles']           3      Games   \n",
       "\n",
       "   printPage              pubDate         sectionName  \\\n",
       "0          2  2017-04-01 00:15:41             Unknown   \n",
       "1         23  2017-04-01 00:23:58             Unknown   \n",
       "2         22  2017-04-01 00:53:06             Unknown   \n",
       "3          1  2017-04-01 01:06:52  College Basketball   \n",
       "4          0  2017-04-01 02:00:14             Unknown   \n",
       "\n",
       "                                             snippet              source  \\\n",
       "0  One of the largest photo displays in Times his...  The New York Times   \n",
       "1                  Meet the gang from under the bus.  The New York Times   \n",
       "2  A court ruling annulling the legislature’s aut...  The New York Times   \n",
       "3  For two decades, until 2013, North Carolina en...  The New York Times   \n",
       "4  In which Howard Barkin and Will Shortz teach u...  The New York Times   \n",
       "\n",
       "  typeOfMaterial                                             webURL  \n",
       "0           News  https://www.nytimes.com/2017/03/31/insider/nig...  \n",
       "1          Op-Ed  https://www.nytimes.com/2017/03/31/opinion/and...  \n",
       "2      Editorial  https://www.nytimes.com/2017/03/31/opinion/ven...  \n",
       "3           News  https://www.nytimes.com/2017/03/31/sports/ncaa...  \n",
       "4           News  https://www.nytimes.com/2017/03/31/crosswords/...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(article_lists[0]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113d08cf",
   "metadata": {},
   "source": [
    "<span style=\"font-size:12px;\">\n",
    "\n",
    "### ▶ LSTM\n",
    "- 입력 : \"나는 파이썬을 좋아합니다. 따라서 나는 __ 을 잘합니다.\"  \n",
    "- 일반 신경망 : 공부( 파이썬 정보가 희석.. 잊어버림)  \n",
    "- LSTM : 프로그래밍(오래된 정보도 기억)  \n",
    " \n",
    " - 핵심 키워드 \n",
    "    - 장기기억: 중요한 정보는 오래기억  \n",
    "    - 단기기억 : 불필요한 정보는 버림  \n",
    "    - 순서이해 : 시간순서 이해  \n",
    "##### ▷ 3개의 GATE를 통해 정보의 흐름을 제어\n",
    "- LSTM 셀 (한 시점 t)  \n",
    "    - 입력 : $x_t$(현재 데이터)    \n",
    "    - 이전 은닉상태 : $h_{t-1}$     -> 단기기억이자 해당 시점의 출력(결과물)\n",
    "    - 이전 셀 상태 : $c_{t-1}$      -> 장기기억을 담당 \n",
    "    > \n",
    "    --> forget gate --> input gate --> output gate  \n",
    "        잊을 데이터/    추가할 데이터/   출력할 데이터  \n",
    "    \n",
    "        - 출력 $h_t$, $c_t$\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4898b702",
   "metadata": {},
   "source": [
    "<span style=\"font-size:12px;\">\n",
    "\n",
    "### ▶ Forget Gate(잊은 관문)\n",
    "\n",
    "\n",
    "$f_t = s(w_f . [h_{t-1}, x_t ] +b_f) $  \n",
    "\n",
    "$s$ : sigmoid 함수(0~1 사이의 값) -> 1에 가까우면 정보를 모두 통과시키고, 0에 가까우면 정보를 모두 차단  \n",
    "\n",
    "이전 셀상태 : [1.5, 0.3, 2.1]  \n",
    "현재입력 : '새로운 문장 입력'\n",
    "\n",
    "$f_t$ = [0.1, 0.05, 0.9]  \n",
    "결과 : [1.5 * 0.1, -0.3 * 0.05, 2.1 * 0.9] = [0.15, -0.015, 1.89]  \n",
    "-> 처음 2개는 버리고 3번째는 유지"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ef1914",
   "metadata": {},
   "source": [
    "<span style=\"font-size:12px;\">\n",
    "\n",
    "### ▶ InputGate : 입력\n",
    "- 첫 번째는 70% 받고 두번째는 30% 받아라\n",
    "\n",
    "### Cell state 업데이트\n",
    "- 이전 기억에서 필요한 것만 유지하고 새로운 정보에서 필요한 것만 유지\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1afe55",
   "metadata": {},
   "source": [
    "input 게이트\n",
    "\n",
    "시점 t에서:\n",
    "\n",
    " : 현재 입력 데이터 (벡터)\n",
    " : 이전 시점의 은닉 상태 (벡터)\n",
    " : 이전 시점의 셀 상태 (벡터) ← 장기 기억!\n",
    ", \n",
    " : 가중치 행렬\n",
    " : 편향 벡터\n",
    " = [0.2, -0.5, 0.8] (3개 입력 피처)\n",
    " = [0.1, 0.3, -0.2, 0.5] (4개 은닉)\n",
    " = [0.4, -0.1, 0.6, 0.2] (4개 셀 상태)\n",
    "Forget Gate ( \n",
    " ) - 어제 기억을 얼마나 유지할까\n",
    "\n",
    " = \n",
    "1단계: \n",
    " 과 \n",
    " 를 연결 (concatenate)\n",
    "\n",
    " = [0.1, 0.3, -0.2, 0.5, 0.2, -0.5, 0.8]\n",
    "\n",
    "           $h_{t-1}$    $x_t$\n",
    "\n",
    "                    4개              3개\n",
    "\n",
    "                           ↓\n",
    "\n",
    "                       총 7개 벡터\n",
    "2단계: 가중치 행렬 곱하기 \n",
    "\n",
    " 는 크기: (4, 7) 행렬 (왜 (4, 7)? → 은닉 크기 4개, 입력 7개)\n",
    "\n",
    "결과: 4개의 값\n",
    "\n",
    "3단계: 편향 더하기\n",
    "\n",
    " (크기: 4)\n",
    "결과: 4개의 값\n",
    "\n",
    "4단계: Sigmoid 함수 적용 \n",
    " = \n",
    " \n",
    "\n",
    "이 함수는 모든 값을 0~1 사이로 변환!\n",
    "\n",
    "\n",
    "의미:\n",
    "\n",
    "첫 번째 셀 상태: 30% 유지 (70% 잊음)\n",
    "두 번째 셀 상태: 80% 유지 (20% 잊음)\n",
    "세 번째 셀 상태: 10% 유지 (90% 잊음)\n",
    "네 번째 셀 상태: 90% 유지 (10% 잊음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "748adc48",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'headline'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m articles:\n\u001b[32m      8\u001b[39m     df = pd.read_csv(a)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     \u001b[43ma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheadline\u001b[49m.values\n\u001b[32m     10\u001b[39m     all_headline.extend( a.headline.values )\n",
      "\u001b[31mAttributeError\u001b[39m: 'str' object has no attribute 'headline'"
     ]
    }
   ],
   "source": [
    "\n",
    "all_headline = []\n",
    "articles = [path for path in article_lists if \"Articles\" in path]\n",
    "# headline 정보만 추출 all_headline에 추가\n",
    "# 전처리 : 소문자로 변경하고 특수문자 제거\n",
    "import string\n",
    "\n",
    "for a in articles:\n",
    "    df = pd.read_csv(a)\n",
    "    a.headline.values\n",
    "    all_headline.extend( a.headline.values )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90fe992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'finding': 0,\n",
       " 'an': 1,\n",
       " 'expansive': 2,\n",
       " 'view': 3,\n",
       " 'of': 4,\n",
       " 'a': 5,\n",
       " 'forgotten': 6,\n",
       " 'people': 7,\n",
       " 'in': 8,\n",
       " 'niger': 9,\n",
       " 'and': 10,\n",
       " 'now,': 11,\n",
       " 'the': 12,\n",
       " 'dreaded': 13,\n",
       " 'trump': 14,\n",
       " 'curse': 15,\n",
       " 'venezuela’s': 16,\n",
       " 'descent': 17,\n",
       " 'into': 18,\n",
       " 'dictatorship': 19,\n",
       " 'stain': 20,\n",
       " 'permeates': 21,\n",
       " 'basketball': 22,\n",
       " 'blue': 23,\n",
       " 'blood': 24,\n",
       " 'taking': 25,\n",
       " 'things': 26,\n",
       " 'for': 27,\n",
       " 'granted': 28,\n",
       " 'caged': 29,\n",
       " 'beast': 30,\n",
       " 'awakens': 31,\n",
       " 'ever-unfolding': 32,\n",
       " 'story': 33,\n",
       " 'o’reilly': 34,\n",
       " 'thrives': 35,\n",
       " 'as': 36,\n",
       " 'settlements': 37,\n",
       " 'add': 38,\n",
       " 'up': 39,\n",
       " 'mouse': 40,\n",
       " 'infestation': 41,\n",
       " 'divide': 42,\n",
       " 'g.o.p.': 43,\n",
       " 'now': 44,\n",
       " 'threatens': 45,\n",
       " 'tax': 46,\n",
       " 'plan': 47,\n",
       " 'variety': 48,\n",
       " 'puzzle:': 49,\n",
       " 'acrostic': 50,\n",
       " 'they': 51,\n",
       " 'can': 52,\n",
       " 'hit': 53,\n",
       " 'ball': 54,\n",
       " '400': 55,\n",
       " 'feet.': 56,\n",
       " 'but': 57,\n",
       " 'play': 58,\n",
       " 'catch?': 59,\n",
       " 'that’s': 60,\n",
       " 'tricky.': 61,\n",
       " 'country,': 62,\n",
       " 'shock': 63,\n",
       " 'at': 64,\n",
       " 'budget': 65,\n",
       " 'cuts': 66,\n",
       " 'why': 67,\n",
       " 'is': 68,\n",
       " 'this': 69,\n",
       " 'hate': 70,\n",
       " 'different': 71,\n",
       " 'from': 72,\n",
       " 'all': 73,\n",
       " 'other': 74,\n",
       " 'hate?': 75,\n",
       " 'pick': 76,\n",
       " 'your': 77,\n",
       " 'favorite': 78,\n",
       " 'ethical': 79,\n",
       " 'offender': 80,\n",
       " 'my': 81,\n",
       " 'son’s': 82,\n",
       " 'growing': 83,\n",
       " 'black': 84,\n",
       " 'pride': 85,\n",
       " 'jerks': 86,\n",
       " 'start-ups': 87,\n",
       " 'ruin': 88,\n",
       " 'needs': 89,\n",
       " 'brain': 90,\n",
       " 'manhood': 91,\n",
       " 'age': 92,\n",
       " 'value': 93,\n",
       " 'college': 94,\n",
       " 'initial': 95,\n",
       " 'description': 96,\n",
       " 'rough': 97,\n",
       " 'estimates': 98,\n",
       " 'el': 99,\n",
       " 'pasatiempo': 100,\n",
       " 'nacional': 101,\n",
       " 'cooling': 102,\n",
       " 'off': 103,\n",
       " 'on': 104,\n",
       " 'hot': 105,\n",
       " 'day': 106,\n",
       " 'yankee': 107,\n",
       " 'stadium': 108,\n",
       " 'trump’s': 109,\n",
       " 'staff': 110,\n",
       " 'mixed': 111,\n",
       " 'politics': 112,\n",
       " 'paydays': 113,\n",
       " 'virtuoso': 114,\n",
       " 'rebuilding': 115,\n",
       " 'act': 116,\n",
       " 'requires': 117,\n",
       " 'everyone': 118,\n",
       " 'tune': 119,\n",
       " '‘homeland,’': 120,\n",
       " 'season': 121,\n",
       " '6,': 122,\n",
       " 'episode': 123,\n",
       " '11:': 124,\n",
       " 'quinn': 125,\n",
       " 'just': 126,\n",
       " 'natural': 127,\n",
       " 'killer?': 128,\n",
       " '‘big': 129,\n",
       " 'little': 130,\n",
       " 'lies’': 131,\n",
       " 'art': 132,\n",
       " 'empathy': 133,\n",
       " 'upending': 134,\n",
       " 'whodunit': 135,\n",
       " '‘feud:': 136,\n",
       " 'bette': 137,\n",
       " 'joan’': 138,\n",
       " '5:': 139,\n",
       " 'stage': 140,\n",
       " '‘billions’': 141,\n",
       " '2,': 142,\n",
       " '7:': 143,\n",
       " 'greed': 144,\n",
       " 'good.': 145,\n",
       " 'except': 146,\n",
       " 'when': 147,\n",
       " 'it’s': 148,\n",
       " 'not.': 149,\n",
       " 'unknown': 150,\n",
       " 'what’s': 151,\n",
       " 'going': 152,\n",
       " 'picture?': 153,\n",
       " '|': 154,\n",
       " 'april': 155,\n",
       " '3,': 156,\n",
       " '2017': 157,\n",
       " 'have': 158,\n",
       " 'you': 159,\n",
       " 'ever': 160,\n",
       " 'felt': 161,\n",
       " 'pressured': 162,\n",
       " 'by': 163,\n",
       " 'family': 164,\n",
       " 'or': 165,\n",
       " 'others': 166,\n",
       " 'making': 167,\n",
       " 'important': 168,\n",
       " 'decision': 169,\n",
       " 'about': 170,\n",
       " 'future?': 171,\n",
       " 'cornerstone': 172,\n",
       " 'peace': 173,\n",
       " 'risk': 174,\n",
       " 'wimping': 175,\n",
       " 'out': 176,\n",
       " 'trade': 177,\n",
       " 'dwindling': 178,\n",
       " 'odds': 179,\n",
       " 'coincidence': 180,\n",
       " 'what': 181,\n",
       " 'was': 182,\n",
       " 'lenin': 183,\n",
       " 'thinking?': 184,\n",
       " 'mitch': 185,\n",
       " 'mcconnell’s': 186,\n",
       " 'trigger': 187,\n",
       " 'finger': 188,\n",
       " 'ad': 189,\n",
       " 'north': 190,\n",
       " 'korea': 191,\n",
       " 'yields': 192,\n",
       " 'nuclear': 193,\n",
       " 'clues': 194,\n",
       " 'good': 195,\n",
       " 'news': 196,\n",
       " 'older': 197,\n",
       " 'mothers': 198,\n",
       " 'does': 199,\n",
       " 'birth': 200,\n",
       " 'control': 201,\n",
       " 'cause': 202,\n",
       " 'depression?': 203,\n",
       " 'turning': 204,\n",
       " 'negative': 205,\n",
       " 'thinkers': 206,\n",
       " 'positive': 207,\n",
       " 'ones': 208,\n",
       " 'new': 209,\n",
       " 'york': 210,\n",
       " 'today:': 211,\n",
       " 'belated': 212,\n",
       " 'middlebury,': 213,\n",
       " 'divided': 214,\n",
       " 'campus': 215,\n",
       " 'democrats’': 216,\n",
       " 'vow': 217,\n",
       " 'to': 218,\n",
       " 'bar': 219,\n",
       " 'gorsuch': 220,\n",
       " 'sets': 221,\n",
       " 'clash': 222,\n",
       " 'h-1b': 223,\n",
       " 'visa': 224,\n",
       " 'applications': 225,\n",
       " 'pour': 226,\n",
       " 'truckload': 227,\n",
       " 'amplified': 228,\n",
       " 'world,': 229,\n",
       " 'critics': 230,\n",
       " 'take': 231,\n",
       " 'aim': 232,\n",
       " 'referees': 233,\n",
       " 'revered': 234,\n",
       " 'milwaukee': 235,\n",
       " 'restaurant,': 236,\n",
       " 'karl': 237,\n",
       " 'ratzsch,': 238,\n",
       " 'says': 239,\n",
       " 'goodbye': 240,\n",
       " 'n.h.l.': 241,\n",
       " 'its': 242,\n",
       " 'players': 243,\n",
       " 'will': 244,\n",
       " 'skip': 245,\n",
       " '2018': 246,\n",
       " 'olympics': 247,\n",
       " 'company': 248,\n",
       " 'classic': 249,\n",
       " 'nasty': 250,\n",
       " 'woman': 251,\n",
       " '22': 252,\n",
       " 'ways': 253,\n",
       " 'teach': 254,\n",
       " 'learn': 255,\n",
       " 'poetry': 256,\n",
       " 'with': 257,\n",
       " 'times': 258,\n",
       " 'search': 259,\n",
       " 'king': 260,\n",
       " 'solomon’s': 261,\n",
       " 'pantry': 262,\n",
       " 'back': 263,\n",
       " 'move': 264,\n",
       " 'forward': 265,\n",
       " 'peek': 266,\n",
       " 'white': 267,\n",
       " 'house': 268,\n",
       " 'swamp': 269,\n",
       " 'disney': 270,\n",
       " 'character': 271,\n",
       " 'living': 272,\n",
       " 'quirky': 273,\n",
       " 'dream': 274,\n",
       " 'justice': 275,\n",
       " 'dept.': 276,\n",
       " 're-examine': 277,\n",
       " 'police': 278,\n",
       " 'accords': 279,\n",
       " 'year,': 280,\n",
       " 'their': 281,\n",
       " 'year': 282,\n",
       " 'latest': 283,\n",
       " 'health': 284,\n",
       " 'proposal': 285,\n",
       " 'weakens': 286,\n",
       " 'coverage': 287,\n",
       " 'pre-existing': 288,\n",
       " 'conditions': 289,\n",
       " 'losing': 290,\n",
       " 'let’s': 291,\n",
       " 'go': 292,\n",
       " 'win': 293,\n",
       " 'opioids': 294,\n",
       " 'florida’s': 295,\n",
       " 'vengeful': 296,\n",
       " 'governor': 297,\n",
       " 'how': 298,\n",
       " 'end': 299,\n",
       " 'politicization': 300,\n",
       " 'courts': 301,\n",
       " 'dr.': 302,\n",
       " 'came': 303,\n",
       " 'against': 304,\n",
       " 'vietnam': 305,\n",
       " 'britain’s': 306,\n",
       " 'trains': 307,\n",
       " 'don’t': 308,\n",
       " 'run': 309,\n",
       " 'time.': 310,\n",
       " 'blame': 311,\n",
       " 'capitalism.': 312,\n",
       " 'questions': 313,\n",
       " 'for:': 314,\n",
       " '‘no': 315,\n",
       " 'license': 316,\n",
       " 'plates': 317,\n",
       " 'here:': 318,\n",
       " 'using': 319,\n",
       " 'transcend': 320,\n",
       " 'prison': 321,\n",
       " 'walls’': 322,\n",
       " 'dry': 323,\n",
       " 'spell': 324,\n",
       " 'are': 325,\n",
       " 'there': 326,\n",
       " 'subjects': 327,\n",
       " 'that': 328,\n",
       " 'should': 329,\n",
       " 'be': 330,\n",
       " 'off-limits': 331,\n",
       " 'artists,': 332,\n",
       " 'certain': 333,\n",
       " 'artists': 334,\n",
       " 'particular?': 335,\n",
       " '‘that': 336,\n",
       " 'great': 337,\n",
       " 'television’': 338,\n",
       " 'thinking': 339,\n",
       " 'code': 340,\n",
       " 'gorsuch’s': 341,\n",
       " 'influence': 342,\n",
       " 'could': 343,\n",
       " 'greater': 344,\n",
       " 'than': 345,\n",
       " 'his': 346,\n",
       " 'vote': 347,\n",
       " 'ease': 348,\n",
       " 'hangover': 349,\n",
       " 'gifts': 350,\n",
       " 'china': 351,\n",
       " 'penn': 352,\n",
       " 'station,': 353,\n",
       " 'rail': 354,\n",
       " 'mishap': 355,\n",
       " 'spurs': 356,\n",
       " 'large': 357,\n",
       " 'lasting': 358,\n",
       " 'headache': 359,\n",
       " 'chemical': 360,\n",
       " 'attack': 361,\n",
       " 'syrians': 362,\n",
       " 'ignites': 363,\n",
       " 'world’s': 364,\n",
       " 'outrage': 365,\n",
       " 'adventure': 366,\n",
       " 'still': 367,\n",
       " 'babbo’s': 368,\n",
       " 'menu': 369,\n",
       " 'swimming': 370,\n",
       " 'fast': 371,\n",
       " 'lane': 372,\n",
       " 'national': 373,\n",
       " 'civics': 374,\n",
       " 'exam': 375,\n",
       " 'obama': 376,\n",
       " 'adviser': 377,\n",
       " 'political': 378,\n",
       " 'cross': 379,\n",
       " 'hairs': 380,\n",
       " 'hippies': 381,\n",
       " 'won': 382,\n",
       " 'check': 383,\n",
       " 'box': 384,\n",
       " 'if': 385,\n",
       " 'you’re': 386,\n",
       " 'person': 387,\n",
       " 'couscous,': 388,\n",
       " 'chef’s': 389,\n",
       " 'patience': 390,\n",
       " 'pays': 391,\n",
       " 'three': 392,\n",
       " 'peas': 393,\n",
       " 'grain,': 394,\n",
       " 'playing': 395,\n",
       " 'well': 396,\n",
       " 'together': 397,\n",
       " 'fox': 398,\n",
       " 'like': 399,\n",
       " 'eagles?': 400,\n",
       " 'tired': 401,\n",
       " '‘hamilton’': 402,\n",
       " 'talk': 403,\n",
       " 'supreme': 404,\n",
       " 'court': 405,\n",
       " 'partisan': 406,\n",
       " 'tool': 407,\n",
       " '2': 408,\n",
       " 'picks': 409,\n",
       " 'education': 410,\n",
       " 'raise': 411,\n",
       " 'fears': 412,\n",
       " 'civil': 413,\n",
       " 'rights': 414,\n",
       " 'trump,': 415,\n",
       " 'focus': 416,\n",
       " 'u.s.': 417,\n",
       " 'interests': 418,\n",
       " 'disdain': 419,\n",
       " 'moralizing': 420,\n",
       " 'center': 421,\n",
       " 'universe': 422,\n",
       " '‘the': 423,\n",
       " 'americans’': 424,\n",
       " '5,': 425,\n",
       " '5': 426,\n",
       " 'recap:': 427,\n",
       " 'whole': 428,\n",
       " 'lotta': 429,\n",
       " 'shakin’': 430,\n",
       " 'badger': 431,\n",
       " 'cow': 432,\n",
       " 'jared': 433,\n",
       " 'kushner,': 434,\n",
       " 'man': 435,\n",
       " 'steel': 436,\n",
       " 'how-to': 437,\n",
       " 'book': 438,\n",
       " 'wielding': 439,\n",
       " 'civic': 440,\n",
       " 'power': 441,\n",
       " 'emperor': 442,\n",
       " 'real-world': 443,\n",
       " 'syria': 444,\n",
       " 'lesson': 445,\n",
       " '‘spacex': 446,\n",
       " 'launches': 447,\n",
       " 'satellite': 448,\n",
       " 'partly': 449,\n",
       " 'used': 450,\n",
       " 'rocket’': 451,\n",
       " 'ben': 452,\n",
       " 'sasse': 453,\n",
       " 'thinks': 454,\n",
       " 'biden': 455,\n",
       " 'would’ve': 456,\n",
       " 'earliest': 457,\n",
       " 'memory?': 458,\n",
       " 'secularist': 459,\n",
       " 'it': 460,\n",
       " 'o.k.': 461,\n",
       " 'our': 462,\n",
       " 'friends': 463,\n",
       " 'constantly': 464,\n",
       " 'suing': 465,\n",
       " 'people?': 466,\n",
       " 'diva': 467,\n",
       " 'departs': 468,\n",
       " 'takes': 469,\n",
       " 'suburb': 470,\n",
       " 'democratic': 471,\n",
       " 'turnout,': 472,\n",
       " 'low': 473,\n",
       " 'off-year': 474,\n",
       " 'races,': 475,\n",
       " 'appears': 476,\n",
       " 'rise': 477,\n",
       " 'mindful': 478,\n",
       " 'angry': 479,\n",
       " 'lessons': 480,\n",
       " 'mellow': 481,\n",
       " 'mice': 482,\n",
       " 'ask': 483,\n",
       " 'well;': 484,\n",
       " 'chewing': 485,\n",
       " 'gum': 486,\n",
       " 'toddlers?': 487,\n",
       " 'anyone?': 488,\n",
       " 'another': 489,\n",
       " 'thorny': 490,\n",
       " 'commute': 491,\n",
       " 'eighth': 492,\n",
       " 'annual': 493,\n",
       " 'found': 494,\n",
       " 'poem': 495,\n",
       " 'student': 496,\n",
       " 'contest': 497,\n",
       " 'women’s': 498,\n",
       " 'soccer': 499,\n",
       " 'team': 500,\n",
       " 'wins': 501,\n",
       " 'more,': 502,\n",
       " 'not': 503,\n",
       " 'equal,': 504,\n",
       " 'pay': 505,\n",
       " 'really': 506,\n",
       " 'security': 507,\n",
       " 'rationale': 508,\n",
       " 'banning': 509,\n",
       " 'laptops': 510,\n",
       " 'planes?': 511,\n",
       " 'seeking': 512,\n",
       " 'truths': 513,\n",
       " 'locked': 514,\n",
       " 'inside': 515,\n",
       " 'child': 516,\n",
       " 'soldier': 517,\n",
       " 'pepsi': 518,\n",
       " 'drops': 519,\n",
       " 'accused': 520,\n",
       " 'trivializing': 521,\n",
       " 'protesters': 522,\n",
       " 'eleven': 523,\n",
       " 'madison': 524,\n",
       " 'park': 525,\n",
       " 'tops': 526,\n",
       " 'list': 527,\n",
       " '50': 528,\n",
       " 'best': 529,\n",
       " 'restaurants': 530,\n",
       " 'bannon': 531,\n",
       " 'removed': 532,\n",
       " 'committee': 533,\n",
       " 'hawk': 534,\n",
       " 'soar': 535,\n",
       " 'suggests': 536,\n",
       " 'bigger': 537,\n",
       " 'role': 538,\n",
       " 'conflict': 539,\n",
       " 'cast': 540,\n",
       " 'doubt': 541,\n",
       " 'infrastructure': 542,\n",
       " 'groups': 543,\n",
       " 'seek': 544,\n",
       " 'court’s': 545,\n",
       " 'aid': 546,\n",
       " 'pesticide': 547,\n",
       " 'lobbying': 548,\n",
       " 'fierce': 549,\n",
       " '(and': 550,\n",
       " 'tasty)': 551,\n",
       " 'passing': 552,\n",
       " 'moe': 553,\n",
       " 'mr.': 554,\n",
       " 'most': 555,\n",
       " 'meeting': 556,\n",
       " 'dies,': 557,\n",
       " 'republicans': 558,\n",
       " 'can’t': 559,\n",
       " 'agree': 560,\n",
       " 'culprit': 561,\n",
       " 'fine': 562,\n",
       " 'romance?': 563,\n",
       " 'yes,': 564,\n",
       " 'gay': 565,\n",
       " 'history': 566,\n",
       " 'tour': 567,\n",
       " 'playbook': 568,\n",
       " 'symbols': 569,\n",
       " 'serving': 570,\n",
       " 'ham': 571,\n",
       " 'soignée': 572,\n",
       " 'silk': 573,\n",
       " 'fashion': 574,\n",
       " 'has': 575,\n",
       " 'covered': 576,\n",
       " 'only': 577,\n",
       " 'small': 578,\n",
       " 'army': 579,\n",
       " 'equal': 580,\n",
       " 'arnie': 581,\n",
       " 'public': 582,\n",
       " 'broadcasting': 583,\n",
       " 'learned': 584,\n",
       " 'younger': 585,\n",
       " '—': 586,\n",
       " 'taught': 587,\n",
       " 'person?': 588,\n",
       " 'i': 589,\n",
       " 'angered': 590,\n",
       " 'readers,': 591,\n",
       " 'again': 592,\n",
       " 'creeping': 593,\n",
       " 'toward': 594,\n",
       " 'crisis': 595,\n",
       " 'mistake': 596,\n",
       " 'war': 597,\n",
       " 'bets': 598,\n",
       " 'messy': 599,\n",
       " 'state': 600,\n",
       " 'u.s.-china': 601,\n",
       " 'ties:': 602,\n",
       " 'do': 603,\n",
       " '‘haley': 604,\n",
       " 'may': 605,\n",
       " '‘take': 606,\n",
       " 'own': 607,\n",
       " 'action’': 608,\n",
       " 'syrian': 609,\n",
       " 'attack’': 610,\n",
       " 'shadow': 611,\n",
       " 'fairy': 612,\n",
       " 'tale': 613,\n",
       " 'gut': 614,\n",
       " 'filibuster': 615,\n",
       " 'rule': 616,\n",
       " 'lift': 617,\n",
       " 'berliner': 618,\n",
       " 'fernsehturm': 619,\n",
       " 'improving': 620,\n",
       " 'original': 621,\n",
       " 'reader': 622,\n",
       " 'stories': 623,\n",
       " 'independence': 624,\n",
       " 'days': 625,\n",
       " 'snooping': 626,\n",
       " 'teenagers': 627,\n",
       " 'o.k.?': 628,\n",
       " 'city': 629,\n",
       " 'train': 630,\n",
       " 'donkey,': 631,\n",
       " 'find': 632,\n",
       " 'zebra': 633,\n",
       " 'facing': 634,\n",
       " 'scrutiny,': 635,\n",
       " 'intelligence': 636,\n",
       " 'chief': 637,\n",
       " 'leaves': 638,\n",
       " 'russia': 639,\n",
       " 'inquiry': 640,\n",
       " 'heart': 641,\n",
       " 'amazon': 642,\n",
       " '‘mrs.’': 643,\n",
       " 'became': 644,\n",
       " '‘ms.’': 645,\n",
       " 'recruiting': 646,\n",
       " 'assistants': 647,\n",
       " 'top': 648,\n",
       " 'job': 649,\n",
       " 'ewing': 650,\n",
       " 'dieting:': 651,\n",
       " 'yo-yo': 652,\n",
       " 'diets': 653,\n",
       " 'heart:': 654,\n",
       " 'race': 655,\n",
       " 'factor': 656,\n",
       " 'mixing': 657,\n",
       " 'drinks': 658,\n",
       " 'message': 659,\n",
       " 'simple': 660,\n",
       " 'taiwanese': 661,\n",
       " 'food': 662,\n",
       " 'doting': 663,\n",
       " 'mama': 664,\n",
       " 'don': 665,\n",
       " 'rickles,': 666,\n",
       " 'comedy’s': 667,\n",
       " 'opportunity': 668,\n",
       " 'offender,': 669,\n",
       " 'dies': 670,\n",
       " '90': 671,\n",
       " 'beaujolais,': 672,\n",
       " 'nouveau': 673,\n",
       " 'missiles': 674,\n",
       " 'feel': 675,\n",
       " 'american,': 676,\n",
       " 'try': 677,\n",
       " 'some': 678,\n",
       " 'velveeta': 679,\n",
       " 'fighting': 680,\n",
       " 'eviction,': 681,\n",
       " 'gardener': 682,\n",
       " 'turns': 683,\n",
       " 'organic': 684,\n",
       " 'industry': 685,\n",
       " 'giants': 686,\n",
       " 'help': 687,\n",
       " 'vaccines:': 688,\n",
       " 'moms’': 689,\n",
       " 'shot': 690,\n",
       " 'protects': 691,\n",
       " 'newborns': 692,\n",
       " 'without': 693,\n",
       " 'rikers': 694,\n",
       " 'island,': 695,\n",
       " 'learning': 696,\n",
       " 'love': 697,\n",
       " 'jail': 698,\n",
       " 'next': 699,\n",
       " 'door': 700,\n",
       " 'fingers': 701,\n",
       " 'crossed': 702,\n",
       " 'across': 703,\n",
       " 'generations': 704,\n",
       " 'donald': 705,\n",
       " 'bill': 706,\n",
       " 'roger': 707,\n",
       " 'ailes': 708,\n",
       " 'common?': 709,\n",
       " 'downsizing': 710,\n",
       " 'rock': 711,\n",
       " 'music': 712,\n",
       " 'sing': 713,\n",
       " 'song': 714,\n",
       " 'face': 715,\n",
       " 'creams': 716,\n",
       " 'after': 717,\n",
       " 'missiles,': 718,\n",
       " 'we': 719,\n",
       " 'need': 720,\n",
       " 'smart': 721,\n",
       " 'diplomacy': 722,\n",
       " 'youth,': 723,\n",
       " 'crowds,': 724,\n",
       " 'goals:': 725,\n",
       " 'germany': 726,\n",
       " 'aims': 727,\n",
       " 'keep': 728,\n",
       " 'way': 729,\n",
       " 'soul': 730,\n",
       " '…': 731,\n",
       " 'corporation': 732,\n",
       " 'coming': 733,\n",
       " 'incompetence': 734,\n",
       " 'bad,': 735,\n",
       " 'worse': 736,\n",
       " 'ugly': 737,\n",
       " 'optimists': 738,\n",
       " 'pessimists': 739,\n",
       " 'more': 740,\n",
       " 'journalists': 741,\n",
       " 'independent': 742,\n",
       " 'editorial': 743,\n",
       " 'control?': 744,\n",
       " 'ghost': 745,\n",
       " 'airbnb': 746,\n",
       " 'borrowers': 747,\n",
       " 'bewildered': 748,\n",
       " 'unemployment': 749,\n",
       " 'falls,': 750,\n",
       " 'feeble': 751,\n",
       " 'growth': 752,\n",
       " 'tempers': 753,\n",
       " 'optimism': 754,\n",
       " 'friday': 755,\n",
       " 'mailbag:': 756,\n",
       " 'senate': 757,\n",
       " 'votes': 758,\n",
       " 'museum': 759,\n",
       " 'drama': 760,\n",
       " 'teaching': 761,\n",
       " 'with:': 762,\n",
       " '‘animated': 763,\n",
       " 'life:': 764,\n",
       " 'mary': 765,\n",
       " 'leakey’': 766,\n",
       " 'muppet': 767,\n",
       " 'autism': 768,\n",
       " 'means': 769,\n",
       " 'preschool': 770,\n",
       " 'teachers': 771,\n",
       " 'graduates?': 772,\n",
       " 'no': 773,\n",
       " 'longer': 774,\n",
       " 'citi': 775,\n",
       " 'field’s': 776,\n",
       " 'alpha': 777,\n",
       " 'dog,': 778,\n",
       " 'harvey': 779,\n",
       " 'bite': 780,\n",
       " 'mexico': 781,\n",
       " 'outlaws': 782,\n",
       " 'school': 783,\n",
       " '‘lunch': 784,\n",
       " 'shaming’': 785,\n",
       " 'economic': 786,\n",
       " 'indicator?': 787,\n",
       " 'isn’t': 788,\n",
       " 'riddle': 789,\n",
       " 'strike': 790,\n",
       " 'air': 791,\n",
       " 'base': 792,\n",
       " 'angers': 793,\n",
       " 'russians': 794,\n",
       " 'nominee': 795,\n",
       " 'confirmed': 796,\n",
       " 'bruising': 797,\n",
       " 'yearlong': 798,\n",
       " 'fight': 799,\n",
       " 'disquieting': 800,\n",
       " 'silence': 801,\n",
       " 'near-zero': 802,\n",
       " 'interest': 803,\n",
       " 'rates:': 804,\n",
       " 'get': 805,\n",
       " 'them': 806,\n",
       " 'slice': 807,\n",
       " 'ambrosia': 808,\n",
       " 'filling)': 809,\n",
       " 'survived': 810,\n",
       " 'sarin': 811,\n",
       " 'gas': 812,\n",
       " 'first': 813,\n",
       " 'televised': 814,\n",
       " 'airstrikes,': 815,\n",
       " 'next?': 816,\n",
       " 'romance,': 817,\n",
       " 'sarcasm,': 818,\n",
       " 'math': 819,\n",
       " 'language': 820,\n",
       " 'who': 821,\n",
       " 'put': 822,\n",
       " 'those': 823,\n",
       " 'endless': 824,\n",
       " 'tunes': 825,\n",
       " 'fumes?': 826,\n",
       " 'share': 827,\n",
       " 'pta': 828,\n",
       " 'aid?': 829,\n",
       " 'parents': 830,\n",
       " 'would': 831,\n",
       " 'rather': 832,\n",
       " 'split': 833,\n",
       " 'district': 834,\n",
       " 'election': 835,\n",
       " 'left': 836,\n",
       " 'over': 837,\n",
       " 'economy': 838,\n",
       " 'marching': 839,\n",
       " 'bands': 840,\n",
       " 'fuels': 841,\n",
       " 'uncertainty': 842,\n",
       " 'ground': 843,\n",
       " 'clinton,': 844,\n",
       " 'free': 845,\n",
       " 'speak': 846,\n",
       " 'her': 847,\n",
       " 'mind': 848,\n",
       " 'passion': 849,\n",
       " 'southern': 850,\n",
       " 'christians': 851,\n",
       " 'wall': 852,\n",
       " 'happened': 853,\n",
       " 'who?': 854,\n",
       " 'president’s': 855,\n",
       " 'generals': 856,\n",
       " 'myth': 857,\n",
       " 'main': 858,\n",
       " 'street': 859,\n",
       " 'interviews': 860,\n",
       " 'offbeat': 861,\n",
       " 'approach': 862,\n",
       " 'covering': 863,\n",
       " 'sports': 864,\n",
       " 'having': 865,\n",
       " 'nothing': 866,\n",
       " 'getting': 867,\n",
       " 'canary': 868,\n",
       " 'tillerson': 869,\n",
       " 'halts': 870,\n",
       " 'any': 871,\n",
       " 'thawing': 872,\n",
       " 'ties': 873,\n",
       " 'words': 874,\n",
       " 'self-empowerment': 875,\n",
       " 'claiming': 876,\n",
       " 'odyssey’s': 877,\n",
       " 'reward': 878,\n",
       " 'familiar': 879,\n",
       " 'signature': 880,\n",
       " 'suits': 881,\n",
       " 'say': 882,\n",
       " 'lender': 883,\n",
       " 'duped': 884,\n",
       " 'students': 885,\n",
       " 'fuel': 886,\n",
       " 'station': 887,\n",
       " 'tie-ups': 888,\n",
       " 'depend': 889,\n",
       " 'knot': 890,\n",
       " 'agencies': 891,\n",
       " 'same': 892,\n",
       " 'old': 893,\n",
       " 'sergio': 894,\n",
       " 'delights': 895,\n",
       " 'crowd': 896,\n",
       " 'script': 897,\n",
       " '8:': 898,\n",
       " 'money,': 899,\n",
       " 'rules': 900,\n",
       " '6': 901,\n",
       " 'midnight': 902,\n",
       " 'descending': 903,\n",
       " 'spring': 904,\n",
       " 'break': 905,\n",
       " 'network': 906,\n",
       " '12:': 907,\n",
       " 'finale,': 908,\n",
       " 'carrie': 909,\n",
       " 'deals': 910,\n",
       " 'death': 911,\n",
       " 'betrayal': 912,\n",
       " 'bo': 913,\n",
       " 'diddley': 914,\n",
       " 'buddha?': 915,\n",
       " 'gig': 916,\n",
       " 'economy’s': 917,\n",
       " 'false': 918,\n",
       " 'promise': 919,\n",
       " 'publicity': 920,\n",
       " 'stunts': 921,\n",
       " 'aren’t': 922,\n",
       " 'policy': 923,\n",
       " 'road': 924,\n",
       " 'weapon': 925,\n",
       " 'alabama': 926,\n",
       " 'resigns': 927,\n",
       " 'pleads': 928,\n",
       " 'guilty': 929,\n",
       " 'amid': 930,\n",
       " 'sex': 931,\n",
       " 'scandal': 932,\n",
       " 'california': 933,\n",
       " 'moves': 934,\n",
       " 'protections': 935,\n",
       " 'immigrants;': 936,\n",
       " 'states': 937,\n",
       " 'follow': 938,\n",
       " 'he': 939,\n",
       " 'led': 940,\n",
       " 'yankees': 941,\n",
       " '4': 942,\n",
       " 'titles.': 943,\n",
       " 'revive': 944,\n",
       " 'them?': 945,\n",
       " 'fox:': 946,\n",
       " 'women': 947,\n",
       " 'often': 948,\n",
       " 'report': 949,\n",
       " 'sexual': 950,\n",
       " 'harassment': 951,\n",
       " 'work': 952,\n",
       " 'parents’': 953,\n",
       " 'mistakes': 954,\n",
       " 'rude': 955,\n",
       " 'doctors,': 956,\n",
       " 'nurses,': 957,\n",
       " 'patients': 958,\n",
       " 'passover,': 959,\n",
       " 'everyday': 960,\n",
       " 'plagues': 961,\n",
       " 'many': 962,\n",
       " 'pills': 963,\n",
       " 'too': 964,\n",
       " 'many?': 965,\n",
       " 'greatest': 966,\n",
       " 'earth': 967,\n",
       " 'wells': 968,\n",
       " 'fargo': 969,\n",
       " 'ex-leaders': 970,\n",
       " 'owe': 971,\n",
       " '$75': 972,\n",
       " 'million': 973,\n",
       " 'saved': 974,\n",
       " 'patients,': 975,\n",
       " 'furious': 976,\n",
       " 'families': 977,\n",
       " 'russian': 978,\n",
       " '‘boris': 979,\n",
       " 'badenov’': 980,\n",
       " 'high': 981,\n",
       " 'highlight': 982,\n",
       " 'president,': 983,\n",
       " 'sworn': 984,\n",
       " '113th': 985,\n",
       " 'deployment': 986,\n",
       " 'carrier': 987,\n",
       " 'masks': 988,\n",
       " 'lack': 989,\n",
       " 'better': 990,\n",
       " 'options': 991,\n",
       " 'dragged': 992,\n",
       " 'full': 993,\n",
       " 'jet,': 994,\n",
       " 'stirring': 995,\n",
       " 'furor': 996,\n",
       " '3': 997,\n",
       " 'pulitzers;': 998,\n",
       " 'service': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(article_lists[0])\n",
    "cleaned_sentence = [doc.lower() for doc in df.headline.values if doc not in string.punctuation ]\n",
    "\n",
    "\n",
    "# 모든 문장의 단어를 추출해 고유 번호 지정\n",
    "bow = {}\n",
    "for line in cleaned_sentence:\n",
    "    for w in line.split():\n",
    "        if w not in bow:\n",
    "            bow[w] = len(bow.keys())\n",
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbf93a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "[bow[w] for w in cleaned_sentence[5].split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395c25b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_headline = []\n",
    "articles = [path for path in article_lists if 'Articles' in path]\n",
    "# headline 정보만 추출하여 all_headline에 추가\n",
    "# 전처리 : 소문자로 통일, 특수문자 제거\n",
    "import string       # 특수문자 라이브러리\n",
    "string.punctuation\n",
    "for a in articles:\n",
    "    df = pd.read_csv(a)\n",
    "    a.headline.values\n",
    "    all_headline.extend(a.headline.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c42eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SAMSUNG\\anaconda3\\envs\\deep\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.13)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "# kaggle에서 데이터 download\n",
    "import kagglehub\n",
    "path = kagglehub.dataset_download('aashita/nyt-comments')\n",
    "\n",
    "# csv 파일이 있는 경로 path\n",
    "csv_lists = glob(path+'/*.*')\n",
    "\n",
    "class TextGeneration(Dataset):\n",
    "    def clean_text(self, txt):\n",
    "        # 모든 단어를 소문자로 바꾸고 특수문자를 제거\n",
    "        txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n",
    "        return txt\n",
    "    def __init__(self,csv_lists):\n",
    "        all_headlines = []\n",
    "\n",
    "        # 모든 헤드라인의 텍스트를 불러옴\n",
    "        for filename in csv_lists:\n",
    "            if 'Articles' in filename:\n",
    "                article_df = pd.read_csv(filename)\n",
    "\n",
    "                # 데이터셋의 headline의 값을 all_headlines에 추가\n",
    "                all_headlines.extend(list(article_df.headline.values))\n",
    "                break\n",
    "\n",
    "        # headline 중 unknown 값은 제거\n",
    "        all_headlines = [h for h in all_headlines if h != \"Unknown\"]    # 의미 없는 \"Unknown\" 문자열을 제거\n",
    "        \n",
    "        # 구두점 제거 및 전처리가 된 문장들을 리스트로 반환\n",
    "        self.corpus = [self.clean_text(x) for x in all_headlines]\n",
    "        # 단어 사전(Vocabulary) 생성 (BOW)\n",
    "        self.BOW = {}\n",
    "\n",
    "        # 모든 문장의 단어를 추출해 고유번호 지정-> 단어별로 넘버링하는 과정\n",
    "        for line in self.corpus:\n",
    "            for word in line.split():\n",
    "                if word not in self.BOW.keys():\n",
    "                    self.BOW[word] = len(self.BOW.keys())\n",
    "\n",
    "        # 모델의 입력으로 사용할 데이터\n",
    "        self.data = self.generate_sequence(self.corpus) #  최종적으로 모델이 학습할 [입력, 정답] 형태의 데이터 쌍을 만든다\n",
    "    def generate_sequence(self, txt):\n",
    "        seq = []\n",
    "\n",
    "        for line in txt:\n",
    "            line = line.split()\n",
    "            line_bow = [self.BOW[word] for word in line]\n",
    "\n",
    "            # 단어 2개를 입력으로, 그다음 단어를 정답으로\n",
    "            data = [([line_bow[i], line_bow[i+1]], line_bow[i+2]) \n",
    "            for i in range(len(line_bow)-2)]\n",
    "            \n",
    "            seq.extend(data)\n",
    "\n",
    "        return seq\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, i):\n",
    "        data = np.array(self.data[i][0])  # 입력 데이터\n",
    "        label = np.array(self.data[i][1]).astype(np.float32)  # 출력 데이터\n",
    "\n",
    "        return torch.tensor(data, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38964019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array(2., dtype=float32))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 확인\n",
    "dataset = TextGeneration(csv_lists)\n",
    "len(dataset)\n",
    "x,y = next(iter(dataset))\n",
    "x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8d563a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM 모델 정의\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self,num_embeddings):      # num_embeddings 전체 단어의 개수(어휘사전의 크기(bow))\n",
    "        super(LSTM,self).__init__()\n",
    "\n",
    "        # 신경망이 이해할 수 있도록 벡터로 변경\n",
    "        self.embed = nn.Embedding(num_embeddings=num_embeddings,embedding_dim=16)\n",
    "        \n",
    "        # LSTM을 5개 층으로 쌓는다 (배치 - 시퀀스 - 피처 순)\n",
    "        self.lstm =nn.LSTM(input_size=16,hidden_size=64, num_layers=5, batch_first=True)    # hidden_size 16~512 사이를 많이 사용/ input_size는 위에 embedding_dim과 일치해야 함\n",
    "        # 분류를 위한 fc층      squence_length * 64 = 2*64 = 128\n",
    "        self.fc1 = nn.Linear(128,num_embeddings)\n",
    "        self.fc2 = nn.Linear(num_embeddings,num_embeddings)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self,x):      # 입력 (배치크기, sequence_length) 배치 32 sequence_length = 2\n",
    "        x = self.embed(x)     # 출력 (배치크기, sequence_length,16) 32 2 16/ x (단어 인덱스로 구성된 텐서)가 임베딩 계층을 통과하여 단어 벡터 시퀀스로 변환\n",
    "\n",
    "        # lstm 모델 예측값\n",
    "        x, _ = self.lstm(x) # 출력 (batch, sq_len, 64) 32, 2, 64\n",
    "        x = torch.reshape(x,(x.shape[0],-1))    # 출력 (batch, sq_len x 64) 32, 128\n",
    "        x = self.relu(self.fc1(x))  # fc1(x) -> 128을 받는다\n",
    "        out = self.fc2(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "942cb1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63 [00:00<?, ?it/s]C:\\Users\\SAMSUNG\\AppData\\Local\\Temp\\ipykernel_24136\\738584793.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pred = model(torch.tensor(data, dtype= torch.long))\n",
      "C:\\Users\\SAMSUNG\\AppData\\Local\\Temp\\ipykernel_24136\\738584793.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss = nn.CrossEntropyLoss()(pred,torch.tensor(label,dtype = torch.long))\n",
      "epoch:1 loss: 7.365715980529785: 100%|██████████| 63/63 [00:08<00:00,  7.59it/s] \n",
      "epoch:2 loss: 7.006417751312256: 100%|██████████| 63/63 [00:07<00:00,  8.13it/s] \n",
      "epoch:3 loss: 6.78374719619751: 100%|██████████| 63/63 [00:07<00:00,  8.33it/s]  \n",
      "epoch:4 loss: 6.588791847229004: 100%|██████████| 63/63 [00:08<00:00,  7.84it/s] \n",
      "epoch:5 loss: 6.448853969573975: 100%|██████████| 63/63 [00:08<00:00,  7.34it/s] \n",
      "epoch:6 loss: 6.335721015930176: 100%|██████████| 63/63 [00:09<00:00,  6.78it/s] \n",
      "epoch:7 loss: 6.148932456970215: 100%|██████████| 63/63 [00:11<00:00,  5.71it/s] \n",
      "epoch:8 loss: 6.173055171966553: 100%|██████████| 63/63 [00:07<00:00,  8.16it/s] \n",
      "epoch:9 loss: 6.068240642547607: 100%|██████████| 63/63 [00:06<00:00,  9.13it/s] \n",
      "epoch:10 loss: 5.874332904815674: 100%|██████████| 63/63 [00:06<00:00,  9.50it/s] \n",
      "epoch:11 loss: 5.966263771057129: 100%|██████████| 63/63 [00:06<00:00,  9.63it/s] \n",
      "epoch:12 loss: 5.811357021331787: 100%|██████████| 63/63 [00:06<00:00,  9.33it/s] \n",
      "epoch:13 loss: 5.563632965087891: 100%|██████████| 63/63 [00:06<00:00,  9.10it/s] \n",
      "epoch:14 loss: 5.417152404785156: 100%|██████████| 63/63 [00:07<00:00,  8.66it/s] \n",
      "epoch:15 loss: 5.276804447174072: 100%|██████████| 63/63 [00:07<00:00,  7.89it/s] \n",
      "epoch:16 loss: 5.22268533706665: 100%|██████████| 63/63 [00:06<00:00,  9.09it/s]  \n",
      "epoch:17 loss: 5.027644634246826: 100%|██████████| 63/63 [00:08<00:00,  7.62it/s] \n",
      "epoch:18 loss: 4.986569404602051: 100%|██████████| 63/63 [00:08<00:00,  7.20it/s] \n",
      "epoch:19 loss: 4.837137222290039: 100%|██████████| 63/63 [00:08<00:00,  7.28it/s] \n",
      "epoch:20 loss: 4.728883743286133: 100%|██████████| 63/63 [00:08<00:00,  7.55it/s] \n",
      "epoch:21 loss: 4.611424922943115: 100%|██████████| 63/63 [00:08<00:00,  7.69it/s] \n",
      "epoch:22 loss: 4.5186543464660645: 100%|██████████| 63/63 [00:09<00:00,  6.34it/s]\n",
      "epoch:23 loss: 4.429816246032715: 100%|██████████| 63/63 [00:07<00:00,  8.00it/s] \n",
      "epoch:24 loss: 4.401076793670654: 100%|██████████| 63/63 [00:09<00:00,  6.47it/s] \n",
      "epoch:25 loss: 4.423693656921387: 100%|██████████| 63/63 [00:11<00:00,  5.65it/s] \n",
      "epoch:26 loss: 4.6135759353637695: 100%|██████████| 63/63 [00:08<00:00,  7.63it/s]\n",
      "epoch:27 loss: 4.5031328201293945: 100%|██████████| 63/63 [00:09<00:00,  6.37it/s]\n",
      "epoch:28 loss: 4.383737087249756: 100%|██████████| 63/63 [00:10<00:00,  5.83it/s] \n",
      "epoch:29 loss: 4.395432472229004: 100%|██████████| 63/63 [00:08<00:00,  7.72it/s] \n",
      "epoch:30 loss: 4.196774005889893: 100%|██████████| 63/63 [00:08<00:00,  7.71it/s] \n",
      "epoch:31 loss: 4.08635139465332: 100%|██████████| 63/63 [00:07<00:00,  7.90it/s]  \n",
      "epoch:32 loss: 4.070107936859131: 100%|██████████| 63/63 [00:07<00:00,  7.93it/s] \n",
      "epoch:33 loss: 3.9847629070281982: 100%|██████████| 63/63 [00:07<00:00,  8.22it/s]\n",
      "epoch:34 loss: 4.03664493560791: 100%|██████████| 63/63 [00:07<00:00,  8.40it/s]  \n",
      "epoch:35 loss: 4.149481296539307: 100%|██████████| 63/63 [00:07<00:00,  8.35it/s] \n",
      "epoch:36 loss: 4.151904106140137: 100%|██████████| 63/63 [00:09<00:00,  6.87it/s] \n",
      "epoch:37 loss: 4.26047945022583: 100%|██████████| 63/63 [00:07<00:00,  8.09it/s]  \n",
      "epoch:38 loss: 4.60199499130249: 100%|██████████| 63/63 [00:09<00:00,  6.60it/s]  \n",
      "epoch:39 loss: 4.171611309051514: 100%|██████████| 63/63 [00:12<00:00,  4.91it/s] \n",
      "epoch:40 loss: 3.828303098678589: 100%|██████████| 63/63 [00:17<00:00,  3.70it/s] \n",
      "epoch:41 loss: 3.701720714569092: 100%|██████████| 63/63 [00:09<00:00,  6.67it/s] \n",
      "epoch:42 loss: 3.8095366954803467: 100%|██████████| 63/63 [00:09<00:00,  6.98it/s]\n",
      "epoch:43 loss: 3.7700259685516357: 100%|██████████| 63/63 [00:10<00:00,  6.29it/s]\n",
      "epoch:44 loss: 3.810746669769287: 100%|██████████| 63/63 [00:09<00:00,  6.52it/s] \n",
      "epoch:45 loss: 3.793018341064453: 100%|██████████| 63/63 [00:10<00:00,  6.23it/s] \n",
      "epoch:46 loss: 3.624413251876831: 100%|██████████| 63/63 [00:09<00:00,  6.44it/s] \n",
      "epoch:47 loss: 3.4617831707000732: 100%|██████████| 63/63 [00:07<00:00,  7.88it/s]\n",
      "epoch:48 loss: 3.4766368865966797: 100%|██████████| 63/63 [00:08<00:00,  7.31it/s]\n",
      "epoch:49 loss: 3.5232348442077637: 100%|██████████| 63/63 [00:09<00:00,  6.89it/s]\n",
      "epoch:50 loss: 3.3613929748535156: 100%|██████████| 63/63 [00:09<00:00,  6.63it/s]\n",
      "epoch:51 loss: 3.2074134349823: 100%|██████████| 63/63 [00:07<00:00,  7.96it/s]   \n",
      "epoch:52 loss: 3.4690463542938232: 100%|██████████| 63/63 [00:07<00:00,  8.02it/s]\n",
      "epoch:53 loss: 3.344423770904541: 100%|██████████| 63/63 [00:07<00:00,  7.99it/s] \n",
      "epoch:54 loss: 3.361046314239502: 100%|██████████| 63/63 [00:07<00:00,  8.23it/s] \n",
      "epoch:55 loss: 3.3261847496032715: 100%|██████████| 63/63 [00:10<00:00,  6.16it/s]\n",
      "epoch:56 loss: 3.368422031402588: 100%|██████████| 63/63 [00:11<00:00,  5.65it/s] \n",
      "epoch:57 loss: 3.007814884185791: 100%|██████████| 63/63 [00:09<00:00,  6.90it/s] \n",
      "epoch:58 loss: 2.9320709705352783: 100%|██████████| 63/63 [00:08<00:00,  7.05it/s]\n",
      "epoch:59 loss: 2.9307541847229004: 100%|██████████| 63/63 [00:08<00:00,  7.42it/s]\n",
      "epoch:60 loss: 2.7462573051452637: 100%|██████████| 63/63 [00:09<00:00,  6.75it/s]\n",
      "epoch:61 loss: 2.654564619064331: 100%|██████████| 63/63 [00:13<00:00,  4.84it/s] \n",
      "epoch:62 loss: 2.6983225345611572: 100%|██████████| 63/63 [00:14<00:00,  4.43it/s]\n",
      "epoch:63 loss: 3.0678484439849854: 100%|██████████| 63/63 [00:09<00:00,  6.38it/s]\n",
      "epoch:64 loss: 2.6322109699249268: 100%|██████████| 63/63 [00:09<00:00,  6.70it/s]\n",
      "epoch:65 loss: 2.5337233543395996: 100%|██████████| 63/63 [00:12<00:00,  5.12it/s]\n",
      "epoch:66 loss: 2.579479694366455: 100%|██████████| 63/63 [00:11<00:00,  5.44it/s] \n",
      "epoch:67 loss: 2.3744659423828125: 100%|██████████| 63/63 [00:10<00:00,  6.28it/s]\n",
      "epoch:68 loss: 2.2282235622406006: 100%|██████████| 63/63 [00:08<00:00,  7.64it/s]\n",
      "epoch:69 loss: 2.0989441871643066: 100%|██████████| 63/63 [00:09<00:00,  6.85it/s]\n",
      "epoch:70 loss: 2.0422415733337402: 100%|██████████| 63/63 [00:07<00:00,  8.22it/s]\n",
      "epoch:71 loss: 1.8875234127044678: 100%|██████████| 63/63 [00:09<00:00,  6.81it/s]\n",
      "epoch:72 loss: 1.8502696752548218: 100%|██████████| 63/63 [00:10<00:00,  5.82it/s]\n",
      "epoch:73 loss: 1.7995681762695312: 100%|██████████| 63/63 [00:11<00:00,  5.32it/s]\n",
      "epoch:74 loss: 1.7506557703018188: 100%|██████████| 63/63 [00:12<00:00,  5.11it/s]\n",
      "epoch:75 loss: 1.7384103536605835: 100%|██████████| 63/63 [00:09<00:00,  6.36it/s]\n",
      "epoch:76 loss: 1.7141584157943726: 100%|██████████| 63/63 [00:07<00:00,  8.02it/s]\n",
      "epoch:77 loss: 1.573571801185608: 100%|██████████| 63/63 [00:08<00:00,  7.61it/s] \n",
      "epoch:78 loss: 1.569574236869812: 100%|██████████| 63/63 [00:07<00:00,  7.95it/s] \n",
      "epoch:79 loss: 1.540209174156189: 100%|██████████| 63/63 [00:08<00:00,  7.27it/s] \n",
      "epoch:80 loss: 1.6083606481552124: 100%|██████████| 63/63 [00:08<00:00,  7.10it/s]\n",
      "epoch:81 loss: 1.4434525966644287: 100%|██████████| 63/63 [00:09<00:00,  6.34it/s]\n",
      "epoch:82 loss: 1.3274866342544556: 100%|██████████| 63/63 [00:10<00:00,  6.05it/s]\n",
      "epoch:83 loss: 1.2542383670806885: 100%|██████████| 63/63 [00:10<00:00,  6.10it/s]\n",
      "epoch:84 loss: 1.115194320678711: 100%|██████████| 63/63 [00:10<00:00,  6.09it/s] \n",
      "epoch:85 loss: 0.9988111257553101: 100%|██████████| 63/63 [00:08<00:00,  7.09it/s]\n",
      "epoch:86 loss: 0.924910843372345: 100%|██████████| 63/63 [00:08<00:00,  7.08it/s] \n",
      "epoch:87 loss: 0.9548748135566711: 100%|██████████| 63/63 [00:08<00:00,  7.31it/s]\n",
      "epoch:88 loss: 0.9644352197647095: 100%|██████████| 63/63 [00:08<00:00,  7.65it/s]\n",
      "epoch:89 loss: 0.9912793040275574: 100%|██████████| 63/63 [00:08<00:00,  7.51it/s]\n",
      "epoch:90 loss: 0.9643890261650085: 100%|██████████| 63/63 [00:08<00:00,  7.32it/s]\n",
      "epoch:91 loss: 0.9342134594917297: 100%|██████████| 63/63 [00:10<00:00,  5.97it/s]\n",
      "epoch:92 loss: 0.995376467704773: 100%|██████████| 63/63 [00:09<00:00,  6.42it/s] \n",
      "epoch:93 loss: 1.1952508687973022: 100%|██████████| 63/63 [00:10<00:00,  5.75it/s]\n",
      "epoch:94 loss: 1.2417168617248535: 100%|██████████| 63/63 [00:08<00:00,  7.05it/s]\n",
      "epoch:95 loss: 1.1538069248199463: 100%|██████████| 63/63 [00:08<00:00,  7.55it/s]\n",
      "epoch:96 loss: 0.84306401014328: 100%|██████████| 63/63 [00:10<00:00,  5.84it/s]  \n",
      "epoch:97 loss: 0.7402042150497437: 100%|██████████| 63/63 [00:12<00:00,  5.17it/s]\n",
      "epoch:98 loss: 1.0872817039489746: 100%|██████████| 63/63 [00:09<00:00,  6.40it/s]\n",
      "epoch:99 loss: 1.1966785192489624: 100%|██████████| 63/63 [00:12<00:00,  5.24it/s]\n",
      "epoch:100 loss: 1.4502253532409668: 100%|██████████| 63/63 [00:09<00:00,  6.30it/s]\n",
      "epoch:101 loss: 1.1923805475234985: 100%|██████████| 63/63 [00:09<00:00,  6.35it/s]\n",
      "epoch:102 loss: 0.9667975306510925: 100%|██████████| 63/63 [00:10<00:00,  6.23it/s]\n",
      "epoch:103 loss: 0.8957552909851074: 100%|██████████| 63/63 [00:14<00:00,  4.48it/s]\n",
      "epoch:104 loss: 0.7034796476364136: 100%|██████████| 63/63 [00:11<00:00,  5.67it/s]\n",
      "epoch:105 loss: 0.7616379857063293: 100%|██████████| 63/63 [00:12<00:00,  5.03it/s]\n",
      "epoch:106 loss: 0.9832009077072144: 100%|██████████| 63/63 [00:09<00:00,  6.60it/s]\n",
      "epoch:107 loss: 1.04914391040802: 100%|██████████| 63/63 [00:11<00:00,  5.69it/s]  \n",
      "epoch:108 loss: 0.7963770627975464: 100%|██████████| 63/63 [00:10<00:00,  5.87it/s]\n",
      "epoch:109 loss: 0.7793053388595581: 100%|██████████| 63/63 [00:13<00:00,  4.83it/s]\n",
      "epoch:110 loss: 0.676958441734314: 100%|██████████| 63/63 [00:13<00:00,  4.63it/s] \n",
      "epoch:111 loss: 0.6045622825622559: 100%|██████████| 63/63 [00:14<00:00,  4.42it/s]\n",
      "epoch:112 loss: 0.5663618445396423: 100%|██████████| 63/63 [00:10<00:00,  6.16it/s]\n",
      "epoch:113 loss: 0.7350950241088867: 100%|██████████| 63/63 [00:10<00:00,  5.87it/s]\n",
      "epoch:114 loss: 0.6769874691963196: 100%|██████████| 63/63 [00:09<00:00,  6.33it/s]\n",
      "epoch:115 loss: 0.5534772276878357: 100%|██████████| 63/63 [00:10<00:00,  6.00it/s]\n",
      "epoch:116 loss: 0.5122917294502258: 100%|██████████| 63/63 [00:10<00:00,  6.04it/s]\n",
      "epoch:117 loss: 0.4277167320251465: 100%|██████████| 63/63 [00:10<00:00,  5.86it/s]\n",
      "epoch:118 loss: 0.47337836027145386: 100%|██████████| 63/63 [00:14<00:00,  4.48it/s]\n",
      "epoch:119 loss: 0.403914213180542: 100%|██████████| 63/63 [00:12<00:00,  5.22it/s] \n",
      "epoch:120 loss: 0.4206388592720032: 100%|██████████| 63/63 [00:13<00:00,  4.52it/s]\n",
      "epoch:121 loss: 0.29653897881507874: 100%|██████████| 63/63 [00:14<00:00,  4.49it/s]\n",
      "epoch:122 loss: 0.30022716522216797: 100%|██████████| 63/63 [00:10<00:00,  6.11it/s]\n",
      "epoch:123 loss: 0.31896206736564636: 100%|██████████| 63/63 [00:13<00:00,  4.62it/s]\n",
      "epoch:124 loss: 0.3782893717288971: 100%|██████████| 63/63 [00:16<00:00,  3.83it/s]\n",
      "epoch:125 loss: 0.332155704498291: 100%|██████████| 63/63 [00:14<00:00,  4.34it/s] \n",
      "epoch:126 loss: 0.30971089005470276: 100%|██████████| 63/63 [00:13<00:00,  4.71it/s]\n",
      "epoch:127 loss: 0.5800096392631531: 100%|██████████| 63/63 [00:12<00:00,  5.04it/s]\n",
      "epoch:128 loss: 0.38363298773765564: 100%|██████████| 63/63 [00:12<00:00,  5.19it/s]\n",
      "epoch:129 loss: 0.43486684560775757: 100%|██████████| 63/63 [00:15<00:00,  4.15it/s]\n",
      "epoch:130 loss: 0.365646094083786: 100%|██████████| 63/63 [00:15<00:00,  4.05it/s] \n",
      "epoch:131 loss: 0.349556565284729: 100%|██████████| 63/63 [00:16<00:00,  3.86it/s] \n",
      "epoch:132 loss: 0.286286324262619: 100%|██████████| 63/63 [00:13<00:00,  4.83it/s] \n",
      "epoch:133 loss: 0.28765958547592163: 100%|██████████| 63/63 [00:12<00:00,  5.18it/s]\n",
      "epoch:134 loss: 0.29898568987846375: 100%|██████████| 63/63 [00:14<00:00,  4.25it/s]\n",
      "epoch:135 loss: 0.22572442889213562: 100%|██████████| 63/63 [00:14<00:00,  4.48it/s]\n",
      "epoch:136 loss: 0.4401260018348694: 100%|██████████| 63/63 [00:13<00:00,  4.62it/s]\n",
      "epoch:137 loss: 0.28974786400794983: 100%|██████████| 63/63 [00:12<00:00,  5.02it/s]\n",
      "epoch:138 loss: 0.3956150412559509: 100%|██████████| 63/63 [00:12<00:00,  5.24it/s]\n",
      "epoch:139 loss: 0.38414397835731506: 100%|██████████| 63/63 [00:12<00:00,  5.20it/s]\n",
      "epoch:140 loss: 0.3877129554748535: 100%|██████████| 63/63 [00:13<00:00,  4.65it/s]\n",
      "epoch:141 loss: 0.5507508516311646: 100%|██████████| 63/63 [00:13<00:00,  4.83it/s]\n",
      "epoch:142 loss: 0.2686159312725067: 100%|██████████| 63/63 [00:13<00:00,  4.69it/s]\n",
      "epoch:143 loss: 0.28390684723854065: 100%|██████████| 63/63 [00:12<00:00,  5.00it/s]\n",
      "epoch:144 loss: 0.24845150113105774: 100%|██████████| 63/63 [00:12<00:00,  5.11it/s]\n",
      "epoch:145 loss: 0.3230251669883728: 100%|██████████| 63/63 [00:10<00:00,  6.01it/s]\n",
      "epoch:146 loss: 0.45187216997146606: 100%|██████████| 63/63 [00:10<00:00,  6.24it/s]\n",
      "epoch:147 loss: 0.3050776720046997: 100%|██████████| 63/63 [00:10<00:00,  5.82it/s]\n",
      "epoch:148 loss: 0.30519866943359375: 100%|██████████| 63/63 [00:11<00:00,  5.58it/s]\n",
      "epoch:149 loss: 0.28111302852630615: 100%|██████████| 63/63 [00:12<00:00,  5.13it/s]\n",
      "epoch:150 loss: 0.38673698902130127: 100%|██████████| 63/63 [00:15<00:00,  4.11it/s]\n",
      "epoch:151 loss: 0.35436517000198364: 100%|██████████| 63/63 [00:19<00:00,  3.26it/s]\n",
      "epoch:152 loss: 0.30819860100746155: 100%|██████████| 63/63 [00:19<00:00,  3.25it/s]\n",
      "epoch:153 loss: 0.29260116815567017: 100%|██████████| 63/63 [00:13<00:00,  4.83it/s]\n",
      "epoch:154 loss: 0.23705749213695526: 100%|██████████| 63/63 [00:14<00:00,  4.42it/s]\n",
      "epoch:155 loss: 0.17945292592048645: 100%|██████████| 63/63 [00:14<00:00,  4.22it/s]\n",
      "epoch:156 loss: 0.19899140298366547: 100%|██████████| 63/63 [00:14<00:00,  4.49it/s]\n",
      "epoch:157 loss: 0.12527503073215485: 100%|██████████| 63/63 [00:11<00:00,  5.30it/s]\n",
      "epoch:158 loss: 0.21435129642486572: 100%|██████████| 63/63 [00:14<00:00,  4.38it/s]\n",
      "epoch:159 loss: 0.19748058915138245: 100%|██████████| 63/63 [00:15<00:00,  4.07it/s]\n",
      "epoch:160 loss: 0.2815974950790405: 100%|██████████| 63/63 [00:15<00:00,  4.03it/s] \n",
      "epoch:161 loss: 0.2987993359565735: 100%|██████████| 63/63 [00:15<00:00,  4.12it/s] \n",
      "epoch:162 loss: 0.22284598648548126: 100%|██████████| 63/63 [00:13<00:00,  4.53it/s]\n",
      "epoch:163 loss: 0.21570733189582825: 100%|██████████| 63/63 [00:13<00:00,  4.57it/s]\n",
      "epoch:164 loss: 0.29763609170913696: 100%|██████████| 63/63 [00:14<00:00,  4.31it/s]\n",
      "epoch:165 loss: 0.17055559158325195: 100%|██████████| 63/63 [00:15<00:00,  4.05it/s]\n",
      "epoch:166 loss: 0.23197153210639954: 100%|██████████| 63/63 [00:18<00:00,  3.40it/s]\n",
      "epoch:167 loss: 0.24506032466888428: 100%|██████████| 63/63 [00:15<00:00,  4.05it/s]\n",
      "epoch:168 loss: 0.17872655391693115: 100%|██████████| 63/63 [00:14<00:00,  4.45it/s]\n",
      "epoch:169 loss: 0.11469236761331558: 100%|██████████| 63/63 [00:13<00:00,  4.74it/s]\n",
      "epoch:170 loss: 0.2664271593093872: 100%|██████████| 63/63 [00:15<00:00,  3.99it/s] \n",
      "epoch:171 loss: 0.1020364984869957: 100%|██████████| 63/63 [00:15<00:00,  3.95it/s]\n",
      "epoch:172 loss: 0.2186294049024582: 100%|██████████| 63/63 [00:15<00:00,  4.14it/s] \n",
      "epoch:173 loss: 0.4930776059627533: 100%|██████████| 63/63 [00:13<00:00,  4.56it/s] \n",
      "epoch:174 loss: 0.3299180269241333: 100%|██████████| 63/63 [00:13<00:00,  4.76it/s]\n",
      "epoch:175 loss: 0.1506013423204422: 100%|██████████| 63/63 [00:12<00:00,  4.87it/s]\n",
      "epoch:176 loss: 0.16886350512504578: 100%|██████████| 63/63 [00:14<00:00,  4.27it/s]\n",
      "epoch:177 loss: 0.23139312863349915: 100%|██████████| 63/63 [00:18<00:00,  3.44it/s]\n",
      "epoch:178 loss: 0.4934328496456146: 100%|██████████| 63/63 [00:20<00:00,  3.09it/s]\n",
      "epoch:179 loss: 0.3450200855731964: 100%|██████████| 63/63 [00:15<00:00,  4.10it/s] \n",
      "epoch:180 loss: 0.32768505811691284: 100%|██████████| 63/63 [00:14<00:00,  4.26it/s]\n",
      "epoch:181 loss: 0.2376122772693634: 100%|██████████| 63/63 [00:16<00:00,  3.79it/s] \n",
      "epoch:182 loss: 0.15414303541183472: 100%|██████████| 63/63 [00:15<00:00,  4.19it/s]\n",
      "epoch:183 loss: 0.18763622641563416: 100%|██████████| 63/63 [00:13<00:00,  4.62it/s]\n",
      "epoch:184 loss: 0.29575198888778687: 100%|██████████| 63/63 [00:13<00:00,  4.68it/s]\n",
      "epoch:185 loss: 0.12051103264093399: 100%|██████████| 63/63 [00:16<00:00,  3.75it/s]\n",
      "epoch:186 loss: 0.2267284095287323: 100%|██████████| 63/63 [00:31<00:00,  1.99it/s] \n",
      "epoch:187 loss: 0.11346700042486191: 100%|██████████| 63/63 [00:24<00:00,  2.57it/s]\n",
      "epoch:188 loss: 0.15062204003334045: 100%|██████████| 63/63 [00:19<00:00,  3.17it/s]\n",
      "epoch:189 loss: 0.11019524186849594: 100%|██████████| 63/63 [00:13<00:00,  4.73it/s]\n",
      "epoch:190 loss: 0.11343248188495636: 100%|██████████| 63/63 [00:15<00:00,  4.12it/s]\n",
      "epoch:191 loss: 0.19236153364181519: 100%|██████████| 63/63 [00:15<00:00,  4.14it/s]\n",
      "epoch:192 loss: 0.11252288520336151: 100%|██████████| 63/63 [00:16<00:00,  3.89it/s]\n",
      "epoch:193 loss: 0.07204004377126694: 100%|██████████| 63/63 [00:17<00:00,  3.56it/s]\n",
      "epoch:194 loss: 0.146260604262352: 100%|██████████| 63/63 [00:13<00:00,  4.59it/s]  \n",
      "epoch:195 loss: 0.16245827078819275: 100%|██████████| 63/63 [00:13<00:00,  4.77it/s]\n",
      "epoch:196 loss: 0.16728845238685608: 100%|██████████| 63/63 [00:14<00:00,  4.44it/s]\n",
      "epoch:197 loss: 0.29819515347480774: 100%|██████████| 63/63 [00:16<00:00,  3.92it/s]\n",
      "epoch:198 loss: 0.1502387970685959: 100%|██████████| 63/63 [00:15<00:00,  4.11it/s] \n",
      "epoch:199 loss: 0.1150149330496788: 100%|██████████| 63/63 [00:14<00:00,  4.30it/s] \n",
      "epoch:200 loss: 0.12690946459770203: 100%|██████████| 63/63 [00:14<00:00,  4.45it/s]\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.optim.adam import Adam\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "dataset = TextGeneration(csv_lists)\n",
    "model = LSTM(num_embeddings=len(dataset.BOW)).to(device)\n",
    "loader = DataLoader(dataset,batch_size = 64)\n",
    "optim = Adam(model.parameters(), lr = 1e-3)\n",
    "\n",
    "for epoch in range(200):\n",
    "    loop = tqdm(loader)\n",
    "    for data, label in loop :\n",
    "        data,label = data.to(device), label.to (device)\n",
    "        optim.zero_grad()\n",
    "        pred = model(torch.tensor(data, dtype= torch.long))\n",
    "        loss = nn.CrossEntropyLoss()(pred,torch.tensor(label,dtype = torch.long))\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        loop.set_description(f'epoch:{epoch+1} loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "db45f0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ugly\n"
     ]
    }
   ],
   "source": [
    "# 문장을 예측\n",
    "# 입력문장을 텐서로 변경, 임베딩 벡터 BOW를 이용해서\n",
    "sample = 'i love'\n",
    "with torch.no_grad():\n",
    "    words = torch.tensor(\n",
    "        [dataset.BOW[w] for w in sample.split()],dtype=torch.long).to(device).unsqueeze(0)\n",
    "    output = model(words)\n",
    "\n",
    "    # 출력은 단어의 개수만큼 len(BOW) (batch, len(Bow))\n",
    "    # 확률이 가장 높은단어 찾기\n",
    "    predicted_index = torch.argmax(output,dim=1).item()\n",
    "\n",
    "    # 단어사전 BOW에서 인덱스에 해당하는 단어를 찾기\n",
    "    # 역 dict를 만들어서 찾기\n",
    "    reverse_bow = {value:key for key,value in dataset.BOW.items()}\n",
    "    predicted_word = reverse_bow[predicted_index]   \n",
    "    print(predicted_word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
