import os
import warnings
from dotenv import load_dotenv
from typing import List, Tuple
import hashlib

from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.retrievers import TavilySearchAPIRetriever, BM25Retriever
#from langchain_community.retrievers import EnsembleRetriever
from langchain_core.documents import Document


from enum import Enum
from pydantic import BaseModel

warnings.filterwarnings("ignore")
load_dotenv()

api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise ValueError("OPENAI_API_KEY ì—†ìŒ! .env í™•ì¸í•´ì¤˜")

TAVILY_API_KEY = os.getenv("TAVILY_API_KEY") 

embedding_model = OpenAIEmbeddings(model="text-embedding-3-small")

# ë²¡í„°DB ë¡œë“œ

# 1) ë©”ì¸ ë¬¸ì„œìš© ë²¡í„°DB
vectorstore = Chroma(
    persist_directory="./chroma_startup_all",
    collection_name="startup_all_rag",
    embedding_function=embedding_model,
)

# 2) ìºì‹œ ì „ìš© ë²¡í„°DB (ë¶„ë¦¬!) âœ… ê°œì„  1
cache_vectorstore = Chroma(
    persist_directory="./chroma_cache",
    collection_name="semantic_cache",
    embedding_function=embedding_model,
)

try:
    all_data = vectorstore.get()
    ids = all_data.get("ids", [])
    print(f"âœ… ë©”ì¸ ë²¡í„°DB ë¡œë“œ ì™„ë£Œ / ì´ ë²¡í„° ê°œìˆ˜: {len(ids)}")
except Exception as e:
    print("âš  ë²¡í„°DB ìƒíƒœ í™•ì¸ ì¤‘ ì—ëŸ¬:", e)

try:
    cache_data = cache_vectorstore.get()
    cache_ids = cache_data.get("ids", [])
    print(f"âœ… ìºì‹œ ë²¡í„°DB ë¡œë“œ ì™„ë£Œ / ì´ ìºì‹œ ê°œìˆ˜: {len(cache_ids)}")
except Exception as e:
    print("âš  ìºì‹œ ë²¡í„°DB ìƒíƒœ í™•ì¸ ì¤‘ ì—ëŸ¬:", e)
    print("âš  ë²¡í„°DB ìƒíƒœ í™•ì¸ ì¤‘ ì—ëŸ¬:", e)

# ========================================
# í•˜ì´ë¸Œë¦¬ë“œ ë¦¬íŠ¸ë¦¬ë²„ êµ¬í˜„ âœ… ê°œì„  2
# ========================================

class HybridRetriever:
    """Dense(ì˜ë¯¸) + BM25(í‚¤ì›Œë“œ) í•˜ì´ë¸Œë¦¬ë“œ ë¦¬íŠ¸ë¦¬ë²„"""
    
    def __init__(
        self,
        vectorstore: Chroma,
        documents: List[Document],
        k: int = 10,
        dense_weight: float = 0.6,
        bm25_weight: float = 0.4,
    ):
        self.vectorstore = vectorstore
        self.k = k
        self.dense_weight = dense_weight
        self.bm25_weight = bm25_weight
        self._internal_k = k * 2  # ë” ë§ì´ ê°€ì ¸ì˜¨ í›„ ìœµí•©
        
        # Dense retriever
        self.dense_retriever = vectorstore.as_retriever(
            search_kwargs={"k": self._internal_k}
        )
        
        # BM25 retriever
        print(f"[í•˜ì´ë¸Œë¦¬ë“œ] BM25 ì¸ë±ìŠ¤ ìƒì„± ì¤‘... (ë¬¸ì„œ ìˆ˜: {len(documents)})")
        self.bm25_retriever = BM25Retriever.from_documents(documents)
        self.bm25_retriever.k = self._internal_k
        print(f"[í•˜ì´ë¸Œë¦¬ë“œ] BM25 ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
    
    def invoke(self, query: str) -> List[Tuple[Document, float]]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤í–‰ (RRF ìœµí•©)"""
        # Dense ê²€ìƒ‰
        dense_docs = self._search_dense(query)
        
        # BM25 ê²€ìƒ‰
        bm25_docs = self._search_bm25(query)
        
        # RRFë¡œ ìœµí•©
        fused_docs = self._fuse_results(dense_docs, bm25_docs)
        
        return fused_docs[:self.k]
    
    def _search_dense(self, query: str) -> List[Document]:
        """Dense ê²€ìƒ‰"""
        try:
            return self.dense_retriever.invoke(query)
        except Exception as e:
            print(f"âš  Dense ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def _search_bm25(self, query: str) -> List[Document]:
        """BM25 ê²€ìƒ‰"""
        try:
            return self.bm25_retriever.invoke(query)
        except Exception as e:
            print(f"âš  BM25 ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def _fuse_results(
        self, 
        dense_docs: List[Document], 
        bm25_docs: List[Document]
    ) -> List[Tuple[Document, float]]:
        """RRF(Reciprocal Rank Fusion)ë¡œ ê²€ìƒ‰ ê²°ê³¼ í†µí•©"""
        scores = {}
        doc_map = {}
        
        # Dense ì ìˆ˜ ê³„ì‚°
        for rank, doc in enumerate(dense_docs):
            key = self._get_doc_key(doc)
            scores[key] = scores.get(key, 0.0) + self.dense_weight / (rank + 1)
            doc_map[key] = doc
        
        # BM25 ì ìˆ˜ ê³„ì‚°
        for rank, doc in enumerate(bm25_docs):
            key = self._get_doc_key(doc)
            scores[key] = scores.get(key, 0.0) + self.bm25_weight / (rank + 1)
            doc_map[key] = doc
        
        # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
        ranked_keys = sorted(scores.keys(), key=lambda k: scores[k], reverse=True)
        
        # (Document, score) í˜•íƒœë¡œ ë°˜í™˜
        return [(doc_map[k], scores[k]) for k in ranked_keys]
    
    @staticmethod
    def _get_doc_key(doc: Document) -> str:
        """ë¬¸ì„œ ì‹ë³„ìš© í‚¤ ìƒì„± (í•´ì‹œ ê¸°ë°˜)"""
        content_hash = hashlib.md5(doc.page_content[:500].encode()).hexdigest()
        return content_hash


# í•˜ì´ë¸Œë¦¬ë“œ ë¦¬íŠ¸ë¦¬ë²„ ì´ˆê¸°í™”
print("\n[ì´ˆê¸°í™”] í•˜ì´ë¸Œë¦¬ë“œ ë¦¬íŠ¸ë¦¬ë²„ ìƒì„± ì¤‘...")
try:
    # ëª¨ë“  ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸° (BM25ìš©)
    all_docs_data = vectorstore.get(include=['documents', 'metadatas'])
    all_documents = [
        Document(page_content=doc, metadata=meta or {})
        for doc, meta in zip(
            all_docs_data.get('documents', []),
            all_docs_data.get('metadatas', [])
        )
    ]
    
    hybrid_retriever = HybridRetriever(
        vectorstore=vectorstore,
        documents=all_documents,
        k=10,
        dense_weight=0.6,  # Dense 60%
        bm25_weight=0.4    # BM25 40%
    )
    print("[ì´ˆê¸°í™”] í•˜ì´ë¸Œë¦¬ë“œ ë¦¬íŠ¸ë¦¬ë²„ ì¤€ë¹„ ì™„ë£Œ!\n")
except Exception as e:
    print(f"âš  í•˜ì´ë¸Œë¦¬ë“œ ë¦¬íŠ¸ë¦¬ë²„ ìƒì„± ì‹¤íŒ¨: {e}")
    print("âš  Dense ë¦¬íŠ¸ë¦¬ë²„ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    hybrid_retriever = None

# ========================================
# ìºì‹œ ì‹œìŠ¤í…œ (ë¶„ë¦¬ëœ ë²¡í„°DB ì‚¬ìš©) âœ… ê°œì„  3
# ========================================

class SimpleCache:
    """L1 ìºì‹œ - ì™„ì „ ì¼ì¹˜ ê¸°ë°˜ ë©”ëª¨ë¦¬ ìºì‹œ"""
    def __init__(self, max_size=100):
        self.cache = {}
        self.max_size = max_size
        self.access_count = {}

    def get(self, key):
        if key in self.cache:
            self.access_count[key] = self.access_count.get(key, 0) + 1
        return self.cache.get(key)

    def set(self, key, value):
        # LFU ê¸°ë°˜ ìºì‹œ ì œê±°
        if len(self.cache) >= self.max_size:
            if self.access_count:
                least_used = min(self.access_count, key=self.access_count.get)
                del self.cache[least_used]
                del self.access_count[least_used]
        
        self.cache[key] = value
        self.access_count[key] = 0

    def stats(self):
        return {
            "size": len(self.cache),
            "max_size": self.max_size,
            "total_accesses": sum(self.access_count.values())
        }


class SemanticCache:
    """L2 ìºì‹œ - ì˜ë¯¸ì  ìœ ì‚¬ë„ ê¸°ë°˜ ìºì‹œ (ë¶„ë¦¬ëœ ìºì‹œ ì „ìš© ë²¡í„°DB ì‚¬ìš©)"""
    
    def __init__(self, cache_vectorstore: Chroma, similarity_threshold=0.85):
        self.cache_vectorstore = cache_vectorstore  # ìºì‹œ ì „ìš© DB
        self.similarity_threshold = similarity_threshold
        self.cache_data = {}  # {cache_id: response}
        self.cache_metadata = {}  # {cache_id: {query, timestamp, hits}}
        
        # ê¸°ì¡´ ìºì‹œ ë³µêµ¬
        self._load_existing_cache()
    
    def _load_existing_cache(self):
        """ê¸°ì¡´ ìºì‹œ ë°ì´í„° ë¡œë“œ"""
        try:
            existing = self.cache_vectorstore.get(include=['metadatas'])
            for meta in existing.get('metadatas', []):
                cache_id = meta.get('cache_id')
                if cache_id:
                    self.cache_metadata[cache_id] = meta
            print(f"[L2 ìºì‹œ] ê¸°ì¡´ ìºì‹œ {len(self.cache_metadata)}ê°œ ë³µêµ¬")
        except Exception as e:
            print(f"âš  ìºì‹œ ë³µêµ¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def get(self, query: str):
        """ìœ ì‚¬í•œ ì§ˆë¬¸ì´ ìºì‹œì— ìˆìœ¼ë©´ ë°˜í™˜"""
        try:
            # ìºì‹œ ì „ìš© DBì—ì„œ ìœ ì‚¬ë„ ê²€ìƒ‰
            similar_docs = self.cache_vectorstore.similarity_search_with_score(
                query, k=1
            )
            
            if similar_docs:
                doc, distance = similar_docs[0]
                # Chroma distance -> ìœ ì‚¬ë„ ë³€í™˜
                similarity = max(0.0, 1.0 - (distance / 2.0))
                
                if similarity >= self.similarity_threshold:
                    cache_id = doc.metadata.get('cache_id')
                    
                    # ì „ì²´ ì‘ë‹µ ë°˜í™˜ (ë©”íƒ€ë°ì´í„°ê°€ ì•„ë‹Œ ì‹¤ì œ ìºì‹œ ë°ì´í„°ì—ì„œ)
                    if cache_id in self.cache_data:
                        full_response = self.cache_data[cache_id]
                        
                        # ìºì‹œ íˆíŠ¸ ì¹´ìš´íŠ¸ ì¦ê°€
                        if cache_id in self.cache_metadata:
                            self.cache_metadata[cache_id]['hits'] = \
                                self.cache_metadata[cache_id].get('hits', 0) + 1
                        
                        print(f"[L2 ìºì‹œ íˆíŠ¸] ìœ ì‚¬ë„: {similarity:.3f}, cache_id: {cache_id}")
                        return full_response
            
            return None
        except Exception as e:
            print(f"âš  L2 ìºì‹œ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return None
    
    def set(self, query: str, response: str):
        """ì§ˆë¬¸ê³¼ ì‘ë‹µì„ ìºì‹œì— ì €ì¥"""
        try:
            import time
            
            cache_id = f"cache_{len(self.cache_data)}_{int(time.time())}"
            
            # ì „ì²´ ì‘ë‹µì„ cache_dataì— ì €ì¥
            self.cache_data[cache_id] = response
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥ (ìš”ì•½ë³¸ë§Œ)
            metadata = {
                'cache_id': cache_id,
                'response_preview': response[:200] + "..." if len(response) > 200 else response,
                'timestamp': time.time(),
                'hits': 0
            }
            self.cache_metadata[cache_id] = metadata
            
            # ìºì‹œ ì „ìš© ë²¡í„°DBì— ì €ì¥
            self.cache_vectorstore.add_texts(
                texts=[query],
                metadatas=[metadata],
                ids=[cache_id]
            )
            
            print(f"[L2 ìºì‹œ ì €ì¥] cache_id: {cache_id}, ì‘ë‹µ ê¸¸ì´: {len(response)}ì")
        except Exception as e:
            print(f"âš  L2 ìºì‹œ ì €ì¥ ì˜¤ë¥˜: {e}")
    
    def stats(self):
        return {
            "size": len(self.cache_data),
            "total_hits": sum(m.get('hits', 0) for m in self.cache_metadata.values()),
            "avg_hits": sum(m.get('hits', 0) for m in self.cache_metadata.values()) / max(len(self.cache_metadata), 1)
        }


class MultiLevelCache:
    """ë©€í‹°ë ˆë²¨ ìºì‹œ - L1(ì™„ì „ì¼ì¹˜) + L2(ì˜ë¯¸ì  ìœ ì‚¬ë„)"""
    
    def __init__(self, cache_vectorstore: Chroma):
        self.l1_cache = SimpleCache(max_size=100)
        self.l2_cache = SemanticCache(
            cache_vectorstore=cache_vectorstore,
            similarity_threshold=0.85
        )
        self.stats_data = {
            'l1_hits': 0,
            'l2_hits': 0,
            'misses': 0
        }
    
    def get(self, key: str, llm_callable):
        """ìºì‹œ ì¡°íšŒ ë˜ëŠ” LLM í˜¸ì¶œ"""
        # L1 ìºì‹œ í™•ì¸
        cached = self.l1_cache.get(key)
        if cached:
            self.stats_data['l1_hits'] += 1
            print('âœ… L1 cache hit')
            return cached
        
        # L2 ìºì‹œ í™•ì¸
        cached = self.l2_cache.get(key)
        if cached:
            self.stats_data['l2_hits'] += 1
            print('âœ… L2 cache hit')
            self.l1_cache.set(key, cached)
            return cached
        
        # ìºì‹œ ë¯¸ìŠ¤ - LLM í˜¸ì¶œ
        self.stats_data['misses'] += 1
        print('âŒ Cache miss - RAG execution')
        response = llm_callable()
        
        # ì–‘ìª½ ìºì‹œì— ì €ì¥
        self.l1_cache.set(key, response)
        self.l2_cache.set(key, response)
        
        return response
    
    def stats(self):
        total = sum(self.stats_data.values())
        hit_rate = (self.stats_data['l1_hits'] + self.stats_data['l2_hits']) / max(total, 1) * 100
        
        print("\n" + "="*60)
        print("ğŸ“Š ìºì‹œ í†µê³„")
        print("="*60)
        print(f"L1 íˆíŠ¸: {self.stats_data['l1_hits']}")
        print(f"L2 íˆíŠ¸: {self.stats_data['l2_hits']}")
        print(f"ìºì‹œ ë¯¸ìŠ¤: {self.stats_data['misses']}")
        print(f"ì´ ìš”ì²­: {total}")
        print(f"ìºì‹œ íˆíŠ¸ìœ¨: {hit_rate:.1f}%")
        print(f"\nL1 ìƒì„¸: {self.l1_cache.stats()}")
        print(f"L2 ìƒì„¸: {self.l2_cache.stats()}")
        print("="*60 + "\n")


# ë©€í‹°ë ˆë²¨ ìºì‹œ ì´ˆê¸°í™”
multi_level_cache = MultiLevelCache(cache_vectorstore=cache_vectorstore)

# ========================================
# í”„ë¡¬í”„íŠ¸ ì •ì˜
# ========================================

# ê´€ë ¨ì„± ê²€ì¦ í”„ë¡¬í”„íŠ¸
relevance_check_prompt = ChatPromptTemplate.from_template("""
ë‹¹ì‹ ì€ ë¬¸ì„œì™€ ì§ˆë¬¸ì˜ ê´€ë ¨ì„±ì„ ì—„ê²©í•˜ê²Œ íŒë‹¨í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

[ì§ˆë¬¸]
{question}

[ê²€ìƒ‰ëœ ë¬¸ì„œ ìƒ˜í”Œ]
{documents}

[íŒë‹¨ ê¸°ì¤€]
1. ì§ˆë¬¸ì˜ í•µì‹¬ ì£¼ì œì™€ ë¬¸ì„œì˜ ë‚´ìš©ì´ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ë˜ëŠ”ê°€?
2. ë¬¸ì„œê°€ ì§ˆë¬¸ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ”ê°€?
3. ë‹¨ìˆœíˆ ìœ ì‚¬í•œ ë‹¨ì–´ê°€ ìˆëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ì‹¤ì œ ë‹µë³€ ê°€ëŠ¥í•œ ë‚´ìš©ì¸ê°€?

[ì˜ˆì‹œ]
- "ì„œìš¸ ë™ë¬¼ë³‘ì›" vs "ì„œìš¸ ì°½ì—… ê³µê°„" â†’ ê´€ë ¨ì—†ìŒ (ì„œìš¸ë§Œ ê³µí†µ)
- "AI êµìœ¡" vs "ì°½ì—… êµìœ¡" â†’ ê´€ë ¨ì—†ìŒ (êµìœ¡ë§Œ ê³µí†µ, ì£¼ì œ ë‹¤ë¦„)
- "ì°½ì—… ì§€ì›ì‚¬ì—…" vs "ì°½ì—… ìê¸ˆ ì§€ì›" â†’ ê´€ë ¨ìˆìŒ (ì§ì ‘ ì—°ê´€)

ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¡œë§Œ ë‹µë³€: "ê´€ë ¨ìˆìŒ" ë˜ëŠ” "ê´€ë ¨ì—†ìŒ"

ë‹µë³€:""")

# Query Transformation
qt_prompt = ChatPromptTemplate.from_template("""
ë‹¤ìŒ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë²¡í„° ê²€ìƒ‰ì— ì í•©í•œ 'í•µì‹¬ í‚¤ì›Œë“œ ì¤‘ì‹¬ ë¬¸ì¥'ìœ¼ë¡œ ë°”ê¾¸ì„¸ìš”.
ë¶ˆí•„ìš”í•œ ë§ì€ ì œê±°í•˜ê³ , í•µì‹¬ ì¡°ê±´ë§Œ ë‚¨ê¸°ì„¸ìš”.

ì›ë³¸ ì§ˆë¬¸: {question}

ë³€í™˜ëœ ê²€ìƒ‰ìš© ë¬¸ì¥:""")

# ë©€í‹°ì¿¼ë¦¬ ìƒì„±
multi_query_prompt = ChatPromptTemplate.from_template("""
ë‹¤ìŒì§ˆë¬¸ì— ëŒ€í•´ 3ê°€ì§€ ë‹¤ë¥¸ ê´€ì ì˜ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ì„¸ìš”.
ê°ì¿¼ë¦¬ëŠ” ì„¸ ì¤„ë¡œ êµ¬ë¶„í•˜ì—¬ ì¶œë ¥í•˜ì„¸ìš”        
ë²ˆí˜¸ë‚˜ ì„¤ëª… ì—†ì´ ì¿¼ë¦¬ë§Œ ì¶œë ¥í•˜ì„¸ìš”

ì›ë³¸ì§ˆë¬¸: {question}""")

# ê¸°ë³¸ RAG í”„ë¡¬í”„íŠ¸
rag_prompt = ChatPromptTemplate.from_messages([
    ("system", """
ë‹¹ì‹ ì€ ì˜ˆë¹„Â·ì´ˆê¸° ì°½ì—…ìë¥¼ ë„ì™€ì£¼ëŠ” 'ì°½ì—… ì§€ì› í†µí•© AI ì–´ì‹œìŠ¤í„´íŠ¸'ì…ë‹ˆë‹¤.

[ì‚¬ìš© ê°€ëŠ¥í•œ ì •ë³´ ìœ í˜•]
- ì§€ì›ì‚¬ì—… ê³µê³  (announcement)
- ì‹¤íŒ¨/ì¬ë„ì „ ì‚¬ë¡€ (cases)
- ì°½ì—… ê³µê°„ ì •ë³´ (space)
- ë²•ë ¹: ì¤‘ì†Œê¸°ì—…ì°½ì—… ì§€ì›ë²• ë“± (law)
- í†µê³„, ë§¤ë‰´ì–¼ ë“± ì°¸ê³  ìë£Œ

[ë‹µë³€ ì›ì¹™]
1. ë°˜ë“œì‹œ ì œê³µëœ ë¬¸ë§¥(Context) ì•ˆì˜ ì •ë³´ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
2. ë¬¸ë§¥ì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³  ì†”ì§í•˜ê²Œ ë§í•˜ì„¸ìš”.
3. ì§ˆë¬¸ ì„±ê²©ì— ë”°ë¼ ë‹¤ìŒ ì •ë³´ ìœ í˜•ì„ ìš°ì„  í™œìš©í•˜ì„¸ìš”.
   - ì§€ì›ì‚¬ì—…Â·ì‹ ì²­ ê°€ëŠ¥ ì—¬ë¶€ â†’ announcement
   - ë²•ì  ì •ì˜Â·ìê²© ìš”ê±´ â†’ law
   - ì¡°ì–¸Â·ì£¼ì˜ì  â†’ cases
   - ê³µê°„Â·ì…ì£¼ â†’ space
4. í•µì‹¬ ë‹µë³€ í›„ í•„ìš”í•˜ë©´ bulletë¡œ ì •ë¦¬í•˜ì„¸ìš”.
5. ë§ˆì§€ë§‰ì— ì°¸ê³  ê·¼ê±° ìœ í˜•ì„ ìš”ì•½í•˜ì„¸ìš”.
"""),
    ("human", "[ë¬¸ë§¥]\n{context}\n\n[ì§ˆë¬¸]\n{question}\n\n[ë‹µë³€]")
])

# ë²•ë ¹ ì „ìš© í”„ë¡¬í”„íŠ¸
law_prompt = ChatPromptTemplate.from_messages([
    ("system", """
ë‹¹ì‹ ì€ ì¤‘ì†Œê¸°ì—…ì°½ì—… ì§€ì›ë²•ì„ ë°”íƒ•ìœ¼ë¡œ ì°½ì—… ì œë„ì™€ ìš”ê±´ì„ ì„¤ëª…í•˜ëŠ” AIì…ë‹ˆë‹¤.

[ê·œì¹™]
1. ë°˜ë“œì‹œ ë¬¸ë§¥ì— ìˆëŠ” ë²•ë ¹ ë‚´ìš©ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
2. ê°€ëŠ¥í•˜ë©´ ì¡°ë¬¸ ë²ˆí˜¸(ì œâ—‹ì¡°)ë¥¼ í•¨ê»˜ ì œì‹œí•˜ì„¸ìš”.
3. ë¬¸ë§¥ì— ì—†ëŠ” ë‚´ìš©ì€ "ì œê³µëœ ë²•ë ¹ ë¬¸ì„œì—ì„œ í•´ë‹¹ ë‚´ìš©ì€ í™•ì¸ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤."ë¼ê³  ë‹µí•˜ì„¸ìš”.
4. ë‹µë³€ ëì— "â€» ë³¸ ë‹µë³€ì€ ì¼ë°˜ ì •ë³´ ì œê³µì´ë©°, êµ¬ì²´ì ì¸ ë²•ë¥  ìë¬¸ì€ ì•„ë‹™ë‹ˆë‹¤."ë¥¼ í¬í•¨í•˜ì„¸ìš”.
"""),
    ("human", "[ë²•ë ¹ ë¬¸ë§¥]\n{context}\n\n[ì§ˆë¬¸]\n{question}\n\n[ì„¤ëª…]")
])

# ì§€ì›ì‚¬ì—… ì¶”ì²œ í”„ë¡¬í”„íŠ¸
recommend_prompt = ChatPromptTemplate.from_messages([
    ("system", """
ë‹¹ì‹ ì€ ì˜ˆë¹„Â·ì´ˆê¸° ì°½ì—…ìì—ê²Œ ê°€ì¥ ì í•©í•œ 'ì§€ì›ì‚¬ì—…ì„ ì¶”ì²œí•˜ëŠ” ì „ë¬¸ê°€ AI'ì…ë‹ˆë‹¤.

[ëª©í‘œ]
ì‚¬ìš©ìì˜ ì¡°ê±´(ë‚˜ì´, ì§€ì—­, ì—…ì¢…, ì°½ì—… ë‹¨ê³„ ë“±)ì„ ê¸°ì¤€ìœ¼ë¡œ
'ì‹¤ì§ˆì ì¸ ë„ì›€ì´ ë˜ëŠ” ì‚¬ì—…(ìê¸ˆÂ·ê³µê°„Â·R&DÂ·ì‹œì œí’ˆÂ·êµìœ¡)'ì„ ìš°ì„ ì ìœ¼ë¡œ ì¶”ì²œí•©ë‹ˆë‹¤.

[ì¶”ì²œ ìš°ì„ ìˆœìœ„]
1. í˜„ê¸ˆì„± ì§€ì›(ì‚¬ì—…í™” ìê¸ˆ, ì‹œì œí’ˆ ì œì‘ë¹„, R&D)
2. ì…ì£¼ ê³µê°„, ì¥ë¹„ ì§€ì›
3. ì—‘ì…€ëŸ¬ë ˆì´íŒ…, ë©˜í† ë§
4. ë‹¨ìˆœ êµìœ¡/íŠ¹ê°•ì€ ë§ˆì§€ë§‰ ìˆœìœ„

[ì¶”ì²œ ê·œì¹™]
1. ë°˜ë“œì‹œ announcement ë¬¸ì„œë§Œ ì‚¬ìš©
2. ì‚¬ìš©ì ì¡°ê±´ê³¼ 'ì§€ì—­Â·ì—°ë ¹Â·ë‹¨ê³„Â·ì—…ì¢…'ì´ ëª…í™•íˆ ë§ëŠ” ê²ƒë§Œ ì¶”ì²œ
3. ìµœëŒ€ 2ê°œê¹Œì§€ë§Œ ì¶”ì²œ
4. ì¡°ê±´ì´ ë§ëŠ” ì‚¬ì—…ì´ ì—†ìœ¼ë©´ ì†”ì§í•˜ê²Œ ë§í•˜ê¸°
5. ITÂ·ì„œë¹„ìŠ¤ì—…ì´ë©´ 'ê¸°ìˆ Â·ì½˜í…ì¸ Â·í”Œë«í¼' í‚¤ì›Œë“œ í¬í•¨ ì‚¬ì—… ìš°ì„ 

[ì¶œë ¥ í˜•ì‹]
â–  ì¶”ì²œ ì‚¬ì—…ëª…
â–  ì™œ ì´ ì‚¬ìš©ìì—ê²Œ ì í•©í•œì§€
â–  ì§€ì› ë‚´ìš©(ìê¸ˆ/ê³µê°„/êµìœ¡ ì¤‘ ë¬´ì—‡ì¸ì§€ ëª…í™•íˆ)
â–  ì‹ ì²­ ëŒ€ìƒ ìš”ì•½
â–  ì ‘ìˆ˜ ê¸°ê°„
â–  ì£¼ì˜ì‚¬í•­

ë§ˆì§€ë§‰ ì¤„: [ì°¸ê³ : ì§€ì›ì‚¬ì—… ê³µê³ ]
"""),
    ("human", "[ì§€ì›ì‚¬ì—… ë¬¸ë§¥]\n{context}\n\n[ì‚¬ìš©ì ì¡°ê±´]\n{question}\n\nìœ„ í˜•ì‹ì— ë§ì¶° ì¶”ì²œí•´ ì£¼ì„¸ìš”.")
])

# Fallback í”„ë¡¬í”„íŠ¸
fallback_prompt = ChatPromptTemplate.from_template("""
ì§ˆë¬¸: {question}

ë‚´ë¶€ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.
ì¼ë°˜ì ì¸ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ë‹µë³€:""")

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# ì²´ì¸ ìƒì„±
qt_chain = qt_prompt | llm | StrOutputParser()
multi_query_chain = multi_query_prompt | llm | StrOutputParser()
relevance_chain = relevance_check_prompt | llm | StrOutputParser()
fallback_chain = fallback_prompt | llm | StrOutputParser()

# ========================================
# í—¬í¼ í•¨ìˆ˜
# ========================================

def choose_prompt(question: str):
    """ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¼ ì ì ˆí•œ í”„ë¡¬í”„íŠ¸ ì„ íƒ"""
    recommend_keywords = ["ì¶”ì²œ", "ë§ëŠ”", "ì‹ ì²­í•  ìˆ˜ ìˆëŠ”", "ì§€ì›í•´ì£¼ëŠ”", 
                         "ì‚¬ì—… ì•Œë ¤ì¤˜", "í˜œíƒ", "ì§€ì›ê¸ˆ", "ì§€ì›ì‚¬ì—…"]
    law_keywords = ["ì •ì˜", "ìê²©", "ìš”ê±´", "ì§€ì›ë²•", "ë²•ì—ì„œ", "ë²•ìƒ", "ì œë„", "ì‹œí–‰","ê·œì •"]
    
    if any(k in question for k in recommend_keywords):
        return recommend_prompt, "recommend_prompt"
    if any(k in question for k in law_keywords):
        return law_prompt, "law_prompt"
    return rag_prompt, "rag_prompt"

def format_docs_as_context(docs):
    """RAG í”„ë¡¬í”„íŠ¸ì— ë„£ê¸° ì¢‹ì€ ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ì¶œì²˜ ë©”íƒ€ë°ì´í„° í¬í•¨)"""
    if not docs:
        return ""
    parts = []
    for i, d in enumerate(docs, 1):
        if isinstance(d, Document):
            src = d.metadata.get("source", d.metadata.get("url", "unknown"))
            parts.append(f"[ë¬¸ì„œ {i}] (ì¶œì²˜: {src})\n{d.page_content}")
    return "\n\n---\n\n".join(parts)

def search_documents_hybrid(queries: List[str], k_per_query=10) -> List[Tuple[Document, float]]:
    """í•˜ì´ë¸Œë¦¬ë“œ ë¦¬íŠ¸ë¦¬ë²„ë¡œ ë©€í‹°ì¿¼ë¦¬ ê²€ìƒ‰"""
    all_docs_with_scores = []
    seen_hashes = set()
    
    if hybrid_retriever is None:
        # Fallback: Denseë§Œ ì‚¬ìš©
        print("[ê²€ìƒ‰] í•˜ì´ë¸Œë¦¬ë“œ ë¦¬íŠ¸ë¦¬ë²„ ì—†ìŒ, Denseë§Œ ì‚¬ìš©")
        for q in queries:
            try:
                docs = vectorstore.similarity_search_with_score(q, k=k_per_query)
                for doc, distance in docs:
                    doc_hash = hashlib.md5(doc.page_content[:500].encode()).hexdigest()
                    if doc_hash in seen_hashes:
                        continue
                    seen_hashes.add(doc_hash)
                    similarity = max(0.0, 1.0 - (distance / 2.0))
                    all_docs_with_scores.append((doc, similarity))
            except Exception as e:
                print(f"âš  ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return all_docs_with_scores
    
    # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
    print(f"[í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰] {len(queries)}ê°œ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰ ì¤‘...")
    for q in queries:
        try:
            docs_with_scores = hybrid_retriever.invoke(q)
            
            for doc, score in docs_with_scores:
                doc_hash = hashlib.md5(doc.page_content[:500].encode()).hexdigest()
                if doc_hash in seen_hashes:
                    continue
                seen_hashes.add(doc_hash)
                all_docs_with_scores.append((doc, score))
        except Exception as e:
            print(f"âš  í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
    
    # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
    all_docs_with_scores.sort(key=lambda x: x[1], reverse=True)
    
    return all_docs_with_scores

def filter_by_similarity(docs_with_scores, threshold=0.3):
    return [(doc, sim) for doc, sim in docs_with_scores if sim >= threshold]

def check_relevance(question, docs_with_scores):
    """LLMì—ê²Œ ìƒìœ„ Nê°œ ë¬¸ì„œë¡œ ê´€ë ¨ì„± ë¬¼ì–´ë³´ê¸° -> True/False ë°˜í™˜"""
    # docs_with_scoresëŠ” (doc, sim)
    top_docs = docs_with_scores[:5]
    if not top_docs:
        return False
    # Document ê°ì²´ì—ì„œ ì§ì ‘ ì ‘ê·¼
    docs_text = "\n\n---\n\n".join(
        f"[ë¬¸ì„œ {i+1}] (ì¶œì²˜: {getattr(doc.metadata,'get',lambda k, d=None: 'unknown')('source','unknown')})\n{getattr(doc,'page_content',str(doc))[:600]}"
        for i, (doc, _) in enumerate(top_docs)
    )
    try:
        res = relevance_chain.invoke({"question": question, "documents": docs_text})
        return "ê´€ë ¨ìˆìŒ" in res
    except Exception as e:
        print(f"âš  ê´€ë ¨ì„± ê²€ì¦ ì˜¤ë¥˜: {e}")
        return False

def web_search(query: str, k=3):
    """Tavily API ê¸°ë°˜ ì›¹ê²€ìƒ‰
    ì™¸ë¶€ ì†ŒìŠ¤ë§Œ Documentë¡œ ë³€í™˜ í•„ìš”"""
    try:
        retriever = TavilySearchAPIRetriever(k=k)
        results = retriever.invoke(query) 

        # Tavily ê²°ê³¼ë¥¼ Documentë¡œ ì •ê·œí™”
        docs = []
        for r in results:
            if isinstance(r, Document):
                docs.append(r)
            elif isinstance(r, dict):
                content = r.get("content") or r.get("snippet") or r.get("title") or str(r)
                docs.append(Document(
                    page_content=content,
                    metadata={"source": "web", "url": r.get("url", "unknown")}
                ))
            else:
                docs.append(Document(
                    page_content=str(r),
                    metadata={"source": "web"}
                ))
        return docs
        
    except Exception as e:
        raise RuntimeError(f"Tavily ì›¹ê²€ìƒ‰ ì˜¤ë¥˜: {e}")

# -----------------------
# RAG ì‘ë‹µ ìƒì„± í•¨ìˆ˜
# -----------------------
def rag_answer_from_docs(question: str, documents):
    """ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸(Document ë˜ëŠ” (doc,score) í˜¼í•©)ì„ ë°›ì•„ RAG ì‘ë‹µ ìƒì„±
    ì´ë¯¸ Document í˜•ì‹ì´ë¯€ë¡œ tupleë§Œ í’€ì–´ì£¼ë©´ ë¨"""
    # tuple í˜•ì‹ì´ë©´ Documentë§Œ ì¶”ì¶œ
    docs = []
    for item in documents:
        if isinstance(item, tuple):
            docs.append(item[0])  # (doc, score) â†’ doc
        else:
            docs.append(item)     # ì´ë¯¸ Document
    
    context = format_docs_as_context(docs)
    if not context.strip():
        print("âš  context ë¹„ì–´ìˆìŒ -> fallback LLMë¡œ ì´ë™")
        return fallback_chain.invoke({"question": question})

    prompt, pname = choose_prompt(question)
    print(f"[í”„ë¡¬í”„íŠ¸ ì‚¬ìš©] {pname} (ë¬¸ì„œ ê¸°ë°˜)")
    
    try:
        answer = (prompt | llm | StrOutputParser()).invoke({
            "context": context,
            "question": question
        })
        return answer
    except Exception as e:
        print(f"âš  RAG ìƒì„± ì˜¤ë¥˜: {e}")
        return fallback_chain.invoke({"question": question})
    
# ========================================
# ë©”ì¸ RAG í•¨ìˆ˜
# ========================================
def multi_query_rag_with_cache(question: str, top_k=10, similarity_threshold=0.3):
    """
    ì „ì²´ íë¦„:
      1) ì¿¼ë¦¬ íŠ¸ëœìŠ¤í¼ (QT)
      2) ë©€í‹°ì¿¼ë¦¬ ìƒì„± (MQ)
      3) ë²¡í„°ê²€ìƒ‰ (ë©€í‹°ì¿¼ë¦¬)
      4) ìœ ì‚¬ë„ í•„í„°ë§
        - ë¬¸ì„œ ìˆìŒ -> LLM ê´€ë ¨ì„± ê²€ì¦
        - ë¬¸ì„œ ì—†ìŒ / ê´€ë ¨ ì—†ìŒ -> ì›¹ê²€ìƒ‰ ë˜ëŠ” Fallback (ë‹¨ì¼ ë¶„ê¸°)
    """
    
    def execute_rag():
        """ì‹¤ì œ RAG ë¡œì§ (ìºì‹œ ë¯¸ìŠ¤ ì‹œ í˜¸ì¶œë¨)"""
        # 1) Query Transform
        try:
            qt_query = qt_chain.invoke({"question": question})
        except Exception as e:
            qt_query = question
        print(f"[QT] ë³€í™˜: {qt_query}")
        
        # 2) Multi Query
        try:
            mq_text = multi_query_chain.invoke({"question": qt_query})
            queries = [line.strip() for line in mq_text.splitlines() if line.strip()]
        except Exception as e:
            queries = [qt_query]
        print(f"[ë©€í‹°ì¿¼ë¦¬] {len(queries)}ê°œ: {queries}")
        
        # 3) í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ âœ…
        all_docs = search_documents_hybrid(queries, k_per_query=10)
        print(f"[ê²€ìƒ‰] ì´ {len(all_docs)}ê°œ ë¬¸ì„œ í›„ë³´")
        
        # 4) ìœ ì‚¬ë„ í•„í„°ë§
        filtered_docs = filter_by_similarity(all_docs, similarity_threshold)
        print(f"[í•„í„°ë§] ìœ ì‚¬ë„ >={similarity_threshold}: {len(filtered_docs)}ê°œ")
        
        # 5) ê´€ë ¨ì„± ê²€ì¦
        is_relevant = False
        if filtered_docs:
            print("[ê´€ë ¨ì„± ê²€ì¦] LLM ê²€ì¦ ì¤‘...")
            is_relevant = check_relevance(question, filtered_docs)
        
        # 6) ìµœì¢… ë¶„ê¸°
        if is_relevant:
            print("âœ… ë‚´ë¶€ ë¬¸ì„œ ê´€ë ¨ìˆìŒ â†’ ë‚´ë¶€ RAG")
            useful = filtered_docs[:top_k]
            return rag_answer_from_docs(question, useful)
        else:
            print("âš ï¸ ë‚´ë¶€ ë¬¸ì„œ ì—†ìŒ/ë¬´ê´€ â†’ ì›¹ê²€ìƒ‰")
            try:
                web_docs = web_search(question)
                if not web_docs:
                    return fallback_chain.invoke({"question": question})
                return rag_answer_from_docs(question, web_docs)
            except Exception as e:
                print(f"âš  ì›¹ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                return fallback_chain.invoke({"question": question})
    
    # ìºì‹œ ì¡°íšŒ ë˜ëŠ” RAG ì‹¤í–‰
    return multi_level_cache.get(question, execute_rag)


# ========================================
# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
# ========================================

if __name__ == "__main__":
    test_cases = [
       "ì„œìš¸ì—ì„œ AI ì±—ë´‡ ì°½ì—…ì„ í•˜ë ¤ê³  í•˜ëŠ”ë° ë°›ì„ ìˆ˜ ìˆëŠ” ì§€ì›ì‚¬ì—… ìˆë‚˜ìš”?",
        "ì„œìš¸ì— ìˆëŠ” ë™ë¬¼ë³‘ì› ì•Œë ¤ì£¼ì„¸ìš”",
        "ì˜¤ëŠ˜ ì„œìš¸ ë‚ ì”¨ ì•Œë ¤ì¤˜",
    ]
    
    print("\n" + "ğŸš€ "*30)
    print("ê³ ê¸‰ RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("- í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (Dense + BM25)")
    print("- ë©€í‹°ë ˆë²¨ ìºì‹± (L1 ë©”ëª¨ë¦¬ + L2 ë²¡í„°DB)")
    print("ğŸš€ "*30 + "\n")
    
    for i, q in enumerate(test_cases, 1):
        print("\n" + "="*80)
        print(f"[í…ŒìŠ¤íŠ¸ {i}/{len(test_cases)}] {q}")
        print("="*80)
        
        try:
            ans = multi_query_rag_with_cache(q)
            print("\n" + "ğŸ“ "*30)
            print("[ìµœì¢… ë‹µë³€]")
            print("ğŸ“ "*30)
            print(ans)
        except Exception as e:
            print(f"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
    
    # ìµœì¢… ìºì‹œ í†µê³„
    multi_level_cache.stats()
    
    print("\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
