import os
import warnings
from dotenv import load_dotenv
from typing import List, Tuple
import hashlib

from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.retrievers import TavilySearchAPIRetriever, BM25Retriever
from langchain_core.documents import Document


from enum import Enum
from pydantic import BaseModel

warnings.filterwarnings("ignore")
load_dotenv()

api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise ValueError("OPENAI_API_KEY ì—†ìŒ! .env í™•ì¸í•´ì¤˜")

TAVILY_API_KEY = os.getenv("TAVILY_API_KEY") 

embedding_model = OpenAIEmbeddings(model="text-embedding-3-small")

# ë²¡í„°DB ë¡œë“œ

# 1) ë©”ì¸ ë¬¸ì„œìš© ë²¡í„°DB
vectorstore = Chroma(
    persist_directory="./chroma_startup_all",
    collection_name="startup_all_rag",
    embedding_function=embedding_model,
)

# 2) ìºì‹œ ì „ìš© ë²¡í„°DB 
cache_vectorstore = Chroma(
    persist_directory="./chroma_cache",
    collection_name="semantic_cache",
    embedding_function=embedding_model,
)

# ë°±í„°ìŠ¤í† ì–´ ìƒíƒœ í™•ì¸ í•¨ìˆ˜
def check_vectorstore(name, store):
    try:
        data = store.get()
        ids = data.get("ids", [])
        print(f"{name} ë¡œë“œ ì™„ë£Œ / ì´ ê°œìˆ˜: {len(ids)}")
        return len(ids) > 0
    except Exception as e:
        print(f"âš  {name} ìƒíƒœ í™•ì¸ ì¤‘ ì—ëŸ¬:", e)
        return False

main_db_ok = check_vectorstore("ë©”ì¸ ë²¡í„°DB", vectorstore)
cache_db_ok = check_vectorstore("ìºì‹œ ë²¡í„°DB", cache_vectorstore)

# ========================================
# í•˜ì´ë¸Œë¦¬ë“œ ë¦¬íŠ¸ë¦¬ë²„ êµ¬í˜„
# ========================================

class HybridRetriever:
    """Dense(ì˜ë¯¸) + BM25(í‚¤ì›Œë“œ) í•˜ì´ë¸Œë¦¬ë“œ ë¦¬íŠ¸ë¦¬ë²„"""
    
    def __init__(self,
        vectorstore: Chroma,
        documents: List[Document],
        k: int = 10,    #-> ê²€ìƒ‰ í›„ ìµœì¢…ì ìœ¼ë¡œ ë°˜í™˜í•  ë¬¸ì„œ
        dense_weight: float = 0.6,  # ì˜ë¯¸ê¸°ë°˜ ë¹„ì¤‘
        bm25_weight: float = 0.4,): # í‚¤ì›Œë“œê¸°ë°˜ ë¹„ì¤‘
        # íŒŒë¼ë¯¸í„° ì €ì¥
        self.vectorstore = vectorstore
        self.k = k
        self.dense_weight = dense_weight
        self.bm25_weight = bm25_weight
        self._internal_k = k * 2  # ë” ë§ì´ ê°€ì ¸ì˜¨ í›„ ìœµí•©( ìµœì¢… ë°˜í™˜ì€ 10ê°œì´ê³  ë‚´ë¶€ì—ì„œëŠ” ê·¸ê²ƒë³´ë‹¤ ë” ë§ì´ ê°€ì ¸ì™€ì„œ ë¹„êµ í›„ 10ê°œë§Œ ë°˜í™˜)
        
        # Dense retriever -> ë²¡í„°ìŠ¤í† ì–´ ê¸°ë°˜ í˜•íƒœ í•„ìš”
        self.dense_retriever = vectorstore.as_retriever(
            search_kwargs={"k": self._internal_k}
        )
        
        # BM25 retriever -> documents ë¦¬ìŠ¤íŠ¸ í˜•íƒœ í•„ìš”
        print(f"[í•˜ì´ë¸Œë¦¬ë“œ] BM25 ì¸ë±ìŠ¤ ìƒì„± ì¤‘... (ë¬¸ì„œ ìˆ˜: {len(documents)})")
        self.bm25_retriever = BM25Retriever.from_documents(documents)
        self.bm25_retriever.k = self._internal_k
        print(f"[í•˜ì´ë¸Œë¦¬ë“œ] BM25 ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
    
    def invoke(self, query: str) -> List[Tuple[Document, float]]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤í–‰ (RRF ìœµí•©)"""
        # Dense ê²€ìƒ‰
        dense_docs = self._search_dense(query)  # ë©”ì„œë“œ ì•ì— ë¶™ì€ _ëŠ” ë‚´ë¶€ ì „ìš©ì´ë¼ëŠ” ëœ». ì™¸ë¶€ì—ì„œ ì§ì ‘ í˜¸ì¶œ ì•ˆë¨
        # BM25 ê²€ìƒ‰
        bm25_docs = self._search_bm25(query)
        # RRFë¡œ ìœµí•©
        fused_docs = self._fuse_results(dense_docs, bm25_docs)
        
        return fused_docs[:self.k]  # internal_k * 2 ê°œë¥¼ ì“´ ë’¤,ìµœì¢…ì ìœ¼ë¡œ ì§€ì •í•œ kë§Œ ë°˜í™˜
    
    def _search_dense(self, query: str) -> List[Document]:
        """Dense ê²€ìƒ‰""" # -> ë²¡í„°ìŠ¤í† ì–´ ê¸°ë°˜ ê²€ìƒ‰
        try:
            return self.dense_retriever.invoke(query)
        except Exception as e:
            print(f"âš  Dense ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def _search_bm25(self, query: str) -> List[Document]:
        """BM25 ê²€ìƒ‰""" # -> bm25 ì¸ë±ìŠ¤ë§Œ ê²€ìƒ‰
        try:
            return self.bm25_retriever.invoke(query)    # ì´ ì‹ì—ì„œ bm25 ê³„ì‚°ì´ ëª¨ë‘ ì´ë£¨ì–´ì§
        except Exception as e:
            print(f"âš  BM25 ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def _fuse_results(self, dense_docs: List[Document],  bm25_docs: List[Document]) -> List[Tuple[Document, float]]:
        """RRF(Reciprocal Rank Fusion)ë¡œ ê²€ìƒ‰ ê²°ê³¼ í†µí•©"""
        scores = {}
        doc_map = {}
        
        # RRF ê³µì‹ = 1/ (rank +1)  -> ë­í¬ê°€ ë’¤ì¸ê±°ëŠ” ì•ì— ìˆëŠ” ê²ƒë³´ë‹¤ ì°¨ì´ë¥¼ ë” ë§ì´ ë‚˜ê²Œ í•˜ë ¤ê³ 
        # Dense ì ìˆ˜ ê³„ì‚°
        for rank, doc in enumerate(dense_docs): # dense_docs ë¦¬ìŠ¤íŠ¸ ìˆœì„œë¥¼ rankë¡œ ë°”ê¿”ì¤€ë‹¤
            key = self._get_doc_key(doc) # ë¬¸ì„œë¥¼ ê³ ìœ í•˜ê²Œ ë‚˜íƒ€ë‚´ëŠ” ê°’ dictì˜ keyë¡œ ì“°ê¸° ìœ„í•´ í•„ìš”
            scores[key] = scores.get(key, 0.0) + self.dense_weight / (rank + 1) # ìˆœìœ„ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°ì‹/ dense_weight(ì˜ˆ: 0.6)ì´ ê³±í•´ì§€ë©´ì„œ Dense ê²€ìƒ‰ ì˜í–¥ë ¥ì„ ë°˜ì˜
            doc_map[key] = doc  # ì ìˆ˜ëŠ” scoresì— ì €ì¥ë˜ê³ , ë¬¸ì„œ ìì²´ëŠ” doc_mapì— ì €ì¥
        
        # BM25 ì ìˆ˜ ê³„ì‚°
        for rank, doc in enumerate(bm25_docs):
            key = self._get_doc_key(doc)
            scores[key] = scores.get(key, 0.0) + self.bm25_weight / (rank + 1)
            doc_map[key] = doc
        
        # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
        ranked_keys = sorted(scores.keys(), key=lambda k: scores[k], reverse=True)  # lambda k: scores[k] -> def k:
                                                                                    #                            return scores[k]   -> ì´ê²ƒê³¼ ê°™ì€ ì˜ë¯¸
        # (Document, score) í˜•íƒœë¡œ ë°˜í™˜
        return [(doc_map[k], scores[k]) for k in ranked_keys]
    
    @staticmethod   # í´ë˜ìŠ¤ ë‚´ë¶€ì—ì„œ ì“°ì§€ë§Œ self í•„ìš”ì—†ìŒ
    def _get_doc_key(doc: Document) -> str:
        """ë¬¸ì„œ ì‹ë³„ìš© í‚¤ ìƒì„± (í•´ì‹œ ê¸°ë°˜)"""
        content_hash = hashlib.md5(doc.page_content[:500].encode()).hexdigest() # í•´ì‹œìƒì„±ì€ 500ìë§Œìœ¼ë¡œë„ ë¬¸ì„œêµ¬ë³„ ì¶©ë¶„
        return content_hash                                                     # encode-> í•´ì‹œ í•¨ìˆ˜ëŠ” ë°”ì´íŠ¸ë§Œ ë°›ìŒ


# í•˜ì´ë¸Œë¦¬ë“œ ë¦¬íŠ¸ë¦¬ë²„ ì´ˆê¸°í™”
print("\n[ì´ˆê¸°í™”] í•˜ì´ë¸Œë¦¬ë“œ ë¦¬íŠ¸ë¦¬ë²„ ìƒì„± ì¤‘...")
hybrid_retriever = None
if main_db_ok:
    try:
        all_docs_data = vectorstore.get(include=['documents', 'metadatas'])
        all_documents = [
            Document(page_content=doc, metadata=meta or {})
            for doc, meta in zip(
                all_docs_data.get('documents', []),
                all_docs_data.get('metadatas', [])
            )
        ]
        
        if len(all_documents) > 0:
            hybrid_retriever = HybridRetriever(
                vectorstore=vectorstore,
                documents=all_documents,
                k=10,
                dense_weight=0.6,
                bm25_weight=0.4
            )
            print("[ì´ˆê¸°í™”] í•˜ì´ë¸Œë¦¬ë“œ ë¦¬íŠ¸ë¦¬ë²„ ì¤€ë¹„ ì™„ë£Œ!\n")
        else:
            print("âš  ë¬¸ì„œê°€ ì—†ì–´ í•˜ì´ë¸Œë¦¬ë“œ ë¦¬íŠ¸ë¦¬ë²„ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âš  í•˜ì´ë¸Œë¦¬ë“œ ë¦¬íŠ¸ë¦¬ë²„ ìƒì„± ì‹¤íŒ¨: {e}")
        print("âš  Dense ë¦¬íŠ¸ë¦¬ë²„ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")

# ========================================
# ìºì‹œ ì‹œìŠ¤í…œ (ë¶„ë¦¬ëœ ë²¡í„°DB ì‚¬ìš©)
# ========================================

class SimpleCache:
    """L1 ìºì‹œ - ì™„ì „ ì¼ì¹˜ ê¸°ë°˜ ë©”ëª¨ë¦¬ ìºì‹œ""" # ê°€ì¥ ì•ˆ ì“°ì¸ í‚¤ ìë™ ì‚­ì œë²„ì „
    def __init__(self, max_size=100):
        self.cache = {}
        self.max_size = max_size        # í¬ê¸° ì œí•œ
        self.access_count = {}          # ì ‘ê·¼ íšŸìˆ˜ ì œí•œ

    def get(self, key):
        if key in self.cache:
            self.access_count[key] = self.access_count.get(key, 0) + 1
            print(f" [L1 ìºì‹œ íˆíŠ¸] key: {key[:50]}...")
            return self.cache[key]
        return None
    
    def set(self, key, value):
        # LFU ê¸°ë°˜ ìºì‹œ ì œê±° -> ìºì‹œë¥¼ ì¼ì •í•œ í¬ê¸° ì•ˆì—ì„œ ìœ ì§€í•˜ê³ , ê°€ì¥ ëœ ì‚¬ìš©ëœ í‚¤ë¥¼ ìë™ì„ ì œê±°
        if len(self.cache) >= self.max_size:
            if self.access_count:
                least_used = min(self.access_count, key=self.access_count.get)  # key=self.access_count.get -> ê° keyì˜ valuseë¥¼ ê°€ì ¸ì˜¤ëŠ” ê°’
                del self.cache[least_used]  # ìºì‹œì— ì €ì¥ëœ ì‹¤ì œ ê°’(value) ì„ ì‚­ì œ
                del self.access_count[least_used]   # í•´ë‹¹ keyì˜ ì ‘ê·¼ íšŸìˆ˜ ê¸°ë¡ ì‚­ì œ
        
        self.cache[key] = value     # ìºì‹œì— ê°’ ì €ì¥
        self.access_count[key] = 0  # ìºì‹œ ì ‘ê·¼ íšŸìˆ˜ ì¹´ìš´íŒ…

    def stats(self):
        return {
            "size": len(self.cache),
            "max_size": self.max_size,
            "total_accesses": sum(self.access_count.values())
        }


class SemanticCache:
    """L2 ìºì‹œ - ì˜ë¯¸ì  ìœ ì‚¬ë„ ê¸°ë°˜ ìºì‹œ (ë¶„ë¦¬ëœ ìºì‹œ ì „ìš© ë²¡í„°DB ì‚¬ìš©)
    DBì—ì„œ ìœ ì‚¬í•œ IDë¥¼ ì°¾ê³ , ê·¸ IDë¡œ ë©”ëª¨ë¦¬ ìºì‹œì—£ ì‹¤ì œ ë‹µë³€ì„ ê°€ì ¸ì˜¤ëŠ” êµ¬ì¡°"""
    
    def __init__(self, cache_vectorstore: Chroma, similarity_threshold=0.85):
        self.cache_vectorstore = cache_vectorstore  # ìºì‹œ ì „ìš© DB
        self.similarity_threshold = similarity_threshold
        self.cache_data = {}  # {cache_id: response}
        self.cache_metadata = {}  # {cache_id: {query, timestamp, hits}}
        self._load_existing_cache()  # í”„ë¡œê·¸ë¨ ì‹œì‘ ì‹œ DBì— ìˆëŠ” ìºì‹œë¥¼ ë©”ëª¨ë¦¬ë¡œ ë¶ˆëŸ¬ì˜¤ëŠ” ê²ƒ
        
    
    def _load_existing_cache(self):
        """ê¸°ì¡´ ìºì‹œ ë°ì´í„° ë¡œë“œ"""
        try:
            existing = self.cache_vectorstore.get(include=['metadatas'])
            for meta in existing.get('metadatas', []):
                cache_id = meta.get('cache_id') # ê° ë©”íƒ€ë°ì´í„° í•­ëª©ì—ì„œ ìºì‹œ ê³ ìœ  ID ê°€ì ¸ì˜¤ê¸°/ IDê°€ ì¡´ì¬í•˜ë©´ â†’ ë©”ëª¨ë¦¬ì— ì €ì¥ ê°€ëŠ¥
                if cache_id:    # ë§Œì•½ DB ë°ì´í„°ê°€ ì´ìƒí•´ì„œ cache_idê°€ ì—†ìœ¼ë©´ ì €ì¥í•˜ì§€ ì•ŠìŒ. ì•ˆì „ì¥ì¹˜
                    self.cache_metadata[cache_id] = meta    # cache_idë¥¼ í‚¤ë¡œ í•´ì„œ ë©”íƒ€ë°ì´í„° ì „ì²´ë¥¼ ë©”ëª¨ë¦¬ì— ì €ì¥
            print(f"[L2 ìºì‹œ] ê¸°ì¡´ ìºì‹œ {len(self.cache_metadata)}ê°œ ë³µêµ¬")
        except Exception as e:
            print(f"âš  ìºì‹œ ë³µêµ¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def get(self, query: str):
        """ìœ ì‚¬í•œ ì§ˆë¬¸ì´ ìºì‹œì— ìˆìœ¼ë©´ ë°˜í™˜"""
        try:
            # ìºì‹œ ì „ìš© DBì—ì„œ ìœ ì‚¬ë„ ê²€ìƒ‰
            similar_docs = self.cache_vectorstore.similarity_search_with_score(query, k=1)  # ìºì‹œ ì „ìš© ë²¡í„° DB(Chroma)ì—ì„œ ê°€ì¥ ìœ ì‚¬í•œ 1ê°œ ë¬¸ì„œ ê²€ìƒ‰
            
            if similar_docs:
                doc, distance = similar_docs[0] # ìœ ì‚¬ë„ê°€ ê°€ì¥ ë†’ì€ 0ë²ˆì§¸ ë¬¸ì„œë¥¼ ê°€ì ¸ì˜¨ ê²ƒ
                # Chroma distance -> ìœ ì‚¬ë„ ë³€í™˜
                similarity = max(0.0, 1.0 - (distance / 2.0))
                
                if similarity >= self.similarity_threshold: # ìœ ì‚¬ë„ê°€ ì„ê³„ê°’ ì´ìƒì¼ ë•Œ
                    cache_id = doc.metadata.get('cache_id') # ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„° ê³ ìœ ID(cache_id) ê°€ì ¸ì˜¤ê¸°
                    
                    # ì „ì²´ ì‘ë‹µ ë°˜í™˜ (ë©”íƒ€ë°ì´í„°ê°€ ì•„ë‹Œ ì‹¤ì œ ìºì‹œ ë°ì´í„°ì—ì„œ)
                    if cache_id in self.cache_data:
                        full_response = self.cache_data[cache_id]
                        # L2ìºì‹œëŠ” ê²€ìƒ‰ ê²°ê³ ì™€ ì‹¤ì œ ì‘ë‹µì„ ë¶„ë¦¬í•œë‹¤/ê²€ìƒ‰ -> DB, ì‹¤ì œì‘ë‹µ -> ë©”ëª¨ë¦¬
                        
                        # ìºì‹œ íˆíŠ¸ ì¹´ìš´íŠ¸ ì¦ê°€
                        if cache_id in self.cache_metadata:
                            self.cache_metadata[cache_id]['hits'] = \
                                self.cache_metadata[cache_id].get('hits', 0) + 1
                        
                        print(f"[L2 ìºì‹œ íˆíŠ¸] ìœ ì‚¬ë„: {similarity:.3f}, cache_id: {cache_id}")
                        return full_response
                    else:
                        print(f"âš  [L2 ê²½ê³ ] cache_id {cache_id}ê°€ cache_dataì— ì—†ìŒ")

            return None
        except Exception as e:
            print(f"âš  L2 ìºì‹œ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return None
    
    def set(self, query: str, response: str):
        """ì§ˆë¬¸ê³¼ ì‘ë‹µì„ ìºì‹œì— ì €ì¥"""
        try:
            import time
            # ê³ ìœ  ID ìƒì„± -> í˜•ì‹ì€ cache_ë©”ëª¨ë¦¬_ì¸ë±ìŠ¤_í˜„ì¬íƒ€ì„ìŠ¤íƒ¬í”„
            cache_id = f"cache_{len(self.cache_data)}_{int(time.time())}"
            
            # ì „ì²´ ì‘ë‹µì„ cache_dataì— ì €ì¥
            self.cache_data[cache_id] = response
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥ (ìš”ì•½ë³¸ë§Œ)
            metadata = {
                'cache_id': cache_id,
                'response_preview': response[:200] + "..." if len(response) > 200 else response,
                'timestamp': time.time(),
                'hits': 0
            }
            self.cache_metadata[cache_id] = metadata
            
            # ìºì‹œ ì „ìš© ë²¡í„°DBì— ì €ì¥
            self.cache_vectorstore.add_texts(
                texts=[query],
                metadatas=[metadata],
                ids=[cache_id]
            )
            self.cache_vectorstore.persist()

            print(f"[L2 ìºì‹œ ì €ì¥] cache_id: {cache_id}, ì‘ë‹µ ê¸¸ì´: {len(response)}ì")
        except Exception as e:
            print(f"âš  L2 ìºì‹œ ì €ì¥ ì˜¤ë¥˜: {e}")
    
    def stats(self):
        return {
            "size": len(self.cache_data),
            "total_hits": sum(m.get('hits', 0) for m in self.cache_metadata.values()),
            "avg_hits": sum(m.get('hits', 0) for m in self.cache_metadata.values()) / max(len(self.cache_metadata), 1)
        }


class MultiLevelCache:
    """ë©€í‹°ë ˆë²¨ ìºì‹œ - L1(ì™„ì „ì¼ì¹˜) + L2(ì˜ë¯¸ì  ìœ ì‚¬ë„)"""
    
    def __init__(self, cache_vectorstore: Chroma):
        self.l1_cache = SimpleCache(max_size=100)
        self.l2_cache = SemanticCache(
            cache_vectorstore=cache_vectorstore,
            similarity_threshold=0.85
        )
        self.stats_data = {
            'l1_hits': 0,
            'l2_hits': 0,
            'misses': 0
        }
    
    def get(self, key: str, execute_rag_callable):
        """ìºì‹œ ì¡°íšŒ > ë‚´ë¶€ë¬¸ì„œ > ì›¹ì„œì¹˜ > LLM í˜¸ì¶œ ìˆœì„œ"""
        # L1 ìºì‹œ í™•ì¸
        cached = self.l1_cache.get(key)
        if cached:
            self.stats_data['l1_hits'] += 1
            print(' L1 cache hit')
            return cached
    
        # L2 ìºì‹œ í™•ì¸
        cached = self.l2_cache.get(key)
        if cached:
            self.stats_data['l2_hits'] += 1
            print(' L2 cache hit')
            self.l1_cache.set(key, cached)
            return cached
                

        # ìºì‹œ ë¯¸ìŠ¤ â†’ execute_rag() í˜¸ì¶œ
        self.stats_data['misses'] += 1
        print('âŒ [ìºì‹œ ë¯¸ìŠ¤] RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰')
        response = execute_rag_callable(key)# execute_rag ì•ˆì—ì„œ QT â†’ MQ â†’ ë‚´ë¶€ ë¬¸ì„œ ê²€ìƒ‰ â†’ ìœ ì‚¬ë„ í•„í„°ë§ â†’ ê´€ë ¨ì„± ê²€ì¦ â†’ ì›¹ â†’ LLM ìˆ˜í–‰

        # ê²°ê³¼ë¥¼ L1/L2 ìºì‹œì— ì €ì¥
        self.l1_cache.set(key, response)
        self.l2_cache.set(key, response)
        print("[ìºì‹œ ì €ì¥ ì™„ë£Œ]\n")
         
        return response
        
    def stats(self):
        total = sum(self.stats_data.values())
        hit_rate = (self.stats_data['l1_hits'] + self.stats_data['l2_hits']) / max(total, 1) * 100
        
        print("\n" + "="*60)
        print("ğŸ“Š ìºì‹œ í†µê³„")
        print("="*60)
        print(f"L1 íˆíŠ¸: {self.stats_data['l1_hits']}")
        print(f"L2 íˆíŠ¸: {self.stats_data['l2_hits']}")
        print(f"ìºì‹œ ë¯¸ìŠ¤: {self.stats_data['misses']}")
        print(f"ì´ ìš”ì²­: {total}")
        print(f"ìºì‹œ íˆíŠ¸ìœ¨: {hit_rate:.1f}%")
        print(f"\nL1 ìƒì„¸: {self.l1_cache.stats()}")
        print(f"L2 ìƒì„¸: {self.l2_cache.stats()}")
        print("="*60 + "\n")

# ========================================
# í”„ë¡¬í”„íŠ¸ ì •ì˜
# ========================================

# ê´€ë ¨ì„± ê²€ì¦ í”„ë¡¬í”„íŠ¸
relevance_check_prompt = ChatPromptTemplate.from_template("""
ë‹¹ì‹ ì€ ë¬¸ì„œì™€ ì§ˆë¬¸ì˜ ê´€ë ¨ì„±ì„ ì—„ê²©í•˜ê²Œ íŒë‹¨í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

[ì§ˆë¬¸]
{question}

[ê²€ìƒ‰ëœ ë¬¸ì„œ ìƒ˜í”Œ]
{documents}

[íŒë‹¨ ê¸°ì¤€]
1. ì§ˆë¬¸ì˜ í•µì‹¬ ì£¼ì œì™€ ë¬¸ì„œì˜ ë‚´ìš©ì´ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ë˜ëŠ”ê°€?
2. ë¬¸ì„œê°€ ì§ˆë¬¸ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ”ê°€?
3. ë‹¨ìˆœíˆ ìœ ì‚¬í•œ ë‹¨ì–´ê°€ ìˆëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ì‹¤ì œ ë‹µë³€ ê°€ëŠ¥í•œ ë‚´ìš©ì¸ê°€?

[ì˜ˆì‹œ]
- "ì„œìš¸ ë™ë¬¼ë³‘ì›" vs "ì„œìš¸ ì°½ì—… ê³µê°„" â†’ ê´€ë ¨ì—†ìŒ (ì„œìš¸ë§Œ ê³µí†µ)
- "AI êµìœ¡" vs "ì°½ì—… êµìœ¡" â†’ ê´€ë ¨ì—†ìŒ (êµìœ¡ë§Œ ê³µí†µ, ì£¼ì œ ë‹¤ë¦„)
- "ì°½ì—… ì§€ì›ì‚¬ì—…" vs "ì°½ì—… ìê¸ˆ ì§€ì›" â†’ ê´€ë ¨ìˆìŒ (ì§ì ‘ ì—°ê´€)

ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¡œë§Œ ë‹µë³€: "ê´€ë ¨ìˆìŒ" ë˜ëŠ” "ê´€ë ¨ì—†ìŒ"

ë‹µë³€:""")

# Query Transformation
qt_prompt = ChatPromptTemplate.from_template("""
ë‹¤ìŒ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë²¡í„° ê²€ìƒ‰ì— ì í•©í•œ 'í•µì‹¬ í‚¤ì›Œë“œ ì¤‘ì‹¬ ë¬¸ì¥'ìœ¼ë¡œ ë°”ê¾¸ì„¸ìš”.
ë¶ˆí•„ìš”í•œ ë§ì€ ì œê±°í•˜ê³ , í•µì‹¬ ì¡°ê±´ë§Œ ë‚¨ê¸°ì„¸ìš”.

ì›ë³¸ ì§ˆë¬¸: {question}

ë³€í™˜ëœ ê²€ìƒ‰ìš© ë¬¸ì¥:""")

# ë©€í‹°ì¿¼ë¦¬ ìƒì„±
multi_query_prompt = ChatPromptTemplate.from_template("""
ë‹¤ìŒì§ˆë¬¸ì— ëŒ€í•´ 3ê°€ì§€ ë‹¤ë¥¸ ê´€ì ì˜ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ì„¸ìš”.
ê°ì¿¼ë¦¬ëŠ” ì„¸ ì¤„ë¡œ êµ¬ë¶„í•˜ì—¬ ì¶œë ¥í•˜ì„¸ìš”        
ë²ˆí˜¸ë‚˜ ì„¤ëª… ì—†ì´ ì¿¼ë¦¬ë§Œ ì¶œë ¥í•˜ì„¸ìš”

ì›ë³¸ì§ˆë¬¸: {question}""")

# ê¸°ë³¸ RAG í”„ë¡¬í”„íŠ¸
rag_prompt = ChatPromptTemplate.from_messages([
    ("system", """
ë‹¹ì‹ ì€ ì˜ˆë¹„Â·ì´ˆê¸° ì°½ì—…ìë¥¼ ë„ì™€ì£¼ëŠ” 'ì°½ì—… ì§€ì› í†µí•© AI ì–´ì‹œìŠ¤í„´íŠ¸'ì…ë‹ˆë‹¤.

[ì‚¬ìš© ê°€ëŠ¥í•œ ì •ë³´ ìœ í˜•]
- ì§€ì›ì‚¬ì—… ê³µê³  (announcement)
- ì‹¤íŒ¨/ì¬ë„ì „ ì‚¬ë¡€ (cases)
- ì°½ì—… ê³µê°„ ì •ë³´ (space)
- ë²•ë ¹: ì¤‘ì†Œê¸°ì—…ì°½ì—… ì§€ì›ë²• ë“± (law)
- í†µê³„, ë§¤ë‰´ì–¼ ë“± ì°¸ê³  ìë£Œ

[ë‹µë³€ ì›ì¹™]
1. ë°˜ë“œì‹œ ì œê³µëœ ë¬¸ë§¥(Context) ì•ˆì˜ ì •ë³´ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
2. ë¬¸ë§¥ì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³  ì†”ì§í•˜ê²Œ ë§í•˜ì„¸ìš”.
3. ì§ˆë¬¸ ì„±ê²©ì— ë”°ë¼ ë‹¤ìŒ ì •ë³´ ìœ í˜•ì„ ìš°ì„  í™œìš©í•˜ì„¸ìš”.
   - ì§€ì›ì‚¬ì—…Â·ì‹ ì²­ ê°€ëŠ¥ ì—¬ë¶€ â†’ announcement
   - ë²•ì  ì •ì˜Â·ìê²© ìš”ê±´ â†’ law
   - ì¡°ì–¸Â·ì£¼ì˜ì  â†’ cases
   - ê³µê°„Â·ì…ì£¼ â†’ space
4. í•µì‹¬ ë‹µë³€ í›„ í•„ìš”í•˜ë©´ bulletë¡œ ì •ë¦¬í•˜ì„¸ìš”.
5. ë§ˆì§€ë§‰ì— ì°¸ê³  ê·¼ê±° ìœ í˜•ì„ ìš”ì•½í•˜ì„¸ìš”.
"""),
    ("human", "[ë¬¸ë§¥]\n{context}\n\n[ì§ˆë¬¸]\n{question}\n\n[ë‹µë³€]")
])

# ë²•ë ¹ ì „ìš© í”„ë¡¬í”„íŠ¸
law_prompt = ChatPromptTemplate.from_messages([
    ("system", """
ë‹¹ì‹ ì€ ì¤‘ì†Œê¸°ì—…ì°½ì—… ì§€ì›ë²•ì„ ë°”íƒ•ìœ¼ë¡œ ì°½ì—… ì œë„ì™€ ìš”ê±´ì„ ì„¤ëª…í•˜ëŠ” AIì…ë‹ˆë‹¤.

[ê·œì¹™]
1. ë°˜ë“œì‹œ ë¬¸ë§¥ì— ìˆëŠ” ë²•ë ¹ ë‚´ìš©ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
2. ê°€ëŠ¥í•˜ë©´ ì¡°ë¬¸ ë²ˆí˜¸(ì œâ—‹ì¡°)ë¥¼ í•¨ê»˜ ì œì‹œí•˜ì„¸ìš”.
3. ë¬¸ë§¥ì— ì—†ëŠ” ë‚´ìš©ì€ "ì œê³µëœ ë²•ë ¹ ë¬¸ì„œì—ì„œ í•´ë‹¹ ë‚´ìš©ì€ í™•ì¸ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤."ë¼ê³  ë‹µí•˜ì„¸ìš”.
4. ë‹µë³€ ëì— "â€» ë³¸ ë‹µë³€ì€ ì¼ë°˜ ì •ë³´ ì œê³µì´ë©°, êµ¬ì²´ì ì¸ ë²•ë¥  ìë¬¸ì€ ì•„ë‹™ë‹ˆë‹¤."ë¥¼ í¬í•¨í•˜ì„¸ìš”.
"""),
    ("human", "[ë²•ë ¹ ë¬¸ë§¥]\n{context}\n\n[ì§ˆë¬¸]\n{question}\n\n[ì„¤ëª…]")
])

# ì§€ì›ì‚¬ì—… ì¶”ì²œ í”„ë¡¬í”„íŠ¸
recommend_prompt = ChatPromptTemplate.from_messages([
    ("system", """
ë‹¹ì‹ ì€ ì˜ˆë¹„Â·ì´ˆê¸° ì°½ì—…ìì—ê²Œ ê°€ì¥ ì í•©í•œ 'ì§€ì›ì‚¬ì—…ì„ ì¶”ì²œí•˜ëŠ” ì „ë¬¸ê°€ AI'ì…ë‹ˆë‹¤.

[ëª©í‘œ]
ì‚¬ìš©ìì˜ ì¡°ê±´(ë‚˜ì´, ì§€ì—­, ì—…ì¢…, ì°½ì—… ë‹¨ê³„ ë“±)ì„ ê¸°ì¤€ìœ¼ë¡œ
'ì‹¤ì§ˆì ì¸ ë„ì›€ì´ ë˜ëŠ” ì‚¬ì—…(ìê¸ˆÂ·ê³µê°„Â·R&DÂ·ì‹œì œí’ˆÂ·êµìœ¡)'ì„ ìš°ì„ ì ìœ¼ë¡œ ì¶”ì²œí•©ë‹ˆë‹¤.

[ì¶”ì²œ ìš°ì„ ìˆœìœ„]
1. í˜„ê¸ˆì„± ì§€ì›(ì‚¬ì—…í™” ìê¸ˆ, ì‹œì œí’ˆ ì œì‘ë¹„, R&D)
2. ì…ì£¼ ê³µê°„, ì¥ë¹„ ì§€ì›
3. ì—‘ì…€ëŸ¬ë ˆì´íŒ…, ë©˜í† ë§
4. ë‹¨ìˆœ êµìœ¡/íŠ¹ê°•ì€ ë§ˆì§€ë§‰ ìˆœìœ„

[ì¶”ì²œ ê·œì¹™]
1. ë°˜ë“œì‹œ announcement ë¬¸ì„œë§Œ ì‚¬ìš©
2. ì‚¬ìš©ì ì¡°ê±´ê³¼ 'ì§€ì—­Â·ì—°ë ¹Â·ë‹¨ê³„Â·ì—…ì¢…'ì´ ëª…í™•íˆ ë§ëŠ” ê²ƒë§Œ ì¶”ì²œ
3. ìµœëŒ€ 2ê°œê¹Œì§€ë§Œ ì¶”ì²œ
4. ì¡°ê±´ì´ ë§ëŠ” ì‚¬ì—…ì´ ì—†ìœ¼ë©´ ì†”ì§í•˜ê²Œ ë§í•˜ê¸°
5. ITÂ·ì„œë¹„ìŠ¤ì—…ì´ë©´ 'ê¸°ìˆ Â·ì½˜í…ì¸ Â·í”Œë«í¼' í‚¤ì›Œë“œ í¬í•¨ ì‚¬ì—… ìš°ì„ 

[ì¶œë ¥ í˜•ì‹]
â–  ì¶”ì²œ ì‚¬ì—…ëª…
â–  ì™œ ì´ ì‚¬ìš©ìì—ê²Œ ì í•©í•œì§€
â–  ì§€ì› ë‚´ìš©(ìê¸ˆ/ê³µê°„/êµìœ¡ ì¤‘ ë¬´ì—‡ì¸ì§€ ëª…í™•íˆ)
â–  ì‹ ì²­ ëŒ€ìƒ ìš”ì•½
â–  ì ‘ìˆ˜ ê¸°ê°„
â–  ì£¼ì˜ì‚¬í•­

ë§ˆì§€ë§‰ ì¤„: [ì°¸ê³ : ì§€ì›ì‚¬ì—… ê³µê³ ]
"""),
    ("human", "[ì§€ì›ì‚¬ì—… ë¬¸ë§¥]\n{context}\n\n[ì‚¬ìš©ì ì¡°ê±´]\n{question}\n\nìœ„ í˜•ì‹ì— ë§ì¶° ì¶”ì²œí•´ ì£¼ì„¸ìš”.")
])

# Fallback í”„ë¡¬í”„íŠ¸
fallback_prompt = ChatPromptTemplate.from_template("""
ì§ˆë¬¸: {question}

ë‚´ë¶€ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.
ì¼ë°˜ì ì¸ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ë‹µë³€:""")

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# ì²´ì¸ ìƒì„±
qt_chain = qt_prompt | llm | StrOutputParser()
multi_query_chain = multi_query_prompt | llm | StrOutputParser()
relevance_chain = relevance_check_prompt | llm | StrOutputParser()
fallback_chain = fallback_prompt | llm | StrOutputParser()

# ========================================
# í—¬í¼ í•¨ìˆ˜
# ========================================

def choose_prompt(question: str):
    """ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¼ ì ì ˆí•œ í”„ë¡¬í”„íŠ¸ ì„ íƒ"""
    recommend_keywords = ["ì¶”ì²œ", "ë§ëŠ”", "ì‹ ì²­í•  ìˆ˜ ìˆëŠ”", "ì§€ì›í•´ì£¼ëŠ”", 
                         "ì‚¬ì—… ì•Œë ¤ì¤˜", "í˜œíƒ", "ì§€ì›ê¸ˆ", "ì§€ì›ì‚¬ì—…"]
    law_keywords = ["ì •ì˜", "ìê²©", "ìš”ê±´", "ì§€ì›ë²•", "ë²•ì—ì„œ", "ë²•ìƒ", "ì œë„", "ì‹œí–‰","ê·œì •"]
    
    if any(k in question for k in recommend_keywords):
        return recommend_prompt, "recommend_prompt"
    if any(k in question for k in law_keywords):
        return law_prompt, "law_prompt"
    return rag_prompt, "rag_prompt"

def format_docs_as_context(docs):
    """RAG í”„ë¡¬í”„íŠ¸ì— ë„£ê¸° ì¢‹ì€ ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ì¶œì²˜ ë©”íƒ€ë°ì´í„° í¬í•¨)"""
    if not docs:
        return ""
    parts = []
    for i, d in enumerate(docs, 1):
        if isinstance(d, Document):
            src = d.metadata.get("source", d.metadata.get("url", "unknown"))
            parts.append(f"[ë¬¸ì„œ {i}] (ì¶œì²˜: {src})\n{d.page_content}")
    return "\n\n---\n\n".join(parts)

def search_documents_hybrid(queries: List[str], k_per_query=10) -> List[Tuple[Document, float]]:
    """í•˜ì´ë¸Œë¦¬ë“œ ë¦¬íŠ¸ë¦¬ë²„ë¡œ ë©€í‹°ì¿¼ë¦¬ ê²€ìƒ‰"""
    all_docs_with_scores = []
    seen_hashes = set()
    
    if hybrid_retriever is None:
        # Fallback: Denseë§Œ ì‚¬ìš©
        print("[ê²€ìƒ‰] í•˜ì´ë¸Œë¦¬ë“œ ë¦¬íŠ¸ë¦¬ë²„ ì—†ìŒ, Denseë§Œ ì‚¬ìš©")
        for q in queries:
            try:
                docs = vectorstore.similarity_search_with_score(q, k=k_per_query)
                for doc, distance in docs:
                    doc_hash = hashlib.md5(doc.page_content[:500].encode()).hexdigest()
                    if doc_hash in seen_hashes:
                        continue
                    seen_hashes.add(doc_hash)
                    similarity = max(0.0, 1.0 - (distance / 2.0))
                    all_docs_with_scores.append((doc, similarity))
            except Exception as e:
                print(f"âš  ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return all_docs_with_scores
    
    # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
    print(f"[í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰] {len(queries)}ê°œ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰ ì¤‘...")
    for q in queries:
        try:
            docs_with_scores = hybrid_retriever.invoke(q)   # Dense + BM25 í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì„ ìˆ˜í–‰
            
            for doc, score in docs_with_scores:
                doc_hash = hashlib.md5(doc.page_content[:500].encode()).hexdigest()
                if doc_hash in seen_hashes:
                    continue
                seen_hashes.add(doc_hash)
                all_docs_with_scores.append((doc, score))
        except Exception as e:
            print(f"âš  í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
    
    # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
    all_docs_with_scores.sort(key=lambda x: x[1], reverse=True)
    return all_docs_with_scores

def check_relevance(question, docs_with_scores):
    """LLMì—ê²Œ ìƒìœ„ Nê°œ ë¬¸ì„œë¡œ ê´€ë ¨ì„± ë¬¼ì–´ë³´ê¸° -> True/False ë°˜í™˜"""
    # docs_with_scoresëŠ” (doc, sim)
    top_docs = docs_with_scores[:5]
    if not top_docs:
        return False
    # Document ê°ì²´ì—ì„œ ì§ì ‘ ì ‘ê·¼
    docs_text = "\n\n---\n\n".join(
        f"[ë¬¸ì„œ {i+1}] (ì¶œì²˜: {getattr(doc.metadata,'get',lambda k, d=None: 'unknown')('source','unknown')})\n{getattr(doc,'page_content',str(doc))[:600]}"
        for i, (doc, _) in enumerate(top_docs)
    )
    try:
        res = relevance_chain.invoke({"question": question, "documents": docs_text})
        return "ê´€ë ¨ìˆìŒ" in res
    except Exception as e:
        print(f"âš  ê´€ë ¨ì„± ê²€ì¦ ì˜¤ë¥˜: {e}")
        return False

def web_search(query: str, k=3):
    """Tavily API ê¸°ë°˜ ì›¹ê²€ìƒ‰
    ì™¸ë¶€ ì†ŒìŠ¤ë§Œ Documentë¡œ ë³€í™˜ í•„ìš”"""
    try:
        retriever = TavilySearchAPIRetriever(k=k)
        results = retriever.invoke(query) 

        # Tavily ê²°ê³¼ë¥¼ Documentë¡œ ì •ê·œí™”
        docs = []
        for r in results:
            if isinstance(r, Document):
                docs.append(r)
            elif isinstance(r, dict):
                content = r.get("content") or r.get("snippet") or r.get("title") or str(r)
                docs.append(Document(
                    page_content=content,
                    metadata={"source": "web", "url": r.get("url", "unknown")}
                ))
            else:
                docs.append(Document(
                    page_content=str(r),
                    metadata={"source": "web"}
                ))
        return docs
        
    except Exception as e:
        raise RuntimeError(f"Tavily ì›¹ê²€ìƒ‰ ì˜¤ë¥˜: {e}")

# -----------------------
# RAG ì‘ë‹µ ìƒì„± í•¨ìˆ˜
# -----------------------
def rag_answer_from_docs(question: str, documents):
    """ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸(Document ë˜ëŠ” (doc,score) í˜¼í•©)ì„ ë°›ì•„ RAG ì‘ë‹µ ìƒì„±
    ì´ë¯¸ Document í˜•ì‹ì´ë¯€ë¡œ tupleë§Œ í’€ì–´ì£¼ë©´ ë¨"""
    # tuple í˜•ì‹ì´ë©´ Documentë§Œ ì¶”ì¶œ
    docs = []
    for item in documents:
        if isinstance(item, tuple):
            docs.append(item[0])  # (doc, score) â†’ doc
        else:
            docs.append(item)     # ì´ë¯¸ Document
    
    context = format_docs_as_context(docs)
    if not context.strip():
        print("âš  context ë¹„ì–´ìˆìŒ -> fallback LLMë¡œ ì´ë™")
        return fallback_chain.invoke({"question": question})

    prompt, pname = choose_prompt(question)
    print(f"[í”„ë¡¬í”„íŠ¸ ì‚¬ìš©] {pname} (ë¬¸ì„œ ê¸°ë°˜)")
    
    try:
        answer = (prompt | llm | StrOutputParser()).invoke({
            "context": context,
            "question": question
        })
        return answer
    except Exception as e:
        print(f"âš  RAG ìƒì„± ì˜¤ë¥˜: {e}")
        return fallback_chain.invoke({"question": question})
    
# ========================================
# ë©”ì¸ RAG í•¨ìˆ˜
# ========================================
# ë©€í‹°ë ˆë²¨ ìºì‹œ ì´ˆê¸°í™”
if cache_db_ok:
    multi_level_cache = MultiLevelCache(cache_vectorstore=cache_vectorstore)
    print("âœ… ë©€í‹°ë ˆë²¨ ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ\n")
else:
    multi_level_cache = None
    print("âš  ìºì‹œ DB ì—†ìŒ - ìºì‹œ ë¹„í™œì„±í™”\n")

def multi_query_rag_with_cache(question: str, top_k=10):
    """
    ì „ì²´ íë¦„:
      1) ì¿¼ë¦¬ íŠ¸ëœìŠ¤í¼ (QT)
      2) ë©€í‹°ì¿¼ë¦¬ ìƒì„± (MQ)
      3) ë²¡í„°ê²€ìƒ‰ (ë©€í‹°ì¿¼ë¦¬)
      4) ìœ ì‚¬ë„ í•„í„°ë§
        - ë¬¸ì„œ ìˆìŒ -> LLM ê´€ë ¨ì„± ê²€ì¦
        - ë¬¸ì„œ ì—†ìŒ / ê´€ë ¨ ì—†ìŒ -> ì›¹ê²€ìƒ‰ ë˜ëŠ” Fallback (ë‹¨ì¼ ë¶„ê¸°)
    """

  # ì‹¤ì œ RAG ë¡œì§ (ìºì‹œ ë¯¸ìŠ¤ ì‹œ í˜¸ì¶œë¨)
    def execute_rag(question):
        # 1) Query Transform
        try:
            qt_query = qt_chain.invoke({"question": question})
            print(f"[1ë‹¨ê³„] QT ì™„ë£Œ: {qt_query}")
        except Exception as e:
            print(f"âš  QT ì‹¤íŒ¨: {e}")
            qt_query = question

        # 2) Multi Query
        try:
            mq_text = multi_query_chain.invoke({"question": qt_query})
            queries = [line.strip() for line in mq_text.splitlines() if line.strip()]
            print(f"[2ë‹¨ê³„] ë©€í‹°ì¿¼ë¦¬ ìƒì„±: {len(queries)}ê°œ")
            for i, q in enumerate(queries, 1):
                print(f"  {i}. {q}")
        except Exception as e:
            print(f"âš  ë©€í‹°ì¿¼ë¦¬ ì‹¤íŒ¨: {e}")
            queries = [qt_query]

        # 3) í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
        print(f"[3ë‹¨ê³„] í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œì‘...")
        all_docs_with_scores = search_documents_hybrid(queries, k_per_query=10)
        print(f"[3ë‹¨ê³„] ê²€ìƒ‰ ì™„ë£Œ: {len(all_docs_with_scores)}ê°œ ë¬¸ì„œ")

        # 4) ìƒìœ„ ë¬¸ì„œ í•„í„°ë§
        top_docs = all_docs_with_scores[:top_k]

        if top_docs:
            print(f"[4ë‹¨ê³„] ìƒìœ„ {len(top_docs)}ê°œ ë¬¸ì„œ í™•ë³´")
            # ê´€ë ¨ì„± ê²€ì¦
            is_relevant = check_relevance(question, top_docs)
            
            if is_relevant:
                print("âœ… [5ë‹¨ê³„] ë‚´ë¶€ ë¬¸ì„œê°€ ê´€ë ¨ìˆìŒ â†’ ë‚´ë¶€ RAG ì‹¤í–‰")
                return rag_answer_from_docs(question, top_docs)
            else:
                print("âš  [5ë‹¨ê³„] ë‚´ë¶€ ë¬¸ì„œ ê´€ë ¨ì—†ìŒ â†’ ì›¹ê²€ìƒ‰ ì‹œë„")
        else:
            print("âš  [4ë‹¨ê³„] ë‚´ë¶€ ë¬¸ì„œ ì—†ìŒ â†’ ì›¹ê²€ìƒ‰ ì‹œë„")

        # 5) ì›¹ ê²€ìƒ‰
        try:
            print("[6ë‹¨ê³„] ì›¹ ê²€ìƒ‰ ì¤‘...")
            web_docs = web_search(question)
            if web_docs:
                print(f"âœ… [6ë‹¨ê³„] ì›¹ ë¬¸ì„œ {len(web_docs)}ê°œ í™•ë³´ â†’ ì›¹ RAG ì‹¤í–‰")
                return rag_answer_from_docs(question, web_docs)
            else:
                print("âš  [6ë‹¨ê³„] ì›¹ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ â†’ Fallback")
                return fallback_chain.invoke({"question": question})
        except Exception as e:
            print(f"âš  ì›¹ê²€ìƒ‰ ì‹¤íŒ¨: {e} â†’ Fallback")
            return fallback_chain.invoke({"question": question})

    # ìºì‹œ ì‚¬ìš©
    if multi_level_cache:
        return multi_level_cache.get(question, execute_rag)
    else:
        print("âš  ìºì‹œ ì—†ìŒ â†’ ì§ì ‘ RAG ì‹¤í–‰")
        return execute_rag(question)
# ========================================
# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
# ========================================

if __name__ == "__main__":
    test_cases = [
        "ì„œìš¸ì—ì„œ AI ì±—ë´‡ ì°½ì—…ì„ í•˜ë ¤ê³  í•˜ëŠ”ë° ë°›ì„ ìˆ˜ ìˆëŠ” ì§€ì›ì‚¬ì—… ìˆë‚˜ìš”?",
        "ì„œìš¸ì—ì„œ AI ì±—ë´‡ ì°½ì—…ì„ í•˜ë ¤ê³  í•˜ëŠ”ë° ë°›ì„ ìˆ˜ ìˆëŠ” ì§€ì›ì‚¬ì—… ìˆë‚˜ìš”?",
        #"í—¤ì–´ì•…ì„¸ì‚¬ë¦¬ ì‚¬ì—…í–ˆëŠ”ë° íŒë§¤ê°€ ë§¤ìš° ì €ì¡°í•´. ì–´ë–»ê²Œ í•˜ë©´ ë ê¹Œ?",
        "ì„œìš¸ì— ìˆëŠ” ë™ë¬¼ë³‘ì› ì•Œë ¤ì£¼ì„¸ìš”",
    ]
    
    for i, q in enumerate(test_cases, 1):
        print("\n" + "-"*40)
        print(f"í…ŒìŠ¤íŠ¸ {i}/{len(test_cases)}")
        print("-"*40)
        print(f"ì§ˆë¬¸: {q}")
        
        try:
            ans = multi_query_rag_with_cache(q)
            
            print("\n" + "-"*40)
            print("ìµœì¢… ë‹µë³€")
            print("-"*40)
            print(ans)
            print("\n")
        except Exception as e:
            print(f" ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
    
    if multi_level_cache:
        multi_level_cache.stats()
    
    print("\n í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
