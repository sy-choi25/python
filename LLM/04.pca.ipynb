{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef045eb1",
   "metadata": {},
   "source": [
    "- TF-IDF : 텍스트 벡터화\n",
    "- PCA : 차원축소\n",
    "- LSA : 잠재 의미 분석\n",
    "- t-SNE : 2D 시각화\n",
    "- 로지스틱회귀\n",
    "- 토큰화&전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e558b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSA\n",
    "# TF - IDF 행렬에 대해서 SVD\n",
    "# 단어와 문서 간의 숨겨진 의미 관계를 찾음\n",
    "# PCA 차이:\n",
    "    # PCA : 데이터 자체 분산 최대화\n",
    "    # LSA : 문서-단어형태의 의미구조 파악\n",
    "# 은행\n",
    "    # '돈' '계좌'  주변에 등장\n",
    "    # '나무' '냄새' '먹는다' 주변에 등장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40402dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE : 고차원 데이터를 2D,3D로 변환 - 시각화 전용(분석에는 부적합), 계산이 오래걸림\n",
    "# PCA VS t-SNE\n",
    "# PCA : 속도가 빠름, 전역 구조 보존\n",
    "# t-SNE : 느림, 국소(지역)군집 명확"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6af2b8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 데이터셋\n",
    "from sklearn.datasets import load_files\n",
    "train_path = r'C:\\python_src\\LLM\\20newsbydate\\20news-bydate-train'\n",
    "test_path = r'C:\\python_src\\LLM\\20newsbydate\\20news-bydate-test'\n",
    "newsgroups_train = load_files(train_path,encoding='latin1')\n",
    "newsgroups_test = load_files(test_path,encoding='latin1')\n",
    "categories =  ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # 헤더 제거\n",
    "    text = re.sub(r'^From:.*\\n', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'^Subject:.*\\n', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # 풋터 제거\n",
    "    text = re.sub(r'\\n--\\n.*$', '', text, flags=re.DOTALL)\n",
    "\n",
    "    # 인용문 제거\n",
    "    text = re.sub(r'(^|\\n)[>|:].*', '', text)\n",
    "\n",
    "    return text\n",
    "# 카테고리 제거\n",
    "def filter_categories(dataset, categories):\n",
    "    target_names = dataset.target_names\n",
    "    selected_idx = [ target_names.index(c) for c in categories  ]\n",
    "    #필터링\n",
    "    data_filtered, target_filtered = [], []\n",
    "    for text,label in zip(dataset.data, dataset.target):\n",
    "        if label in selected_idx:\n",
    "            new_label = selected_idx.index(label)  # 라벨 재 정렬\n",
    "            data_filtered.append(text) ; target_filtered.append( new_label  )\n",
    "    return data_filtered,target_filtered,categories\n",
    "train_data, train_target, target_names = filter_categories(newsgroups_train,categories)\n",
    "test_data, test_target, _ = filter_categories(newsgroups_test,categories)\n",
    "\n",
    "x_train = [ clean_text(t) for t in train_data]\n",
    "x_test = [ clean_text(t) for t in test_data]\n",
    "y_train = train_target\n",
    "y_test = test_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beff29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 전처리 (토큰화(3글자이상) + 불용어제어(Stop words) + 어간추출(stemming)) --> 영어\n",
    "# 파이프라인\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from TTfidfVectorizer\n",
    "RegexpTokenizer(r\"[\\w']{3,}\")  # 3글자 이상 단어 추출\n",
    "english_stopws = set(stopwords.words('english'))\n",
    "# 커스텀 토크나이저\n",
    "def tokenizer(text):\n",
    "    tokens = regtok.tokenize(text)\n",
    "    words = [word for word in tokens if word not in english_stopws]\n",
    "    features = list(map(lambda x:PorterStemmer().stem(x), words))\n",
    "    return features\n",
    "#TF-IDF 벡터화\n",
    "tfidf = TfidfVectorizer( tokenizer=tokenizer, max_df=0.5, min_df=2)\n",
    "x_train_tfidf = tfidf.fit_transform(x_train)\n",
    "x_test_tfidf = tfidf.transform(x_test)\n",
    "# 분류모델\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_clf = LogisticRegression(max_iter=200,\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60acb51c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
