{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bccf3712",
   "metadata": {},
   "source": [
    "Seq2Seq(Sequence-to-Sequence) 문제 정의와 해결 전략 이해  \n",
    "기본 순환신경망(RNN) 한계와 LSTM 개선 아이디어 파악  \n",
    "Encoder-Decoder 구조의 정보 흐름(컨텍스트 벡터/숨겨진 상태) 이해  \n",
    "Teacher Forcing 기법의 학습 안정화 역할 및 Trade-off 이해  \n",
    "Softmax와 CrossEntropy 손실의 수식 및 직관적 해석  \n",
    "Inference 단계(그리디 vs 빔 서치)의 실전 활용 전략  \n",
    "전체 파이프라인을 하나의 흐름으로 연결하여 종합 정리  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8e618a",
   "metadata": {},
   "source": [
    "<span style=\"font-size:12px;\">\n",
    "\n",
    "Seq2Seq(Sequence-to-Sequence)\n",
    "- 하나의 시퀀스(입력 문장) 를 다른 시퀀스(출력 문장) 로 바꾸는 모델\n",
    "- 출현 배경\n",
    "    - 입력과 출력의 길이가 다를 수 있는 문제(번역, 요약, 챗봇)  \n",
    "    - 고정크기의 피처벡터는 순서정보와 문맥관계를 충분히 반영 못함  \n",
    "    - 단순 분류모델은 시퀀스 간 종속적 생성(토큰별 점직적 예측)에 부적합  \n",
    "- 예시\n",
    "    - 영어를 한국어로 번역할 경우\n",
    "        - 입력 시퀀스 : \"I love you\"\n",
    "        - 출력 시퀀스 : \"나는 너를 사랑해\"  \n",
    "        - 실제 사용 예시 : 번역, 대화시스템, 문서요약, 코드자동생성  \n",
    "<br>\n",
    "- 동작 원리\n",
    "    - `인코더 (Encoder)` : 입력 문장을 한 단어씩 받아서 전체 의미를 하나의 벡터로 요약 -> 마지막/전체 Hidden state 집약\n",
    "        - 입력: I → love → you\n",
    "        - 출력: 문장 의미 벡터 (context vector)\n",
    "    - `context(요약표현)` : 인코더의 정보를 압축\n",
    "    - `디코더 (Decoder)`  : 인코더에서 전달받은 context vector를 이용해서 출력 문장을 한 단어씩 생성\n",
    "        - 입력: context vector\n",
    "        - 출력: 나는 → 너를 → 사랑해\n",
    "    - `종료` : 특별한 EOS 토큰이 나올때까지\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f40590",
   "metadata": {},
   "outputs": [],
   "source": [
    "입력 문장: I love you\n",
    "──────────────────────────────\n",
    "Encoder RNN:\n",
    "   I     →     love     →     you\n",
    "   ↓             ↓              ↓\n",
    " [h1] → [h2] → [h3] = context vector\n",
    "\n",
    "──────────────────────────────\n",
    "Decoder RNN:\n",
    "   context vector → \"나는\" → \"너를\" → \"사랑해\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1370634d",
   "metadata": {},
   "source": [
    "<span style=\"font-size:12px;\">\n",
    "\n",
    "인코더-디코더 구조\n",
    "1. 개념 정의\n",
    "Encoder는 입력 시퀀스를 내부 상태로 변환하는 모듈, Decoder는 그 상태(컨텍스트)를 참조하며 출력 시퀀스를 생성하는 모듈\n",
    "\n",
    "2. 왜 필요한가\n",
    "\n",
    "- 기능 분리: 이해(Encoding)와 생성(Decoding)을 모듈화하여 확장성 확보.\n",
    "- 다양한 입력/출력 도메인(텍스트→텍스트, 텍스트→태그 등)에 쉽게 적용.\n",
    "- 나중에 Attention 같은 추가 요소 삽입이 용이 (기본 Encoder-Decoder가 뼈대).\n",
    "\n",
    "3. 동작 원리\n",
    "\n",
    "- Encoder 단계:\n",
    "    - 입력 토큰 x₁, x₂, …, x_T\n",
    "    - 순환구조(RNN/LSTM/GRU)가 $h_t = f(x_t, h_{t-1})$ 계산\n",
    "    - 최종 $h_T$ 혹은 전체 ${h₁…h_T}$ 를 요약(기본 Seq2Seq는 $h_T$ 단일)\n",
    "\n",
    "- Decoder 단계:\n",
    "    - 초기 Hidden State: $h_T$ (Encoder 마지막 상태)\n",
    "    - 반복: $y_t = Softmax(W·h_t^dec)$\n",
    "    - 다음 입력: (Teacher Forcing 시 GT 토큰, 아니면 이전 예측)\n",
    "\n",
    "4. 실제 사용 예시\n",
    "\n",
    "| 작업 | Encoder 입력 | Decoder 출력 |\n",
    "|------|---------------|---------------|\n",
    "| 번역 | 영어 문장 토큰 | 한국어 문장 토큰 |\n",
    "| 질의응답 | 질문 토큰 | 답변 토큰 |\n",
    "| 요약 | 긴 문서 토큰 | 요약 문장 토큰 |\n",
    "| 형태소 분석 | 음절 시퀀스 | 품사 태그 시퀀스 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
