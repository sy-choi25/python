{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27732017",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e479d3ba",
   "metadata": {},
   "source": [
    "#### Attetion Mechanism\n",
    "\n",
    "- \"고양이가 잔다\" -> \"the cat sleeps\"\n",
    "\n",
    "- 입력 : \"고양이가 잔다\"\n",
    "\n",
    "    - [전처리] -> \\<start\\> 고양이가 잔다 \\<end\\>  \n",
    "    - [ENCODER] -> 각 단어마다 hidden state 생성  \n",
    "    - [ATTENTION] -> 어디에 집중할지 계산  \n",
    "    - [DECODER] -> 한 단어씩 생성 \n",
    "\n",
    "- 출력 : the cat sleeps  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544a5a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본\n",
    "입력 = '고양이가 자요'\n",
    "# 1단계 전처리\n",
    "전처리_후 = f'<start> {input} <end>'\n",
    "# 2단계 토큰화 (단어 -> 숫자)\n",
    "단어_사전 = {\n",
    "    '<pad>' : 0,        # 패딩\n",
    "    '<start>' : 1,\n",
    "    '<end>' : 2,\n",
    "    '고양이가' : 3,\n",
    "    '자요' : 4\n",
    "}\n",
    "# 최대길이 5개로 맞추자\n",
    "정수_시퀀스 = [1,3,4,2,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb6c2ec",
   "metadata": {},
   "source": [
    "<span style=\"color: Gold\"> 1. EMBEDDING </span> (숫자 -> 백터: 숫자를 벡터로 바꾼다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450989d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩레이어 === 256 차원\n",
    "정수_시퀀스 = [1,3,4,2,0]\n",
    "# Embedding...\n",
    "임베딩_결과 = [\n",
    "    [0.2, - 0.5, ..., 0.1],     # 정수_시퀀스 중 1번 숫자에 대한 것 -> (<start>)를 벡터화 한 것 (256차원)\n",
    "    [0.2, - 0.5, ..., 0.3],     # 정수_시퀀스 중 3번 숫자에 대한 것 -> (고양이가)를 벡터화 한 것 (256차원)\n",
    "    [0.2, - 0.5, ..., 0.4],     # 정수_시퀀스 중 4번 숫자에 대한 것 -> (자요)를 벡터화 한 것 (256차원)\n",
    "    [0.2, - 0.5, ..., 0.2],     # 정수_시퀀스 중 2번 숫자에 대한 것 -> (<end>)를 벡터화 한 것 (256차원)\n",
    "    [0.0, 0.0, ..., 0.0],     # 정수_시퀀스 중 0번 숫자에 대한 것 -> (<pad>)를 벡터화 한 것 (256차원)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eca313e",
   "metadata": {},
   "source": [
    "<span style=\"color: Gold\"> 2. ENCODER </span> (벡터->Hidden States : 벡터를 Hidden States로 만든다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db8a268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Encoder\n",
    "# 각 타임스텝마다 hidden state 생성\n",
    "\n",
    "# 초기상태\n",
    "h0 = [0,0,0,...,0]  # 512차원 영벡터\n",
    "c0 = [0,0,0,...,0]  # 512차원 영벡터\n",
    "\n",
    "# ===========  타임스텝 1 : <start> 처리  ============\n",
    "입력_t1 = [0.2, - 0.5, ..., 0.1]    # <start> 의 임베딩\n",
    "h1, c1 = LSTM(입력_t1, h0,c0)       # h와 c는 이전의 것을 가져옴\n",
    "# h1 = [0.1 .....] 512 차원\n",
    "\n",
    "# ===========  타임스텝 2 : (고양이가) 처리  ============\n",
    "입력_t2 = [0.2, - 0.5, ..., 0.3]\n",
    "h2, c2 = LSTM(입력_t2, h1,c1)\n",
    "\n",
    "\n",
    "# ===========  타임스텝 3 : (자요) 처리  ============\n",
    "입력_t3 = [0.2, - 0.5, ..., 0.4]\n",
    "h3, c3 = LSTM(입력_t3, h2,c2)\n",
    "\n",
    "\n",
    "# ===========  타임스텝 4 : (<end>) 처리  ============\n",
    "입력_t4 = [0.2, - 0.5, ..., 0.2]\n",
    "h4, c4 = LSTM(입력_t4, h3,c3)\n",
    "\n",
    "\n",
    "# ===========  타임스텝 5 : (<pad>)은 무시  ============\n",
    "입력_t5 = [0.0, 0.0, ..., 0.0]\n",
    "h5, c5 = LSTM(입력_t5, h4,c4)\n",
    "# h5는 사용하지 않음(마스킹)\n",
    "\n",
    "# ===========  인코더의 출력  ============\n",
    "encoder_output = [h1, h2, h3, h4, h5] # 모든 hidden states\n",
    "# 형태[1,5,512] ->  [배치, 시퀀스, units]\n",
    "encoder_final_h = h4  # h5는 패딩 0이라 무시 -> hidden state(디코더 초기화용)\n",
    "encoder_final_c = c4  # 패딩 무시 -> 마지막 cell state\n",
    "\n",
    "# ============== 정리 =====================\n",
    "<start> ----> h1\n",
    "고양이가 ---> h2     \"고양이\" 정보를 얍축\n",
    "자요    ----> h3    \"자요\" 정보를 압축\n",
    "<end>   ---> h4     전체 문장 요약\n",
    "<pad>  ----> h5     무시됨\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91492f5a",
   "metadata": {},
   "source": [
    "<span style=\"color: lightblue;\"> 인코더 요약 </span>  \n",
    "LSTM은 hidden state과 cell state를 통해 문맥 정보를 누적해 학습한다  \n",
    "첫번쨰 문장에서 인코딩된 마지막 \\<end>를 다음 문장에서 초기값으로 설정하며 점차 문맥 정보를 누적한다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf692acc",
   "metadata": {},
   "source": [
    "<span style=\"color: Gold\"> 3. DECODER의 시작 (첫 단어 생성) </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e8efd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= 디코더 초기 상태 ===============\n",
    "decoder_h = h4      # 인코더의 마지막 hidden state\n",
    "decoder_c = c4\n",
    "\n",
    "# ============= 첫번째 입력 : <start> 토큰 ===============\n",
    "decoder_input = [1]  # <start> 의 인덱스\n",
    "decoder_input_imbedding = [0.3, 0.1,...,0.2] # 256 차원 -->  <start>의 임베딩 값"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5752b03",
   "metadata": {},
   "source": [
    "<span style=\"color: Gold\"> 4. Attention 계산 (첫 번째 생성) </span> \n",
    "\n",
    "- the 를 생성하기 위해서 어디를 봐야 하나? 가 관심사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2590ab85",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = decoder_h  # 512 차원\n",
    "# 의미 : 지금 뭘 번역하려고 하는가?\n",
    "\n",
    "# key 준비\n",
    "keys = [h1, h2, h3, h4]  # 패딩을 제외한 인코더 출력\n",
    "# 의미 : 각 입력 단어의 정보\n",
    "\n",
    "# value 준비(key와 동일)\n",
    "values = [h1, h2, h3, h4]  # 패딩을 제외한 인코더 출력\n",
    "\n",
    "# 1단계 score 계산\n",
    "# W1, W2, V 는 학습된 가중치 행렬\n",
    "# h1 <start>에 대한 score 계산\n",
    "# h2 (고양이가) 에 대한 score 계산\n",
    "# h3 (자요)에 대한 score 계산\n",
    "# h4 <end>에 대한 score 계산\n",
    "scores = [0.3, 0.8, 0.2, 0.1]\n",
    "\n",
    "# 2단계 Attention Weights 계산\n",
    "# softtmax 확률 분포    exp(scores)\n",
    "exp_scores = [exp(0.3, exp(0.8)),...] = [1.35, 2.23, 1.22, 1.11]\n",
    "# sum\n",
    "sum_exp = 1.35+2.23+1.22+1.11 = 5.91\n",
    "# 정규화\n",
    "attention_weight = [\n",
    "    1.35/5.91= 0.23,        # <start>\n",
    "    2.23/5.91= 0.38,        # 고양이가      -> 가중치가 가장 높음\n",
    "    1.22/5.91= 0.21,        # 자요\n",
    "    1.11/5.91= 0.18,        # <end>\n",
    "\n",
    "]\n",
    "\n",
    "# 3단계 context vector 계산\n",
    "# 가중합으로 인코더 정보 통합\n",
    "context_vector = (\n",
    "    0.23 x h1 +     <start>의정보 23%\n",
    "    0.38 x h2 +     고양이가 의정보 38%  --> 가장 많이 참조\n",
    "    0.21 x h3 +     자요 의정보 21%\n",
    "    0.18 x h4 +     <end>의정보 18%\n",
    ")\n",
    "\n",
    "# 최종 결과 512차원\n",
    "입력단어 별로 score을 구하고 attetion weight를 구해서 기여도를 본다\n",
    "# 입력단어     score       Attention Weight    기여도\n",
    "# ----         ----        --------            고양이가에 대한 기여도가 가장 높게 나오고 ---> 집중"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b16caf",
   "metadata": {},
   "source": [
    "<span style=\"color: Gold\"> 5. DECODER 실행 (첫 단어 생성) </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c30ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# context vector 와 입력을 결합해서 단어 생성\n",
    "\n",
    "# ---- 1단계 입력과 context 결합\n",
    "decoder_input_emb = [0.3, ..., 0.2]  # <start>의 임베딩 (256차원)\n",
    "context_vector = [0.32, ....., 0.22]\n",
    "\n",
    "# concatenate 이어 붙이기\n",
    "combined_input = [\n",
    "    0.3, 0.2, ..., 0.2,     # 임베딩 256\n",
    "    0.3, 0.2, ..., 0.2,     # context 512\n",
    "]\n",
    "\n",
    "# 형태 : 256 + 512  ->  768차원\n",
    "\n",
    "# --- 2단계 LSTM 처리\n",
    "decoder_outm, new_h, new_c = LSTM(\n",
    "    combined_input,\n",
    "    decoder_h,          # 이전 hidden state\n",
    "    decoder_c           # 이전 cell state\n",
    ")\n",
    "\n",
    "# --- 3단계 : 단어 확률 분포 생성\n",
    "# Dense Layer 사용 -> 512 + vocab_siz\n",
    "logits = Dense(docoder_output)\n",
    "# logits = [-0.5, 2.3, ...]     50개의 단어 점수\n",
    "\n",
    "# softmax로 확률 변환\n",
    "probailities = softmax(logits)\n",
    "# probailities = [0.0, 0.42, 0.28, ...] 인덱스 1(the)가 가장 높게\n",
    "\n",
    "# --- 4단계 가장 높은 확률의 단어 선택\n",
    "predict_di = argmax(probailities)   # 1\n",
    "\n",
    "# 단어 사전에서 lookup\n",
    "영어_단어_사전 = {\n",
    "    0 : '<pad>'         \n",
    "    1 : 'the'  # -> 선택됨\n",
    "    ...\n",
    "}\n",
    "predict_new = 'the'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2af916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= 디코더 초기 상태 ===============\n",
    "decoder_h = new_h  \n",
    "decoder_c = new_c\n",
    "\n",
    "# ============= 두번째 입력 : <end> 토큰 ===============\n",
    "decoder_input = [2]  # <end> 의 인덱스\n",
    "decoder_input_imbedding = [0.3, 0.1,...,0.2] # 256 차원 -->  고양이가의 임베딩 값"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
