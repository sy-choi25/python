{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b9abf2a",
   "metadata": {},
   "source": [
    "<span style=\"font-size:13px;\">\n",
    "\n",
    "##### ▶ 텍스트 벡터화 : 텍스트 데이터를 메신러닝 모델이 이해할 수 있는 숫자 형태의 벡터로 변환하는 기법\n",
    "-  BOW (Bag of Words) CountVectorizer\n",
    "    - 문서를 고정된 길이의 벡터로 변환\n",
    "    - 문서 - 단어행렬\n",
    "    - 장점 : 간단하고 빠름,\n",
    "    - 단점 : 단어순서 손실, 희소성, 의미적 유사성 무시\n",
    "\n",
    "-  TF-IDF TfidVetorizer\n",
    "    - 모든 문서에서 자주 등장하는 단어의 영향을 줄이고, 문서의 특이 단어를 강조\n",
    "\n",
    "-  N-gram \n",
    "    - 단어의 순서를 보존하기 위해 N개의 연속된 단어 시퀀스를 하나의 단위로 보는 것\n",
    "    - 단점 차원폭발에 주의 (정규화/ 차원 축소 고려)\n",
    "\n",
    "##### ▶ 머신러닝 분류 모델 : 벡터화된 텍스트 데이터를 기반으로 문서를 특정 카테고리로 분류하는 알고리즘\n",
    "-  multinomal Naive Bayes (다항분포 나이브 베이즈)\n",
    "    - 확률 모델\n",
    "\n",
    "-  LogisticRegression \n",
    "    - 회귀를 기반으로하지만 시그모이드 함수를 사용하여 클래스를 분류하는데 널리 사용되는 모델\n",
    "    - 다중클래스 회귀기반 분류\n",
    "\n",
    "- RidgeClassifer\n",
    "    - L2규제를 적용한 회귀 기반 분류 모델로 모델의 과적합을 방지\n",
    "\n",
    "\n",
    "##### ▶ 한국어 형태소 분석기 : 한국어 텍스트를 처리하기 위한 라이브러리\n",
    "- Kolnpy Okt\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c7a513",
   "metadata": {},
   "source": [
    "### <span style=\"color: gold;\">1. 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18f34322",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# scikit-learn\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "#분류모델\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression,RidgeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e5a53bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "categories =  ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "# # data load\n",
    "# newsgroups_train =  fetch_20newsgroups(subset='train'\n",
    "#                     ,remove = ('headers','footers','quotes')\n",
    "#                     ,categories=categories\n",
    "#                    )\n",
    "# newsgroups_test =  fetch_20newsgroups(subset='test'\n",
    "#                     ,remove = ('headers','footers','quotes')\n",
    "#                     ,categories=categories\n",
    "#                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5460a626",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "train_path = r'C:\\python_src\\LLM\\20newsbydate\\20news-bydate-train'\n",
    "test_path = r'C:\\python_src\\LLM\\20newsbydate\\20news-bydate-test'\n",
    "newsgroups_train = load_files(train_path,encoding='latin1')\n",
    "newsgroups_test = load_files(test_path,encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bac4ee25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 카테고리 제거\n",
    "def filter_categories(dataset, categories):\n",
    "    target_names = dataset.target_names\n",
    "    selected_idx = [ target_names.index(c) for c in categories  ]\n",
    "    #필터링\n",
    "    data_filtered, target_filtered = [], []\n",
    "    for text,label in zip(dataset.data, dataset.target):\n",
    "        if label in selected_idx:\n",
    "            new_label = selected_idx.index(label)  # 라벨 재 정렬\n",
    "            data_filtered.append(text) ; target_filtered.append( new_label  )\n",
    "    return data_filtered,target_filtered,categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03433be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_target, target_names = filter_categories(newsgroups_train,categories)\n",
    "test_data, test_target, _ = filter_categories(newsgroups_test,categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ed8b2ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df82838f",
   "metadata": {},
   "source": [
    "---\n",
    "### <span style=\"color: gold;\"> 2. 텍스트 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d43cad3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 헤더 푸터 인용문 제거\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # 헤더 제거\n",
    "    text = re.sub(r'^From:.*\\n', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'^Subject:.*\\n', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # 풋터 제거\n",
    "    text = re.sub(r'\\n--\\n.*$', '', text, flags=re.DOTALL)\n",
    "\n",
    "    # 인용문 제거\n",
    "    text = re.sub(r'(^|\\n)[>|:].*', '', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51b2de4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 전처리\n",
    "train_data = [ clean_text(t) for t in train_data]\n",
    "test_data = [ clean_text(t) for t in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2f0a0aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(970, 970, 0, 0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(train_target), len(test_data), len(test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4f278a",
   "metadata": {},
   "source": [
    "<span style=\"font-size:13px;\">\n",
    "\n",
    "#### 멀티노멀 나이즈베이즈  \n",
    "    - 단어의 **출현 횟수(빈도)**를 세어서, 주어진 문서가 특정 카테고리(예: 스팸, 정상)에 속할 확률을 계산하는 방법\n",
    "    - 활용분야: 스팸필터링, 뉴스기사 카테고리, 감성분석  \n",
    "#### 1.  베이즈정리 : 확률 이론 -> 조건부 확률\n",
    "- 단어 A가 나왔을 때 이 문서가 스팸 B일 확률은 얼마인가  \n",
    "- $P(\\text{스팸} | \\text{단어들}) = \\frac{P(\\text{단어들} | \\text{스팸}) \\cdot P(\\text{스팸})}{P(\\text{단어들})}$\n",
    "\n",
    "#### 2. 나이브 Naive\n",
    "- 가정: 문서 안의 모든 단어는 서로 독립적이라고 가정\n",
    "- 현실: 스팸에 자주 나오는 단어들은 서로 독립적이지 않다\n",
    "- 실제: 이러한 가정은 계산량을 빠르게 하고 단순하지만 정확도가 어느정도 나온다\n",
    "\n",
    "#### 3. 멀티노멀 : 다항분포\n",
    "- 의미 : \n",
    "    - '다항 분포'를 의미하며, 단어의 출현 횟수를 가장 중요한 정보로 사용\n",
    "    -  어떤 단어가 단순히 '있다/없다'가 아니라 '몇 번 나왔는가'\n",
    "- 회수를 세는 멀티노미얼 방식이 NLP와 잘 맞는다\n",
    "- 모델은 단어의 빈도수 통계\n",
    "---\n",
    "- 예시 - 이러한 통계를 바탕으로 이 카테고리에서 특정 단어가 나올 확률 P('free'|스팸)을 모두 계산\n",
    "-   **스팸 메일 통계**\n",
    "    -   `free`: 150번\n",
    "    -   `money`: 100번\n",
    "    -   `viagra`: 50번\n",
    "    -   `report`: 5번\n",
    "-   **정상(Ham) 메일 통계**\n",
    "    -   `report`: 80번\n",
    "    -   `meeting`: 60번\n",
    "    -   `free`: 10번\n",
    "-   **새로운 메일 분류**  'free money metting'\n",
    "\n",
    "**1. 이 메일이 '스팸'일 확률 점수 계산**\n",
    "\n",
    "> **점수(스팸) = P(스팸) × P('free'|스팸) × P('money'|스팸) × P('meeting'|스팸)**  \n",
    "        -> 기본스팸확률 x 스팸일때 free가 나올 확률 x 스팸일때 money가 나올 확률 x 스팸일 때 meeting 나올 확률                   \n",
    "\n",
    "-   `P(스팸)`: 전체 메일 중 스팸 메일이 차지하는 기본 확률\n",
    "-   `P('free'|스팸)`: 스팸 메일에서 'free'가 나올 확률\n",
    "-   `P('money'|스팸)`: 스팸 메일에서 'money'가 나올 확률\n",
    "-   `P('meeting'|스팸)`: 스팸 메일에서 'meeting'이 나올 확률\n",
    "\n",
    "**2. 이 메일이 '정상'일 확률 점수 계산**\n",
    "\n",
    "> **점수(정상) = P(정상) × P('free'|정상) × P('money'|정상) × P('meeting'|정상)**  \n",
    "        -> 기본 정상확률 X 정상일때\n",
    "-   `P(정상)`: 전체 메일 중 정상 메일이 차지하는 기본 확률\n",
    "-   `P('free'|정상)`: 정상 메일에서 'free'가 나올 확률\n",
    "-   `P('money'|정상)`: 정상 메일에서 'money'가 나올 확률\n",
    "-   `P('meeting'|정상)`: 정상 메일에서 'meeting'이 나올 확률"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353d8d78",
   "metadata": {},
   "source": [
    "---\n",
    "### <span style=\"color: gold;\"> 3. 텍스트 벡터화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "50f55f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SAMSUNG\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# nltk tokenizer stemer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1e3aa193",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# min_df : 단어의 빈도가 최소 5개의 문서에 등장  - 노이즈 감소\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# max_df : 50% 너무흔한 단어는 제거\u001b[39;00m\n\u001b[32m      3\u001b[39m cv = CountVectorizer(max_features=\u001b[32m2000\u001b[39m,min_df=\u001b[32m5\u001b[39m, max_df=\u001b[32m0.5\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m x_train_cv = \u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m x_test_cv = cv.transform(test_data)\n\u001b[32m      6\u001b[39m x_train_cv.shape,  x_test_cv.shape\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SAMSUNG\\anaconda3\\envs\\deep\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SAMSUNG\\anaconda3\\envs\\deep\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1376\u001b[39m, in \u001b[36mCountVectorizer.fit_transform\u001b[39m\u001b[34m(self, raw_documents, y)\u001b[39m\n\u001b[32m   1368\u001b[39m             warnings.warn(\n\u001b[32m   1369\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mUpper case characters found in\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1370\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m vocabulary while \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlowercase\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1371\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m is True. These entries will not\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1372\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m be matched with any documents\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1373\u001b[39m             )\n\u001b[32m   1374\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1376\u001b[39m vocabulary, X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.binary:\n\u001b[32m   1379\u001b[39m     X.data.fill(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SAMSUNG\\anaconda3\\envs\\deep\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1263\u001b[39m, in \u001b[36mCountVectorizer._count_vocab\u001b[39m\u001b[34m(self, raw_documents, fixed_vocab)\u001b[39m\n\u001b[32m   1261\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[32m   1262\u001b[39m     feature_counter = {}\n\u001b[32m-> \u001b[39m\u001b[32m1263\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1264\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1265\u001b[39m             feature_idx = vocabulary[feature]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SAMSUNG\\anaconda3\\envs\\deep\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:104\u001b[39m, in \u001b[36m_analyze\u001b[39m\u001b[34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    103\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m         doc = \u001b[43mpreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    106\u001b[39m         doc = tokenizer(doc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SAMSUNG\\anaconda3\\envs\\deep\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:62\u001b[39m, in \u001b[36m_preprocess\u001b[39m\u001b[34m(doc, accent_function, lower)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[33;03mapply to a document.\u001b[39;00m\n\u001b[32m     45\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     59\u001b[39m \u001b[33;03m    preprocessed string\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     doc = \u001b[43mdoc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m()\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m accent_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     64\u001b[39m     doc = accent_function(doc)\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "\n",
    "# min_df : 단어의 빈도가 최소 5개의 문서에 등장  - 노이즈 감소\n",
    "# max_df : 50% 너무흔한 단어는 제거\n",
    "cv = CountVectorizer(max_features=2000,min_df=5, max_df=0.5)\n",
    "x_train_cv = cv.fit_transform(train_data)\n",
    "x_test_cv = cv.transform(test_data)\n",
    "x_train_cv.shape,  x_test_cv.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7399b791",
   "metadata": {},
   "source": [
    "---\n",
    "### <span style=\"color: gold;\"> 4. 모델 학습 및 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24a9dfd",
   "metadata": {},
   "source": [
    "<span style=\"color: pink\">1. MultinomialNB (다항분포 나이브 베이즈)</span>\n",
    "\n",
    "- BoW(CountVectorizer) + MultinomialNB    \n",
    "- TF-IDF + MultinomialNB    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4ea0a556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9508357915437562 0.861049519586105\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "       alt.atheism       0.80      0.79      0.80       319\n",
      "talk.religion.misc       0.74      0.73      0.73       251\n",
      "     comp.graphics       0.92      0.95      0.93       389\n",
      "         sci.space       0.93      0.91      0.92       394\n",
      "\n",
      "          accuracy                           0.86      1353\n",
      "         macro avg       0.85      0.85      0.85      1353\n",
      "      weighted avg       0.86      0.86      0.86      1353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BOW 기반 + 멀티노미얼 나이브베이즈 MNB\n",
    "# 텍스트 분류의 강력한 BASELINE 희소데이터에 강함\n",
    "# 모델선택\n",
    "nb = MultinomialNB()\n",
    "# 학습용데이터 벡터데이터\n",
    "nb.fit(x_train_cv,train_target)\n",
    "print(nb.score(x_train_cv, train_target)  , nb.score(x_test_cv,test_target))\n",
    "# 분류 리포트\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred_nb = nb.predict(x_test_cv)\n",
    "print( classification_report(test_target,y_pred_nb,target_names=categories)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ab040650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9513274336283186 0.8566149297856614\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF + 멜티노비얼 나이브베이즈MNB + 노지스틱레그레이션 LogiscticRegression\n",
    "# TF-IDF로 중요단어 강조, 선형모델과 자주사용 BOW 대비 흔한 단어 영향 감소\n",
    "tfidf = TfidfVectorizer(max_features=2000, min_df=5, max_df=0.5)\n",
    "x_train_tfid = tfidf.fit_transform(train_data)\n",
    "x_test_tfid = tfidf.transform(test_data)\n",
    "\n",
    "# NB + tf-idf \n",
    "nb_tfidf  = MultinomialNB()\n",
    "nb_tfidf.fit(x_train_tfid,train_target)\n",
    "print(nb_tfidf.score(x_train_tfid,train_target),  nb_tfidf.score(x_test_tfid, test_target) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327dce53",
   "metadata": {},
   "source": [
    "<span style=\"color: pink\">2. LogisticRegression (로지스틱 회귀)</span>\n",
    "-  TF-IDF + LogisticRegression (기본, L2 규제)\n",
    "-  TF-IDF + LogisticRegression (L1 규제)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "3a7ef6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9834050399508297 0.9434889434889435\n"
     ]
    }
   ],
   "source": [
    "# logistic regression + tf-idf\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train_tfidf, train_target, test_size=0.2,stratify=train_target, random_state=42)\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(x_train, y_train)\n",
    "print(lr.score(x_train,y_train), lr.score(x_test,y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8799c42",
   "metadata": {},
   "source": [
    "<span style=\"color: pink\">3. RidgeClassifier (릿지 분류기)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "de98609d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9759095378564405 0.8625277161862528\n"
     ]
    }
   ],
   "source": [
    "# 과적합 해결을 위한 규제\n",
    "rc = RidgeClassifier(alpha=5)\n",
    "rc.fit(x_train_tfidf, train_target)\n",
    "print(rc.score(x_train_tfidf,train_target), rc.score(x_test_tfidf,test_target)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a412f4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9373079287031346 0.914004914004914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SAMSUNG\\anaconda3\\envs\\deep\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# L1 규제L1 Logistic (Lasso와 유사)\n",
    "# 일부 계수를 0으로 만들어서 특성 선택을 수행 ... 주요 피처 선택 효과\n",
    "l1_lr = LogisticRegression(penalty= 'l1', max_iter=1000, solver='liblinear')\n",
    "l1_lr.fit(x_train, y_train)\n",
    "print(l1_lr.score(x_train,y_train), l1_lr.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95928d2c",
   "metadata": {},
   "source": [
    "<span style=\"color: pink\">3. 트리 기반 앙상블 모델</span>\n",
    "- DecisionTreeClassifier( 결정트리)\n",
    "- RandomForestClassifier (랜덤 포레스트)\n",
    "- GradientBoostingClassifier (그래디언트 부스팅)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "723baf91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5138291333743086 0.4864864864864865\n",
      "0.7572218807621389 0.7272727272727273\n",
      "0.9987707437000615 0.9287469287469288\n"
     ]
    }
   ],
   "source": [
    "# 트리모델 + TF-IDF\n",
    "tree = DecisionTreeClassifier(max_depth=3)\n",
    "fores = RandomForestClassifier(max_depth=3)\n",
    "gb = GradientBoostingClassifier(max_depth=3)\n",
    "\n",
    "tree.fit(x_train, y_train)\n",
    "fores.fit(x_train, y_train)\n",
    "gb.fit(x_train, y_train)\n",
    "\n",
    "print(tree.score(x_train,y_train), tree.score(x_test,y_test))\n",
    "print(fores.score(x_train,y_train), fores.score(x_test,y_test))\n",
    "print(gb.score(x_train,y_train), gb.score(x_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5f55a4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(train_data, train_target,stratify=train_target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "da878e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SAMSUNG\\anaconda3\\envs\\deep\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'csr_matrix' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[125]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m toks\n\u001b[32m     10\u001b[39m tfidf_custom = TfidfVectorizer(tokenizer=custom_tokenizer, max_features=\u001b[32m2000\u001b[39m,min_df=\u001b[32m5\u001b[39m, max_df=\u001b[32m0.5\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m x_train_tfidf_c = \u001b[43mtfidf_custom\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m x_test_tfidf_c = tfidf_custom.transform(x_test)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SAMSUNG\\anaconda3\\envs\\deep\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2104\u001b[39m, in \u001b[36mTfidfVectorizer.fit_transform\u001b[39m\u001b[34m(self, raw_documents, y)\u001b[39m\n\u001b[32m   2097\u001b[39m \u001b[38;5;28mself\u001b[39m._check_params()\n\u001b[32m   2098\u001b[39m \u001b[38;5;28mself\u001b[39m._tfidf = TfidfTransformer(\n\u001b[32m   2099\u001b[39m     norm=\u001b[38;5;28mself\u001b[39m.norm,\n\u001b[32m   2100\u001b[39m     use_idf=\u001b[38;5;28mself\u001b[39m.use_idf,\n\u001b[32m   2101\u001b[39m     smooth_idf=\u001b[38;5;28mself\u001b[39m.smooth_idf,\n\u001b[32m   2102\u001b[39m     sublinear_tf=\u001b[38;5;28mself\u001b[39m.sublinear_tf,\n\u001b[32m   2103\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2104\u001b[39m X = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2105\u001b[39m \u001b[38;5;28mself\u001b[39m._tfidf.fit(X)\n\u001b[32m   2106\u001b[39m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[32m   2107\u001b[39m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SAMSUNG\\anaconda3\\envs\\deep\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SAMSUNG\\anaconda3\\envs\\deep\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1376\u001b[39m, in \u001b[36mCountVectorizer.fit_transform\u001b[39m\u001b[34m(self, raw_documents, y)\u001b[39m\n\u001b[32m   1368\u001b[39m             warnings.warn(\n\u001b[32m   1369\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mUpper case characters found in\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1370\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m vocabulary while \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlowercase\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1371\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m is True. These entries will not\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1372\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m be matched with any documents\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1373\u001b[39m             )\n\u001b[32m   1374\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1376\u001b[39m vocabulary, X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.binary:\n\u001b[32m   1379\u001b[39m     X.data.fill(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SAMSUNG\\anaconda3\\envs\\deep\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1263\u001b[39m, in \u001b[36mCountVectorizer._count_vocab\u001b[39m\u001b[34m(self, raw_documents, fixed_vocab)\u001b[39m\n\u001b[32m   1261\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[32m   1262\u001b[39m     feature_counter = {}\n\u001b[32m-> \u001b[39m\u001b[32m1263\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1264\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1265\u001b[39m             feature_idx = vocabulary[feature]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SAMSUNG\\anaconda3\\envs\\deep\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:104\u001b[39m, in \u001b[36m_analyze\u001b[39m\u001b[34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    103\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m         doc = \u001b[43mpreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    106\u001b[39m         doc = tokenizer(doc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SAMSUNG\\anaconda3\\envs\\deep\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:62\u001b[39m, in \u001b[36m_preprocess\u001b[39m\u001b[34m(doc, accent_function, lower)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[33;03mapply to a document.\u001b[39;00m\n\u001b[32m     45\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     59\u001b[39m \u001b[33;03m    preprocessed string\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     doc = \u001b[43mdoc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m()\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m accent_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     64\u001b[39m     doc = accent_function(doc)\n",
      "\u001b[31mAttributeError\u001b[39m: 'csr_matrix' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "\n",
    "# 전처리 \n",
    "# RegexpTokenizer + stopwords + PorterStemmer\n",
    "english_stops = set(stopwords.words('english'))\n",
    "regtok = RegexpTokenizer(r\"[\\w']{3,}\")\n",
    "def custom_tokenizer(text):\n",
    "    toks = regtok.tokenize(text.lower())\n",
    "    toks = [t for t in toks if t not in english_stops]\n",
    "    toks = [PorterStemmer().stem(t) for t in toks]\n",
    "    return toks\n",
    "tfidf_custom = TfidfVectorizer(tokenizer=custom_tokenizer, max_features=2000,min_df=5, max_df=0.5)\n",
    "x_train_tfidf_c = tfidf_custom.fit_transform(x_train)\n",
    "x_test_tfidf_c = tfidf_custom.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f74b1d71",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [2034, 1627]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[124]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinear_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression\n\u001b[32m      2\u001b[39m lr_c = LogisticRegression(max_iter=\u001b[32m1000\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mlr_c\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train_tfidf_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m( lr_c.score(x_train_tfidf_c, y_train) , lr_c.score(x_test_tfidf_c, y_test))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SAMSUNG\\anaconda3\\envs\\deep\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SAMSUNG\\anaconda3\\envs\\deep\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247\u001b[39m, in \u001b[36mLogisticRegression.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m   1244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1245\u001b[39m     _dtype = [np.float64, np.float32]\n\u001b[32m-> \u001b[39m\u001b[32m1247\u001b[39m X, y = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1248\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1249\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1250\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1251\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1252\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1253\u001b[39m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mC\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1254\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43msolver\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mliblinear\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msag\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msaga\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1255\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1256\u001b[39m check_classification_targets(y)\n\u001b[32m   1257\u001b[39m \u001b[38;5;28mself\u001b[39m.classes_ = np.unique(y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SAMSUNG\\anaconda3\\envs\\deep\\Lib\\site-packages\\sklearn\\utils\\validation.py:2971\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2969\u001b[39m         y = check_array(y, input_name=\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m, **check_y_params)\n\u001b[32m   2970\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2971\u001b[39m         X, y = \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2972\u001b[39m     out = X, y\n\u001b[32m   2974\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SAMSUNG\\anaconda3\\envs\\deep\\Lib\\site-packages\\sklearn\\utils\\validation.py:1387\u001b[39m, in \u001b[36mcheck_X_y\u001b[39m\u001b[34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[39m\n\u001b[32m   1368\u001b[39m X = check_array(\n\u001b[32m   1369\u001b[39m     X,\n\u001b[32m   1370\u001b[39m     accept_sparse=accept_sparse,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1382\u001b[39m     input_name=\u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1383\u001b[39m )\n\u001b[32m   1385\u001b[39m y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n\u001b[32m-> \u001b[39m\u001b[32m1387\u001b[39m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1389\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SAMSUNG\\anaconda3\\envs\\deep\\Lib\\site-packages\\sklearn\\utils\\validation.py:473\u001b[39m, in \u001b[36mcheck_consistent_length\u001b[39m\u001b[34m(*arrays)\u001b[39m\n\u001b[32m    471\u001b[39m lengths = [_num_samples(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m arrays \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(lengths)) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    474\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    475\u001b[39m         % [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[32m    476\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Found input variables with inconsistent numbers of samples: [2034, 1627]"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_c = LogisticRegression(max_iter=1000)\n",
    "lr_c.fit(x_train_tfidf_c,y_train)\n",
    "print( lr_c.score(x_train_tfidf_c, y_train) , lr_c.score(x_test_tfidf_c, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7092d93c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [407, 1627]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[106]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m lr_c = LogisticRegression(max_iter=\u001b[32m1000\u001b[39m)\n\u001b[32m     11\u001b[39m lr_c.fit(x_train_12, y_train)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28mprint\u001b[39m(lr_c.score(x_train_12,y_train), \u001b[43mlr_c\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train_12\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SAMSUNG\\anaconda3\\envs\\deep\\Lib\\site-packages\\sklearn\\base.py:548\u001b[39m, in \u001b[36mClassifierMixin.score\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    523\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    524\u001b[39m \u001b[33;03mReturn :ref:`accuracy <accuracy_score>` on provided data and labels.\u001b[39;00m\n\u001b[32m    525\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    544\u001b[39m \u001b[33;03m    Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\u001b[39;00m\n\u001b[32m    545\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    546\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SAMSUNG\\anaconda3\\envs\\deep\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SAMSUNG\\anaconda3\\envs\\deep\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:359\u001b[39m, in \u001b[36maccuracy_score\u001b[39m\u001b[34m(y_true, y_pred, normalize, sample_weight)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[32m    358\u001b[39m y_true, y_pred = attach_unique(y_true, y_pred)\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m y_type, y_true, y_pred = \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y_type.startswith(\u001b[33m\"\u001b[39m\u001b[33mmultilabel\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SAMSUNG\\anaconda3\\envs\\deep\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:97\u001b[39m, in \u001b[36m_check_targets\u001b[39m\u001b[34m(y_true, y_pred)\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[32m     71\u001b[39m \n\u001b[32m     72\u001b[39m \u001b[33;03mThis converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     94\u001b[39m \u001b[33;03my_pred : array or indicator matrix\u001b[39;00m\n\u001b[32m     95\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     96\u001b[39m xp, _ = get_namespace(y_true, y_pred)\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m type_true = type_of_target(y_true, input_name=\u001b[33m\"\u001b[39m\u001b[33my_true\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     99\u001b[39m type_pred = type_of_target(y_pred, input_name=\u001b[33m\"\u001b[39m\u001b[33my_pred\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SAMSUNG\\anaconda3\\envs\\deep\\Lib\\site-packages\\sklearn\\utils\\validation.py:473\u001b[39m, in \u001b[36mcheck_consistent_length\u001b[39m\u001b[34m(*arrays)\u001b[39m\n\u001b[32m    471\u001b[39m lengths = [_num_samples(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m arrays \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(lengths)) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    474\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    475\u001b[39m         % [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[32m    476\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Found input variables with inconsistent numbers of samples: [407, 1627]"
     ]
    }
   ],
   "source": [
    "# n_gram 실험 1,2 1,3\n",
    "# 성능향상 기대 연속된 단어패턴 포착\n",
    "tfidf_12 = TfidfVectorizer(token_pattern= r\"[\\w'] {3,}\", stop_words=stopwords.words('english'),\n",
    "                           ngram_range = (1,2), min_df=5, max_df=0.5) #max_features=2000)\n",
    "\n",
    "x_train_12 = tfidf_12.fit_transform(x_train)\n",
    "x_test_12 = tfidf_12.transform(x_test)\n",
    "x_train_12.shape#, x_test_12.shape\n",
    "\n",
    "lr_c = LogisticRegression(max_iter=1000)\n",
    "lr_c.fit(x_train_12, y_train)\n",
    "print(lr_c.score(x_train_12,y_train), lr_c.score(x_train_12,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ced34c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdedc58d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>돈 들인건 티가 나지만 보는 내내 하품만</td>\n",
       "      <td>1</td>\n",
       "      <td>2018.10.29</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>몰입할수밖에 없다. 어렵게 생각할 필요없다. 내가 전투에 참여한듯 손에 땀이남.</td>\n",
       "      <td>10</td>\n",
       "      <td>2018.10.26</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>이전 작품에 비해 더 화려하고 스케일도 커졌지만.... 전국 맛집의 음식들을 한데 ...</td>\n",
       "      <td>8</td>\n",
       "      <td>2018.10.24</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>이 정도면 볼만하다고 할 수 있음!</td>\n",
       "      <td>8</td>\n",
       "      <td>2018.10.22</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>재미있다</td>\n",
       "      <td>10</td>\n",
       "      <td>2018.10.20</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  rating        date  \\\n",
       "0                             돈 들인건 티가 나지만 보는 내내 하품만       1  2018.10.29   \n",
       "1       몰입할수밖에 없다. 어렵게 생각할 필요없다. 내가 전투에 참여한듯 손에 땀이남.      10  2018.10.26   \n",
       "2  이전 작품에 비해 더 화려하고 스케일도 커졌지만.... 전국 맛집의 음식들을 한데 ...       8  2018.10.24   \n",
       "3                                이 정도면 볼만하다고 할 수 있음!       8  2018.10.22   \n",
       "4                                               재미있다      10  2018.10.20   \n",
       "\n",
       "    title  \n",
       "0  인피니티 워  \n",
       "1  인피니티 워  \n",
       "2  인피니티 워  \n",
       "3  인피니티 워  \n",
       "4  인피니티 워  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 한국어처리 konply\n",
    "# 품사기반 태깅 tokenizer noun verb adjectiv \n",
    "\n",
    "# 데이터로딩\n",
    "import pandas as pd\n",
    "url = \"https://drive.google.com/uc?id=1KOKgZ4qCg49bgj1QNTwk1Vd29soeB27o\"\n",
    "df = pd.read_csv(url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4b3a2a4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['인피니티 워', '라라랜드', '곤지암', '신과함께', '범죄도시', '택시운전사', '코코'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.title.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80389257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12864    광주민주항쟁은  개인에  영웅담이  아니라 우리민족이 가진  비극적인 역사중 하나이...\n",
       " 5766     왜 그렇게 인기가 많은지 이해가안됍니다. 중간에 중간 지루함도있고 영화에서 전달하려...\n",
       " 4858             평점 높은사람들이 원작보면, 다 평점깎을거라 생각한다, 모든걸 망친 각색.\n",
       " 12728                                             맘이 아픕니다.\n",
       " 8699     이렇게 한국 영화를 재밌게 보는 건 처음 입니다.스토리도 탄탄하고 한국에서 이런 영...\n",
       "                                ...                        \n",
       " 2117                                           쓰레기   시간아깝다\n",
       " 2810                                                 넘 지루함\n",
       " 6758     CG는 많이 늘었네 느낌 눈물은 원래 많은 편이라 그냥 울었음 내용은 산만하고 조금...\n",
       " 10486                                                  bbb\n",
       " 2954                                       이런 천재감독을 봤나~~대박\n",
       " Name: review, Length: 11780, dtype: object,\n",
       " 4651                      킬링타임용 여러가지 시도를 한것같긴한데 뭔가가 빠진듯...\n",
       " 9094     원작내용을 잘녹였고 cg도 최선을다한느낌인데 지루함. 걍 지루해. 노잼이 아닐수도있...\n",
       " 273                                           이 영화는 무조건 백점\n",
       " 11897    광주시민이 아닌 서울시민의 광주항쟁 경험사. 좀 더 리얼하게 그렸으면 하는 아쉬움....\n",
       " 4489     미쳣다 이건.. 한국에서 만든게 맞나 싶을 정도로 놀랍다... 너무 무섭고 감탄하면...\n",
       "                                ...                        \n",
       " 3212     농담아니고 너무재미없네 언제 재미있어지나 하고 계속기다리다가 끝난영화 정말 약간의 ...\n",
       " 12506    송강호 연기력이  인물에 쏙 빠져든 느낌이 안들엇어요 좀 아쉬엇음. 친일파로  국민...\n",
       " 186      일부 오역이 있긴 했지만, 딱히 보는데 지장있는 수준은 아니었다고 봅니다. (특히 ...\n",
       " 4644     1차 조일전쟁(임진왜란)당시 참전했던, 신립 장군 기병대가 갯벌 진창에서 함정에 걸...\n",
       " 10522                             가볍게 즐거운 영화, 그리고 윤계상의 발전!\n",
       " Name: review, Length: 2945, dtype: object,\n",
       " 12864    택시운전사\n",
       " 5766      신과함께\n",
       " 4858      신과함께\n",
       " 12728    택시운전사\n",
       " 8699      신과함께\n",
       "          ...  \n",
       " 2117      라라랜드\n",
       " 2810      라라랜드\n",
       " 6758      신과함께\n",
       " 10486     범죄도시\n",
       " 2954      라라랜드\n",
       " Name: title, Length: 11780, dtype: object,\n",
       " 4651        곤지암\n",
       " 9094       신과함께\n",
       " 273      인피니티 워\n",
       " 11897     택시운전사\n",
       " 4489        곤지암\n",
       "           ...  \n",
       " 3212        곤지암\n",
       " 12506     택시운전사\n",
       " 186      인피니티 워\n",
       " 4644        곤지암\n",
       " 10522      범죄도시\n",
       " Name: title, Length: 2945, dtype: object]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df.review, df.title, stratify=df.title, test_size=0.2, random_state=42)\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c31fb0e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[117]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# simple 버전\u001b[39;00m\n\u001b[32m      2\u001b[39m tfidf = TfidfVectorizer(tokenizer=okt.nouns, max_features=\u001b[32m2000\u001b[39m, min_df=\u001b[32m5\u001b[39m, max_df=\u001b[32m0.5\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m x_train_tfidf = \u001b[43mtfidf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m x_test_tfidf = tfidf.transform(x_test)\n\u001b[32m      5\u001b[39m clf = LogisticRegression(max_iter=\u001b[32m1000\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SAMSUNG\\anaconda3\\envs\\deep\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2104\u001b[39m, in \u001b[36mTfidfVectorizer.fit_transform\u001b[39m\u001b[34m(self, raw_documents, y)\u001b[39m\n\u001b[32m   2097\u001b[39m \u001b[38;5;28mself\u001b[39m._check_params()\n\u001b[32m   2098\u001b[39m \u001b[38;5;28mself\u001b[39m._tfidf = TfidfTransformer(\n\u001b[32m   2099\u001b[39m     norm=\u001b[38;5;28mself\u001b[39m.norm,\n\u001b[32m   2100\u001b[39m     use_idf=\u001b[38;5;28mself\u001b[39m.use_idf,\n\u001b[32m   2101\u001b[39m     smooth_idf=\u001b[38;5;28mself\u001b[39m.smooth_idf,\n\u001b[32m   2102\u001b[39m     sublinear_tf=\u001b[38;5;28mself\u001b[39m.sublinear_tf,\n\u001b[32m   2103\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2104\u001b[39m X = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2105\u001b[39m \u001b[38;5;28mself\u001b[39m._tfidf.fit(X)\n\u001b[32m   2106\u001b[39m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[32m   2107\u001b[39m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SAMSUNG\\anaconda3\\envs\\deep\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SAMSUNG\\anaconda3\\envs\\deep\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1376\u001b[39m, in \u001b[36mCountVectorizer.fit_transform\u001b[39m\u001b[34m(self, raw_documents, y)\u001b[39m\n\u001b[32m   1368\u001b[39m             warnings.warn(\n\u001b[32m   1369\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mUpper case characters found in\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1370\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m vocabulary while \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlowercase\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1371\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m is True. These entries will not\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1372\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m be matched with any documents\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1373\u001b[39m             )\n\u001b[32m   1374\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1376\u001b[39m vocabulary, X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.binary:\n\u001b[32m   1379\u001b[39m     X.data.fill(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SAMSUNG\\anaconda3\\envs\\deep\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1282\u001b[39m, in \u001b[36mCountVectorizer._count_vocab\u001b[39m\u001b[34m(self, raw_documents, fixed_vocab)\u001b[39m\n\u001b[32m   1280\u001b[39m     vocabulary = \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[32m   1281\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[32m-> \u001b[39m\u001b[32m1282\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1283\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1284\u001b[39m         )\n\u001b[32m   1286\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m indptr[-\u001b[32m1\u001b[39m] > np.iinfo(np.int32).max:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[32m   1287\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[31mValueError\u001b[39m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "# simple 버전\n",
    "tfidf = TfidfVectorizer(tokenizer=okt.nouns, max_features=2000, min_df=5, max_df=0.5)\n",
    "x_train_tfidf = tfidf.fit_transform(x_train)\n",
    "x_test_tfidf = tfidf.transform(x_test)\n",
    "clf = LogisticRegression(max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3124b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf.fit(x_train_tfidf, y_train)\n",
    "clf.score(x_train_tfidf, y_train), clf.score(x_test_tfidf, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1819bd4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SAMSUNG\\anaconda3\\envs\\deep\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[118]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [w \u001b[38;5;28;01mfor\u001b[39;00m w, tag \u001b[38;5;129;01min\u001b[39;00m okt.pos(text,norm=\u001b[38;5;28;01mTrue\u001b[39;00m, stem=\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m target]\n\u001b[32m      5\u001b[39m tfidf = TfidfVectorizer(tokenizer=okt.nouns, max_features=\u001b[32m2000\u001b[39m, min_df=\u001b[32m5\u001b[39m, max_df=\u001b[32m0.5\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m x_train_tfidf = \u001b[43mtfidf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m x_test_tfidf = tfidf.transform(x_test)\n\u001b[32m      8\u001b[39m clf = LogisticRegression(max_iter=\u001b[32m1000\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SAMSUNG\\anaconda3\\envs\\deep\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2104\u001b[39m, in \u001b[36mTfidfVectorizer.fit_transform\u001b[39m\u001b[34m(self, raw_documents, y)\u001b[39m\n\u001b[32m   2097\u001b[39m \u001b[38;5;28mself\u001b[39m._check_params()\n\u001b[32m   2098\u001b[39m \u001b[38;5;28mself\u001b[39m._tfidf = TfidfTransformer(\n\u001b[32m   2099\u001b[39m     norm=\u001b[38;5;28mself\u001b[39m.norm,\n\u001b[32m   2100\u001b[39m     use_idf=\u001b[38;5;28mself\u001b[39m.use_idf,\n\u001b[32m   2101\u001b[39m     smooth_idf=\u001b[38;5;28mself\u001b[39m.smooth_idf,\n\u001b[32m   2102\u001b[39m     sublinear_tf=\u001b[38;5;28mself\u001b[39m.sublinear_tf,\n\u001b[32m   2103\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2104\u001b[39m X = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2105\u001b[39m \u001b[38;5;28mself\u001b[39m._tfidf.fit(X)\n\u001b[32m   2106\u001b[39m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[32m   2107\u001b[39m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SAMSUNG\\anaconda3\\envs\\deep\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SAMSUNG\\anaconda3\\envs\\deep\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1376\u001b[39m, in \u001b[36mCountVectorizer.fit_transform\u001b[39m\u001b[34m(self, raw_documents, y)\u001b[39m\n\u001b[32m   1368\u001b[39m             warnings.warn(\n\u001b[32m   1369\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mUpper case characters found in\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1370\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m vocabulary while \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlowercase\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1371\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m is True. These entries will not\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1372\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m be matched with any documents\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1373\u001b[39m             )\n\u001b[32m   1374\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1376\u001b[39m vocabulary, X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.binary:\n\u001b[32m   1379\u001b[39m     X.data.fill(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SAMSUNG\\anaconda3\\envs\\deep\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1282\u001b[39m, in \u001b[36mCountVectorizer._count_vocab\u001b[39m\u001b[34m(self, raw_documents, fixed_vocab)\u001b[39m\n\u001b[32m   1280\u001b[39m     vocabulary = \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[32m   1281\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[32m-> \u001b[39m\u001b[32m1282\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1283\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1284\u001b[39m         )\n\u001b[32m   1286\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m indptr[-\u001b[32m1\u001b[39m] > np.iinfo(np.int32).max:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[32m   1287\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[31mValueError\u001b[39m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "def custom_tokenizer(text):\n",
    "    target = ['None','Verb','Adjective']\n",
    "    return [w for w, tag in okt.pos(text,norm=True, stem=True) if tag in target]\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=okt.nouns, max_features=2000, min_df=5, max_df=0.5)\n",
    "x_train_tfidf = tfidf.fit_transform(x_train)\n",
    "x_test_tfidf = tfidf.transform(x_test)\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(x_train_tfidf, y_train)\n",
    "clf.score(x_train_tfidf, y_train), clf.score(x_test_tfidf, y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
