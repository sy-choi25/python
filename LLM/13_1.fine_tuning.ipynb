{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "308efe82",
   "metadata": {},
   "source": [
    "Attention Mask : 실제 토큰 1/0 패딩  \n",
    "Token Type IDS(Segment IDs) -> 한 문장인지 두 문장인지 BERT에게 알려주는 문장구분번호 -> 문장 구분을 위한 추가 레이어  \n",
    "CLS Token Pooling : [CLS] + token + [SEP]  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2862ab",
   "metadata": {},
   "source": [
    "<span style=\"color: Gold\"> 1. BERT TOKENIZER </span>: 단어를 의미있는 조각(subword)로 나눈다.  \n",
    "unblievable \"un\" \"believ\" \"able\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a70e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문 : Hello, world!\n",
      "원문 : ['hello', ',', 'world', '!']\n",
      "원문 : [7592, 1010, 2088, 999]\n",
      "역변환 : hello, world!\n",
      "\n",
      "원문 : unbelievable performance!\n",
      "원문 : ['unbelievable', 'performance', '!']\n",
      "원문 : [23653, 2836, 999]\n",
      "역변환 : unbelievable performance!\n",
      "\n",
      "원문 : COVID-19 pandemic\n",
      "원문 : ['co', '##vid', '-', '19', 'pan', '##de', '##mic']\n",
      "원문 : [2522, 17258, 1011, 2539, 6090, 3207, 7712]\n",
      "역변환 : covid - 19 pandemic\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')  # uncase 는 소문자로 변환/ case는 대소문자 변환\n",
    "sentences = [\n",
    "    'Hello, world!',\n",
    "    'unbelievable performance!',\n",
    "    'COVID-19 pandemic'\n",
    "]\n",
    "for sentence in sentences:\n",
    "  # 토큰화\n",
    "  tokens = tokenizer.tokenize(sentence)\n",
    "  print(f'원문 : {sentence}')  \n",
    "  print(f'원문 : {tokens}')  \n",
    "\n",
    "  # ID 변환\n",
    "  ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "  print(f'원문 : {ids}')\n",
    "\n",
    "  # 역변환\n",
    "  decoded_str = tokenizer. decode(ids)\n",
    "  print(f'역변환 : {decoded_str}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd05df1",
   "metadata": {},
   "source": [
    "-> 원문은 토큰화(숫자로변환)하고 다시 읽을 수 있는 문자로 바꿈(역변환)  \n",
    "대문자도 모두 소문자로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d679d9f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3ed185",
   "metadata": {},
   "source": [
    "<span style=\"color: Gold\"> 2. Attention Mask\n",
    "- 패딩이 attention 계산에 섞여서 모델이 혼란을 일으켜 불필요한 연산이 증가하고 노이즈가 증가한다  \n",
    "-> `토큰이 실제 단어면 1, 패딩이면 0으로 표시하여 패딩을 무시하라고 알려주는 것`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c27d0f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2460, 6251,  102,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [ 101, 2023, 2003, 1037, 2172, 2936, 6251, 2007, 2062, 2616,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 어텐션마스크\n",
    "from transformers import BertTokenizer\n",
    "# 토크나이저 로드\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "sentences = [\n",
    "    'short sentence',\n",
    "    'This is a much longer sentence with more words'\n",
    "]\n",
    "# 여러문장을 한꺼번에 토크나이징하고 가장 긴 문장길이에 맞춰 자동 패딩 수행\n",
    "encoded = tokenizer(\n",
    "    sentences,\n",
    "    padding = True,\n",
    "    return_tensors = 'pt' # 토크나이저가 출력하는 결과를 파이토치 텐서(torch.Tensor) 형태로 만들어라 -> 모델(BERT, GPT 등)은 텐서(tensor) 형식만 받아서 연산\n",
    ")\n",
    "encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de80f984",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febf985c",
   "metadata": {},
   "source": [
    "<span style=\"color: Gold\"> 3. token_type_ids\n",
    "- 한 문장인지 두 문장인지 BERT에게 알려주는 문장구분번호 -> 문장 구분을 위한 추가 레이어  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a655520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'the', 'weather', 'is', 'nice', '[SEP]', 'let', \"'\", 's', 'f', '##o', 'for', 'a', 'work', '[SEP]']\n",
      "[CLS]               :    101(     0)(시작)\n",
      "the                 :   1996(     0)(문장 A)\n",
      "weather             :   4633(     0)(문장 A)\n",
      "is                  :   2003(     0)(문장 A)\n",
      "nice                :   3835(     0)(문장 A)\n",
      "[SEP]               :    102(     0)(구분자)\n",
      "let                 :   2292(     1)(문장 B)\n",
      "'                   :   1005(     1)(문장 B)\n",
      "s                   :   1055(     1)(문장 B)\n",
      "f                   :   1042(     1)(문장 B)\n",
      "##o                 :   2080(     1)(문장 B)\n",
      "for                 :   2005(     1)(문장 B)\n",
      "a                   :   1037(     1)(문장 B)\n",
      "work                :   2147(     1)(문장 B)\n",
      "[SEP]               :    102(     1)(구분자)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "# 토크나이저 로드\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "sentence_A = 'The weather is nice'\n",
    "sentence_B = \"Let's fo for a work\"\n",
    "# 두 문장을 하나의 입력으로 인코딩\n",
    "encoded = tokenizer(\n",
    "    sentence_A,\n",
    "    sentence_B,\n",
    "    padding = True,\n",
    "    return_tensors = 'pt'\n",
    ")\n",
    "encoded\n",
    "# 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]]) A를 0, B를 1로 변환하여 다른 문장임을 표시\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoded['input_ids'][0])\n",
    "# input_ids': tensor([[ 101, 1996, 4633, 2003, 3835,  102, 2292, 1005, 1055, 1042, 2080, 2005,1037, 2147,  102]])\n",
    "# 4글자말고 3글자가 특수토큰이라는 것을 알 수 있음. 101은 시작, 102는 문장의 끝이자 다른 문장의 시작\n",
    "print(tokens)\n",
    "for token, token_id, type_id in zip(tokens,encoded['input_ids'][0],encoded['token_type_ids'][0]):\n",
    "  segment = '문장 A' if type_id == 0 else '문장 B'\n",
    "  if token == '[SEP]': \n",
    "    segment = '구분자'\n",
    "  elif token == '[CLS]':\n",
    "    segment = '시작'\n",
    "  print(f'{token:20s}: {token_id.item():6d}({type_id.item():6d})({segment})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5364114",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26480ca5",
   "metadata": {},
   "source": [
    "<span style=\"color: Gold\"> [CLS]Token Pooling : \n",
    "- BERT 첫번째 토큰 [CLS] 문서 전체의 요약 => 분류 작업을 할때  \n",
    "이 토큰의 출력만 가져와서 분류기 (classifier)에 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db1ac71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력문장 : BERT is amazing for NLP tasks!\n",
      "last_hidden_state 형태 : torch.Size([1, 10, 768])\n",
      "batch_size = 1 sequence_length : 10 hidden_size = 768\n",
      "cls_embedding 형태 : torch.Size([1, 768])\n",
      "logits = tensor([[ 0.0050, -0.0674]], grad_fn=<AddmmBackward0>)\n",
      "probs = tensor([[0.5181, 0.4819]], grad_fn=<SoftmaxBackward0>)\n",
      "predicted class = 0\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer,BertModel\n",
    "import torch\n",
    "# 토크나이저 로드\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "sentence = \"BERT is amazing for NLP tasks!\"\n",
    "# 인코딩\n",
    "inputs = tokenizer(sentence,return_tensors = 'pt') # 'return_tensorse' -> 'return_tensors' 수정\n",
    "# BERT 통과\n",
    "with torch.no_grad():\n",
    "  outputs = model(**inputs)\n",
    "# 출력 형태 확인\n",
    "last_hidden_state = outputs.last_hidden_state # 문장 가장 마지막에 쌓인 state\n",
    "print(f'입력문장 : {sentence}')\n",
    "print(f'last_hidden_state 형태 : {last_hidden_state.shape}')\n",
    "print(f'batch_size = 1 sequence_length : {last_hidden_state.shape[1]} hidden_size = {last_hidden_state.shape[2]}')\n",
    "# [CLS] 토큰 추출\n",
    "cls_embedding = last_hidden_state[:,0,:]    # last_hidden_state : [batch_size, seq_len, hidden_size]/ \n",
    "                                            # batch_size = 1 (문장 1개), seq_len = 문장 토큰 수, hidden_size = 768 (BERT-base)\n",
    "                                            # [:,0,:] 의미:/   : → batch 전체 선택/   0 → 첫 번째 토큰([CLS])/  : → hidden_size 전체 선택\n",
    "print(f'cls_embedding 형태 : {cls_embedding.shape}')\n",
    "# 분류기 (2-class)\n",
    "classifier = torch.nn.Linear(768,2) # Linear 선형변환 / 입력: [CLS] 벡터 768차원 / 출력: 2차원 logits (클래스 2개) 긍정,부정\n",
    "logits = classifier(cls_embedding)  # (1,2) (batch, class개수)\n",
    "probs = torch.softmax(logits, dim = -1) # softmax → logits → 확률로 변환 / dim=-1 → 마지막 차원 기준으로 계산 (클래스 차원)\n",
    "print(f'logits = {logits}')\n",
    "print(f'probs = {probs}')\n",
    "print(f'predicted class = {torch.argmax(probs).item()}')    # argmax : 가장 큰 값의 인덱스(위치)를 반환하는 함수/ -> 즉 확률이 가장 높을 것을 가져오는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69751125",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2fc71c",
   "metadata": {},
   "source": [
    "<span style=\"color: Gold\"> 미세 조정 학습 Fine_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee68ec9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1, loss : 0.7223823070526123\n",
      "epoch : 2, loss : 0.6858187019824982\n",
      "epoch : 3, loss : 0.6633428931236267\n",
      "epoch : 4, loss : 0.5623259544372559\n",
      "epoch : 5, loss : 0.5752913355827332\n",
      "epoch : 6, loss : 0.5011169910430908\n",
      "epoch : 7, loss : 0.47055280208587646\n",
      "epoch : 8, loss : 0.47721824049949646\n",
      "epoch : 9, loss : 0.3633720725774765\n",
      "epoch : 10, loss : 0.4408431947231293\n",
      "epoch : 11, loss : 0.4125638008117676\n",
      "epoch : 12, loss : 0.3528033047914505\n",
      "epoch : 13, loss : 0.40727663040161133\n",
      "epoch : 14, loss : 0.2697630375623703\n",
      "epoch : 15, loss : 0.28098955750465393\n",
      "epoch : 16, loss : 0.254472091794014\n",
      "epoch : 17, loss : 0.2099958211183548\n",
      "epoch : 18, loss : 0.23029977083206177\n",
      "epoch : 19, loss : 0.18939511477947235\n",
      "epoch : 20, loss : 0.20245113968849182\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "texts = [\n",
    "    \"This movie is fantastic!\",\n",
    "    \"Terrible film, waste of time.\",\n",
    "    \"Amazing plot and great acting.\",\n",
    "    \"Boring and predictable.\"\n",
    "]\n",
    "labels = [1, 0, 1, 0]  # 1=positive, 0=negative\n",
    "\n",
    "# 토크나이져 모델\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# 모델\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=2)\n",
    "# 데이터셋\n",
    "class SimpleDataset(Dataset):\n",
    "  def __init__(self, texts, labels):\n",
    "    self.encodings = tokenizer(texts, truncation=True, padding=True, return_tensors='pt')\n",
    "    self.labels = labels\n",
    "  def __getitem__(self, idx):\n",
    "    item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "    item['labels'] = torch.tensor(self.labels[idx])\n",
    "    return item\n",
    "  def __len__(self):\n",
    "    return len(self.labels)\n",
    "dataset = SimpleDataset(texts,labels)\n",
    "loader = DataLoader(dataset, batch_size=2)\n",
    "# 학습설정\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "# 미세조정(학습)\n",
    "model.train()\n",
    "for epoch in range(20):\n",
    "  total_loss = 0\n",
    "  for batch in loader:\n",
    "    optimizer.zero_grad()\n",
    "    inputs = { k:v.to(device) for k,v in batch.items() if k != 'labels'}\n",
    "    labels = batch['labels'].to(device)\n",
    "    outputs = model(**inputs, labels=labels)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    total_loss += loss.item()\n",
    "  print(f'epoch : {epoch+1}, loss : {total_loss/len(loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c72b148",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0749938b",
   "metadata": {},
   "source": [
    "추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84865dc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.3854, 0.6146],\n",
       "         [0.4785, 0.5215],\n",
       "         [0.4353, 0.5647]]),\n",
       " array([1, 1, 1]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval() # 평가모드\n",
    "sample_sentences= [\n",
    "\"I am really disappointed with the result.\",\n",
    "\"The service was terrible and not worth the money.\",\n",
    "\"I don't like this product at all.\"\n",
    "]\n",
    "# 토큰화\n",
    "inputs = tokenizer(\n",
    "    sample_sentences,\n",
    "    truncation = True,\n",
    "    padding = True,\n",
    "    return_tensors = 'pt'\n",
    ")\n",
    "\n",
    "# gpu, cpu 설정\n",
    "inputs = {k: v.to(device) for k,v in inputs.items()}\n",
    "# 추론\n",
    "with torch.no_grad():\n",
    "  outputs = model(**inputs)\n",
    "  logits = outputs.logits\n",
    "  probs = torch.softmax(logits, dim=-1)\n",
    "  pred = torch.argmax(probs, dim=-1).detach().numpy()\n",
    "probs, pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
